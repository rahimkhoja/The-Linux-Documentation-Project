<html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Le traitement en parall&egrave;le sous Linux</title><link href="style.css" rel="stylesheet" type="text/css"><meta content="DocBook XSL Stylesheets V1.68.1" name="generator"><meta name="description" content="

Le traitement en parall&egrave;le 
(Parallel Processing) se 
r&eacute;f&egrave;re &agrave; l'id&eacute;e d'acc&eacute;l&eacute;rer l'ex&eacute;cution d'un programme en divisant 
celui-ci en plusieurs fragments pouvant &ecirc;tre ex&eacute;cut&eacute;s simultan&eacute;ment, 
chacun sur son propre processeur, un programme ex&eacute;cut&eacute; sur 
N processeurs pouvant alors fonctionner 
N fois plus vite qu'il le ferait en utilisant un 
seul processeur. Ce document traite des quatre approches de base du 
traitement en parall&egrave;le accessibles aux utilisateurs de Linux&nbsp;: les 
syst&egrave;mes Linux SMP, les grappes 
(cluster) de syst&egrave;mes 
Linux mis en r&eacute;seau, l'ex&eacute;cution en parall&egrave;le avec utilisation des 
instructions multim&eacute;dia (ex&nbsp;: MMX), et l'utilisation des 
processeurs secondaires embarqu&eacute;s dans une machine fonctionnant sous 
Linux.

"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="article" lang="fr"><div class="titlepage"><div><div><h1 class="title"><a name="N10001"></a>Le traitement en parall&egrave;le sous Linux</h1></div><div><h3 class="subtitle"><i>
      Version fran&ccedil;aise du <span class="foreignphrase"><em class="foreignphrase">Parallel Processing HOWTO</em></span>
   </i></h3></div><div><div class="author"><h3 class="author"><span class="firstname">Hank</span> <span class="surname">Dietz</span></h3><code class="email">&lt;<a href="mailto:hankd CHEZ engr POINT uky POINT edu">hankd CHEZ engr POINT uky POINT edu</a>&gt;</code></div></div><div><p class="othercredit"><span class="contrib">Adaptation fran&ccedil;aise</span>&nbsp;: <span class="firstname">Dominique</span> <span class="surname">van den Broeck</span></p></div><div><p class="othercredit"><span class="contrib">Relecture de la version fran&ccedil;aise</span>&nbsp;: <span class="firstname">Isabelle</span> <span class="surname">Hurbain</span></p></div><div><p class="othercredit"><span class="contrib">Pr&eacute;paration de la publication de la v.f.</span>&nbsp;: <span class="firstname">Jean-Philippe</span> <span class="surname">Gu&eacute;rard</span></p></div><div><p class="releaseinfo">Version&nbsp;: 980105.fr.1.1</p></div><div><p class="pubdate">4 septembre 2005</p></div><div><div class="revhistory"><table summary="Revision history" width="100%" border="1"><tr><th colspan="3" valign="top" align="left"><b>Historique des versions</b></th></tr><tr><td align="left">Version 980105.fr.1.1</td><td align="left">2005-09-04</td><td align="left">DV</td></tr><tr><td colspan="3" align="left">Quelques am&eacute;liorations mineures de la version 
          fran&ccedil;aise</td></tr><tr><td align="left">Version 980105.fr.1.0</td><td align="left">2004-09-13</td><td align="left">DV, IH, JPG</td></tr><tr><td colspan="3" align="left">Premi&egrave;re traduction fran&ccedil;aise</td></tr><tr><td align="left">Version 980105</td><td align="left">1998-01-05</td><td align="left">HGD</td></tr><tr><td colspan="3" align="left">Version originale</td></tr></table></div></div><div><div class="abstract"><p class="title"><b>R&eacute;sum&eacute;</b></p><p>

Le <span class="emphasis"><em>traitement en parall&egrave;le</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Processing</em></span></span>&nbsp;&raquo;) se 
r&eacute;f&egrave;re &agrave; l'id&eacute;e d'acc&eacute;l&eacute;rer l'ex&eacute;cution d'un programme en divisant 
celui-ci en plusieurs fragments pouvant &ecirc;tre ex&eacute;cut&eacute;s simultan&eacute;ment, 
chacun sur son propre processeur, un programme ex&eacute;cut&eacute; sur 
<span class="emphasis"><em>N</em></span> processeurs pouvant alors fonctionner 
<span class="emphasis"><em>N</em></span> fois plus vite qu'il le ferait en utilisant un 
seul processeur. Ce document traite des quatre approches de base du 
traitement en parall&egrave;le accessibles aux utilisateurs de Linux&nbsp;: les 
syst&egrave;mes Linux SMP, les grappes 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">cluster</em></span></span>&nbsp;&raquo;) de syst&egrave;mes 
Linux mis en r&eacute;seau, l'ex&eacute;cution en parall&egrave;le avec utilisation des 
instructions multim&eacute;dia (ex&nbsp;: MMX), et l'utilisation des 
processeurs secondaires embarqu&eacute;s dans une machine fonctionnant sous 
Linux.

</p></div></div></div><hr></div><div class="toc"><p><b>Table des mati&egrave;res</b></p><dl><dt><span class="sect1"><a href="#N10073">1. Introduction</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N1009A">1.1. Le traitement en parall&egrave;le correspond-il &agrave; mes besoins&nbsp;?</a></span></dt><dt><span class="sect2"><a href="#N100C3">1.2. Terminologie</a></span></dt><dt><span class="sect2"><a href="#N1021D">1.3. Algorithme d'exemple</a></span></dt><dt><span class="sect2"><a href="#N1023D">1.4. Structure du document</a></span></dt><dt><span class="sect2"><a href="#N1027A">1.5. Note du traducteur</a></span></dt></dl></dd><dt><span class="sect1"><a href="#N102AE">2. Linux sur SMP</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N102FD">2.1. L'&eacute;lectronique SMP</a></span></dt><dt><span class="sect2"><a href="#N1034E">2.2. Introduction &agrave; la programmation en m&eacute;moire partag&eacute;e</a></span></dt><dt><span class="sect2"><a href="#N10564">2.3. bb_threads</a></span></dt><dt><span class="sect2"><a href="#N10601">2.4. LinuxThreads</a></span></dt><dt><span class="sect2"><a href="#N1066C">2.5. La m&eacute;moire partag&eacute;e de System V</a></span></dt><dt><span class="sect2"><a href="#N106D8">2.6. Projection m&eacute;moire (<span class="foreignphrase"><em class="foreignphrase">Memory Map Call</em></span>)</a></span></dt></dl></dd><dt><span class="sect1"><a href="#N10704">3. <span class="foreignphrase"><em class="foreignphrase">Clusters</em></span> de syst&egrave;mes Linux</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N1071A">3.1. Pourquoi un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>&nbsp;?</a></span></dt><dt><span class="sect2"><a href="#N1078F">3.2. Le mat&eacute;riel r&eacute;seau</a></span></dt><dt><span class="sect2"><a href="#N10E23">3.3. Interface Logicielle R&eacute;seau</a></span></dt><dt><span class="sect2"><a href="#N10EEF">3.4. PVM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Virtual Machine</em></span></span>&nbsp;&raquo;)</a></span></dt><dt><span class="sect2"><a href="#N10F38">3.5. MPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Message Passing Interface</em></span></span>&nbsp;&raquo;)</a></span></dt><dt><span class="sect2"><a href="#N10FF0">3.6. AFAPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Aggregate Function API</em></span></span>&nbsp;&raquo;)</a></span></dt><dt><span class="sect2"><a href="#N11022">3.7. Autres biblioth&egrave;ques de gestion de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span></a></span></dt><dt><span class="sect2"><a href="#N11073">3.8. R&eacute;f&eacute;rences g&eacute;n&eacute;rales aux <span class="foreignphrase"><em class="foreignphrase">clusters</em></span></a></span></dt></dl></dd><dt><span class="sect1"><a href="#N1119A">4. SIMD <span class="foreignphrase"><em class="foreignphrase">Within A Register</em></span>&nbsp;: SWAR (Ex&nbsp;: utilisation de MMX)</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N111DA">4.1. Quels usages pour le SWAR&nbsp;?</a></span></dt><dt><span class="sect2"><a href="#N1121A">4.2. Introduction &agrave; la programmation SWAR</a></span></dt><dt><span class="sect2"><a href="#N113E5">4.3. SWAR MMX sous Linux</a></span></dt></dl></dd><dt><span class="sect1"><a href="#N1143E">5. Processeurs auxiliaires des machines Linux</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N11443">5.1. Un PC Linux est une bonne station d'accueil</a></span></dt><dt><span class="sect2"><a href="#N1145E">5.2. Avez-vous essay&eacute; le DSP&nbsp;?</a></span></dt><dt><span class="sect2"><a href="#N114A8">5.3. Calcul &agrave; l'aide des FPGA et circuits logiques reconfigurables</a></span></dt></dl></dd><dt><span class="sect1"><a href="#N114D1">6. D'int&eacute;r&ecirc;t g&eacute;n&eacute;ral</a></span></dt><dd><dl><dt><span class="sect2"><a href="#N114D6">6.1. Compilateurs et langages de programmation</a></span></dt><dt><span class="sect2"><a href="#N11635">6.2. Question de performance</a></span></dt><dt><span class="sect2"><a href="#N11662">6.3. Conclusion &mdash; C'est fini&nbsp;!</a></span></dt></dl></dd></dl></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N10073"></a>1.&nbsp;Introduction</h2></div></div></div><p>

L'<span class="emphasis"><em>ex&eacute;cution en parall&egrave;le</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Processing</em></span></span>&nbsp;&raquo;) se 
rapporte &agrave; l'id&eacute;e d'acc&eacute;l&eacute;rer l'ex&eacute;cution d'un programme en le divisant 
en plusieurs fragments pouvant &ecirc;tre ex&eacute;cut&eacute;s simultan&eacute;ment, chacun sur 
son propre processeur. Un programme ex&eacute;cut&eacute; sur <span class="emphasis"><em>N</em></span> 
processeurs pourrait alors fonctionner <span class="emphasis"><em>N</em></span> fois plus 
vite qu'il ne le ferait en utilisant un seul processeur.

</p><p>

Traditionnellement, les processeurs multiples sont fournis avec un 
&laquo;&nbsp;<span class="quote">ordinateur en parall&egrave;le</span>&nbsp;&raquo; sp&eacute;cialement con&ccedil;u. De ce fait, 
Linux g&egrave;re d&eacute;sormais les syst&egrave;mes <span class="emphasis"><em>SMP</em></span> (souvent 
vendus en tant que &laquo;&nbsp;<span class="quote">serveurs</span>&nbsp;&raquo;) dans lesquels plusieurs 
processeurs partagent le m&ecirc;me bus et la m&ecirc;me m&eacute;moire au sein d'un m&ecirc;me 
ordinateur. Il est &eacute;galement possible &agrave; un groupe d'ordinateurs (par 
exemple un groupe de PC fonctionnant chacun avec Linux) d'&ecirc;tre 
interconnect&eacute;s par un r&eacute;seau pour former un ensemble de traitement en 
parall&egrave;le (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">cluster</em></span></span>&nbsp;&raquo;). La 
troisi&egrave;me alternative pour l'ex&eacute;cution parall&egrave;le sous Linux est 
l'utilisation du jeu d'instructions multim&eacute;dias &eacute;tendu (MultiMedia 
Extend&nbsp;: MMX) pour agir en parall&egrave;le sur des vecteurs de donn&eacute;es 
enti&egrave;res. Il est enfin possible d'utiliser un syst&egrave;me Linux comme 
&laquo;&nbsp;<span class="quote">h&ocirc;te</span>&nbsp;&raquo; h&eacute;bergeant un moteur de calcul en parall&egrave;le 
<span class="emphasis"><em>d&eacute;di&eacute;</em></span>. Toutes ces approches sont trait&eacute;es en d&eacute;tails 
dans ce document.

</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1009A"></a>1.1.&nbsp;Le traitement en parall&egrave;le correspond-il &agrave; mes besoins&nbsp;?</h3></div></div></div><p>
Bien que l'emploi de multiples processeurs puisse acc&eacute;l&eacute;rer nombre
d'op&eacute;rations, la plupart des applications ne peuvent encore tirer
profit du traitement en parall&egrave;le. A la base, le traitement en
parall&egrave;le est appropri&eacute; si&nbsp;:
</p><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Votre application est suffisamment parall&eacute;lis&eacute;e pour faire bon
usage de multiples processeurs. C'est, en partie, une question
d'identification des diff&eacute;rentes portions du programme pouvant
&ecirc;tre ex&eacute;cut&eacute;es ind&eacute;pendamment et simultan&eacute;ment sur des processeurs
s&eacute;par&eacute;s, mais vous d&eacute;couvrirez aussi que certaines choses qui
<span class="emphasis"><em>peuvent</em></span> &ecirc;tre ex&eacute;cut&eacute;es en parall&egrave;le
ralentissent le traitement si elles sont ex&eacute;cut&eacute;es en parall&egrave;le
sur un syst&egrave;me particulier. Par exemple, un programme qui
s'ex&eacute;cute en quatre secondes sur une machine unique pourrait
&ecirc;tre capable de n'occuper qu'une seconde du temps de chacune
de quatre machines, mais n'apportera pourtant aucun gain de
temps s'il faut trois secondes ou plus &agrave; ces machines pour
coordonner leurs actions.

</p></li><li><p>

Soit l'application qui vous int&eacute;resse en particulier a &eacute;t&eacute;
<span class="emphasis"><em>parall&eacute;lis&eacute;e</em></span>, c'est-&agrave;-dire r&eacute;&eacute;crite
pour tirer profit du traitement en parall&egrave;le, soit vous comptez
produire au moins un peu de code original qui le fasse.

</p></li><li><p>

Vous &ecirc;tes int&eacute;ress&eacute; par la recherche de nouvelles solutions impliquant 
un traitement en parall&egrave;le, ou au moins souhaitez vous familiariser 
avec. Le traitement en parall&egrave;le sous Linux n'est pas forc&eacute;ment 
difficile, mais ce n'est pas une notion famili&egrave;re &agrave; beaucoup 
d'utilisateurs, et il n'existe pas de livre intitul&eacute; &laquo;&nbsp;<span class="quote">Le 
<span class="foreignphrase"><em class="foreignphrase">Parallel Processing</em></span> pour les 
nuls</span>&nbsp;&raquo;, en tout cas pas encore. Ce guide est un bon point de 
d&eacute;part, mais il ne contient pas l'int&eacute;gralit&eacute; de ce que vous devez 
conna&icirc;tre.

</p></li></ul></div>

</p><p>

La bonne nouvelle, c'est que si tout ce qui vient d'&ecirc;tre dit est vrai, 
vous d&eacute;couvrirez cependant que le traitement en parall&egrave;le sous Linux 
peut apporter les performances d'un supercalculateur &agrave; des programmes 
effectuant des op&eacute;rations complexes ou travaillant sur de tr&egrave;s grandes 
quantit&eacute;s de donn&eacute;es. Mais en plus, cela peut &ecirc;tre fait en utilisant du 
mat&eacute;riel peu on&eacute;reux et que vous poss&eacute;dez s&ucirc;rement d&eacute;j&agrave;. Avec &ccedil;&agrave;, il 
reste ais&eacute; d'utiliser un syst&egrave;me de traitement en parall&egrave;le sous Linux &agrave; 
d'autres choses lorsqu'il n'est pas en train d'accomplir un traitement 
en parall&egrave;le.

</p><p>

Si le traitement en parall&egrave;le ne correspond <span class="emphasis"><em>pas</em></span> &agrave; 
vos besoins, mais que vous souhaitez tout de m&ecirc;me am&eacute;liorer sensiblement 
les performances de votre machine, il reste des choses que vous pouvez 
faire. Par exemple, vous pouvez am&eacute;liorer les performances d'un 
programme s&eacute;quentiel en rempla&ccedil;ant votre processeur par un plus rapide, 
en ajoutant de la m&eacute;moire, en rempla&ccedil;ant un disque IDE par un 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">fast wide SCSI</em></span></span>&nbsp;&raquo;, et c&aelig;tera. 
Si c'est ce qui vous int&eacute;resse, sautez directement &agrave; la section 6.2, 
sinon poursuivez votre lecture.

</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N100C3"></a>1.2.&nbsp;Terminologie</h3></div></div></div><p>

Bien que le traitement en parall&egrave;le ait &eacute;t&eacute; utilis&eacute; pendant de 
nombreuses ann&eacute;es par de nombreux syst&egrave;mes, il reste &eacute;tranger &agrave; la 
plupart des utilisateurs. Aussi, avant de traiter des diff&eacute;rentes 
alternatives, il est important de se familiariser avec une poign&eacute;e de 
termes usuels&nbsp;:

</p><div class="variablelist"><dl><dt><span class="term">SIMD&nbsp;:</span></dt><dd><p>

SIMD (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Single Instruction stream, Multiple Data stream</em></span></span>&nbsp;&raquo;, ou
&laquo;&nbsp;<span class="quote">Un seul flot d'instruction, plusieurs flots de donn&eacute;es</span>&nbsp;&raquo;),
fait r&eacute;f&eacute;rence &agrave; un mod&egrave;le d'ex&eacute;cution en parall&egrave;le dans
lequel tous les processeurs traitent la m&ecirc;me op&eacute;ration &agrave;
la fois, mais o&ugrave; chaque processeur est autoris&eacute; &agrave; agir sur
sa propre donn&eacute;e. Ce mod&egrave;le convient naturellement au concept
o&ugrave; l'on applique le m&ecirc;me traitement sur chaque &eacute;l&eacute;ment d'un
tableau, et est ainsi souvent associ&eacute; &agrave; la manipulation de
vecteurs ou de tableaux. Toutes les op&eacute;rations &eacute;tant
implicitement synchronis&eacute;es, les interactions entre processeurs
SIMD tendent &agrave; &ecirc;tre facilement et efficacement mises en &#339;uvre.

</p></dd><dt><span class="term">MIMD&nbsp;:</span></dt><dd><p>

MIMD (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Multiple Instruction stream, Multiple Data stream</em></span></span>&nbsp;&raquo;,
ou &laquo;&nbsp;<span class="quote">Plusieurs flots d'instructions, plusieurs flots de donn&eacute;es</span>&nbsp;&raquo;),
se rapporte au mod&egrave;le d'ex&eacute;cution en parall&egrave;le dans lequel chaque
processeur agit essentiellement seul. C'est le mod&egrave;le qui convient
le mieux &agrave; la d&eacute;composition d'un programme pour une ex&eacute;cution en
parall&egrave;le sur une base fonctionnelle. Par exemple, une processeur
peut mettre &agrave; jour une base de donn&eacute;es pendant qu'un autre produit
l'affichage graphique de la nouvelle entr&eacute;e. C'est un mod&egrave;le plus
flexible que l'ex&eacute;cution en SIMD, mais qui s'accomplit au risque
d'un cauchemar pour le d&eacute;bogueur, les &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">race conditions</em></span></span>&nbsp;&raquo;
ou &laquo;&nbsp;<span class="quote">acc&egrave;s concurrents</span>&nbsp;&raquo;, dans lesquels un programme peut planter
de fa&ccedil;on intermittente &agrave; cause des diff&eacute;rences de minutage entre
les op&eacute;rations des diff&eacute;rents processeurs lorsque celles d'un de ces
processeurs sont r&eacute;organis&eacute;es en fonction de celles d'un autre.

</p></dd><dt><span class="term">SPMD&nbsp;:</span></dt><dd><p>

SPMD (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Single Program, Multiple Data</em></span></span>&nbsp;&raquo;),
ou &laquo;&nbsp;<span class="quote">Un seul programme, plusieurs donn&eacute;es</span>&nbsp;&raquo;, est une version
restreinte du MIMD dans laquelle tous les processeurs
ex&eacute;cutent le m&ecirc;me programme. Contrairement au SIMD, chaque
processeur peut suivre un chemin diff&eacute;rent dans le programme.

</p></dd><dt><span class="term">Bande passante&nbsp;:</span></dt><dd><p>

La bande passante 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">bandwidth</em></span></span>&nbsp;&raquo;) d'un syst&egrave;me 
de communication correspond &agrave; la quantit&eacute; maximum de donn&eacute;es que l'on 
peut transmettre en une unit&eacute; de temps&hellip; une fois que la 
transmission de donn&eacute;es a commenc&eacute;. La bande passante des connexions 
s&eacute;rie est souvent mesur&eacute;e en <span class="emphasis"><em>bauds</em></span> ou en 
<span class="emphasis"><em>bits par seconde (b/s)</em></span>, ce qui correspond en 
g&eacute;n&eacute;ral &agrave; huit fois ou dix fois le nombre d'<span class="emphasis"><em>octets par 
secondes (O/s ou B/s</em></span>, B = 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Byte</em></span></span>&nbsp;&raquo; = 
&laquo;&nbsp;<span class="quote">Octet</span>&nbsp;&raquo;). Par exemple, un modem &agrave; 1200 bauds (N.D.T.&nbsp;: 
comme celui du Minitel) peut transf&eacute;rer environ 120 octets &agrave; la seconde 
(B/s), tandis qu'une connexion r&eacute;seau ATM &agrave; 155 Mb/s est environ 
130&nbsp;000 fois plus rapide, en transf&eacute;rant environ 17&nbsp;Mo/s. Une bande 
passante &eacute;lev&eacute;e permet un transfert efficace de larges blocs de donn&eacute;es 
entre processeurs.

</p></dd><dt><span class="term">Latence&nbsp;:</span></dt><dd><p>

La latence d'un syst&egrave;me de communication repr&eacute;sente le temps minimum 
n&eacute;cessaire pour la transmission d'un objet, en incluant toutes les 
donn&eacute;es &laquo;&nbsp;<span class="quote">forfaitaires</span>&nbsp;&raquo; logicielles pour l'&eacute;mission et la 
r&eacute;ception (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">overhead</em></span></span>&nbsp;&raquo;). Le 
temps de latence est tr&egrave;s important dans les traitements en parall&egrave;le 
car il d&eacute;termine la <span class="emphasis"><em>granularit&eacute;</em></span>, la 
dur&eacute;e minimum d'ex&eacute;cution d'un segment de code pour gagner en vitesse 
d'ex&eacute;cution gr&acirc;ce au traitement en parall&egrave;le. Concr&egrave;tement, si un 
segment de code s'ex&eacute;cute en moins de temps qu'il n'en faut pour 
transmettre son r&eacute;sultat (ce d&eacute;lai-ci formant la latence), ex&eacute;cuter ce 
segment en s&eacute;rie plut&ocirc;t qu'en parall&egrave;le sera plus rapide puisqu'il n'y 
aura pas de d&eacute;lai de transmission.

</p></dd><dt><span class="term">Envoi de messages (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Message passing</em></span></span>&nbsp;&raquo;)&nbsp;:</span></dt><dd><p>

Les envois de message sont un mod&egrave;le d'interaction entre les diff&eacute;rents 
processeurs d'un syst&egrave;me parall&egrave;le. En g&eacute;n&eacute;ral, un message est construit 
logiciellement sur un processeur et envoy&eacute; via une interconnexion r&eacute;seau 
&agrave; un autre processeur. Bien que le surco&ucirc;t en temps 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">overhead</em></span></span>&nbsp;&raquo;) engendr&eacute; par la 
gestion de chaque message (ce d&eacute;lai formant la latence) soit &eacute;lev&eacute;, 
typiquement, il n'y a que peu de restrictions quant &agrave; la quantit&eacute; 
d'informations que ce message peut contenir. Ainsi, l'envoi de messages 
peut assurer une bande passante &eacute;lev&eacute;e en apportant une m&eacute;thode efficace 
pour transmettre de grandes quantit&eacute;s d'informations d'un processeur &agrave; 
un autre. En revanche, afin de r&eacute;duire les besoins en co&ucirc;teuses 
op&eacute;rations d'envoi de message, les structures de donn&eacute;es &agrave; l'int&eacute;rieur 
d'un programme doivent &ecirc;tre r&eacute;parties &agrave; travers tous les processeurs de 
fa&ccedil;on &agrave; ce que la plupart des donn&eacute;es r&eacute;f&eacute;renc&eacute;es par chaque processeur 
se trouve dans sa m&eacute;moire locale&hellip; Cette t&acirc;che porte le nom de 
&laquo;&nbsp;<span class="quote">r&eacute;partition des donn&eacute;es</span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote"><span class="emphasis"><em>data 
layout</em></span></span>&nbsp;&raquo;).

</p></dd><dt><span class="term">M&eacute;moire partag&eacute;e&nbsp;:</span></dt><dd><p>

La m&eacute;moire partag&eacute;e est elle aussi un mod&egrave;le d'interaction entre les 
processeurs d'un syst&egrave;me parall&egrave;le. Les syst&egrave;mes comme les machines 
biprocesseurs Pentium faisant fonctionner Linux partagent 
<span class="emphasis"><em>physiquement</em></span> la m&ecirc;me m&eacute;moire entre tous leurs 
processeurs, si bien qu'une valeur &eacute;crite par un processeur est 
directement accessible par un autre processeur. A l'inverse, la m&eacute;moire 
partag&eacute;e <span class="emphasis"><em>logiquement</em></span> peut &ecirc;tre impl&eacute;ment&eacute;e sur les 
syst&egrave;mes o&ugrave; chaque processeur dispose d'une m&eacute;moire qui lui est propre, 
en convertissant chaque r&eacute;f&eacute;rence &agrave; une zone non locale de la m&eacute;moire en 
une communication inter-processeur appropri&eacute;e. Cette impl&eacute;mentation de 
la m&eacute;moire partag&eacute;e est g&eacute;n&eacute;ralement consid&eacute;r&eacute;e comme &eacute;tant plus facile 
&agrave; utiliser que les files de messages. La m&eacute;moire partag&eacute;e physiquement 
peut pr&eacute;senter &agrave; la fois une bande passante &eacute;lev&eacute;e et des temps de 
latence tr&egrave;s bas, mais seulement lorsque les diff&eacute;rents processeurs 
n'essaient pas d'acc&eacute;der au bus simultan&eacute;ment. Ainsi, le mod&egrave;le de 
r&eacute;partition des donn&eacute;es peut avoir une s&eacute;rieuse influence sur les 
performances, et les effets de cache et autres peuvent rendre tr&egrave;s 
difficile la d&eacute;termination du meilleur mod&egrave;le.

</p></dd><dt><span class="term">Fonctions d'agr&eacute;gation (<span class="foreignphrase"><em class="foreignphrase">Aggregate Functions</em></span>)&nbsp;:</span></dt><dd><p>

Dans le mod&egrave;le des files de messages comme dans celui de la m&eacute;moire 
partag&eacute;e, une communication est initi&eacute;e par un processeur seul. Par 
contraste, une fonction d'agr&eacute;gation est un mod&egrave;le implicitement 
parall&egrave;le dans lequel tous les processeurs d'un groupe agissent 
ensemble. Le cas le plus simple est celui des <span class="emphasis"><em>barri&egrave;res de 
synchronisation</em></span>, dans lequel chaque processeur se met en 
attente jusqu'&agrave; ce que le groupe entier ait atteint la barri&egrave;re. Si 
chaque processeur &eacute;met une donn&eacute;e en atteignant une barri&egrave;re, il est 
possible de demander &agrave; l'&eacute;lectronique responsable des communications 
d'&eacute;mettre en retour une valeur &agrave; chaque processeur, valeur qui pourrait 
&ecirc;tre fonction des donn&eacute;es collect&eacute;es sur tous les processeurs. Par 
exemple, la valeur de retour pourrait &ecirc;tre la r&eacute;ponse &agrave; la question 
&laquo;&nbsp;<span class="quote">Est-ce qu'un processeur a trouv&eacute; la r&eacute;ponse&nbsp;?</span>&nbsp;&raquo; ou 
pourrait &ecirc;tre la somme d'une valeur propre &agrave; chaque processeur. Les 
temps de latence peuvent &ecirc;tre bas, mais la bande passante a tendance &agrave; 
&ecirc;tre basse elle aussi. Traditionnellement, ce mod&egrave;le est surtout utilis&eacute; 
pour contr&ocirc;ler l'ex&eacute;cution en parall&egrave;le, plut&ocirc;t que pour distribuer les 
donn&eacute;es.

</p></dd><dt><span class="term">Communication collective&nbsp;:</span></dt><dd><p>

C'est un autre nom pour les fonctions d'agr&eacute;gation, utilis&eacute; le plus 
souvent en r&eacute;f&eacute;rence &agrave; celles qui sont construites en utilisant de 
multiples op&eacute;rations d'envoi de message.

</p></dd><dt><span class="term">SMP&nbsp;:</span></dt><dd><p>

SMP (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Symmetric 
Multi-Processor</em></span></span>&nbsp;&raquo;) se rapporte au concept d'un 
groupe de processeurs travaillant ensemble en tant qu'homologues, si 
bien que chaque partie d'un travail peut &ecirc;tre effectu&eacute;e de la m&ecirc;me fa&ccedil;on 
par n'importe quel processeur de ce groupe. Typiquement, le SMP implique 
la combinaison du MIMD et de la m&eacute;moire partag&eacute;e. Dans l'univers IA32, 
SMP signifie souvent &laquo;&nbsp;<span class="quote">compatible MPS</span>&nbsp;&raquo; 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Intel MultiProcessor 
Specification</em></span></span>&nbsp;&raquo;). &Agrave; l'avenir, cela pourrait 
signifier 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Slot&nbsp;2</em></span></span>&nbsp;&raquo;&hellip;

</p></dd><dt><span class="term">SWAR&nbsp;:</span></dt><dd><p>

SWAR (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">SIMD Within A 
Register</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">SIMD &agrave; l'int&eacute;rieur d'un 
registre</span>&nbsp;&raquo;) est un terme g&eacute;n&eacute;rique qui d&eacute;signe le concept 
consistant &agrave; partitionner un registre en plusieurs champs entiers et &agrave; 
effectuer des op&eacute;rations sur toute la largeur du registre pour faire du 
calcul en parall&egrave;le SIMD sur tous les champs &agrave; la fois. En consid&eacute;rant 
une machine avec des registres longs de <span class="emphasis"><em>k</em></span> bits (et 
donc autant pour les chemins de donn&eacute;es, et les unit&eacute;s des fonctions), 
on sait depuis longtemps que les op&eacute;rations sur les registres ordinaires 
peuvent fonctionner comme des op&eacute;rations parall&egrave;le SIMD sur 
<span class="emphasis"><em>n</em></span> champs de <span class="emphasis"><em>k/n</em></span>&nbsp;bits. Bien que ce 
type de parall&eacute;lisme puisse &ecirc;tre mis en &#339;uvre en utilisant les 
registres entiers et les instructions ordinaires, plusieurs mod&egrave;les 
r&eacute;cents de microprocesseurs int&egrave;grent des instructions sp&eacute;cialis&eacute;es pour 
am&eacute;liorer les performances de cette technique pour des t&acirc;ches orient&eacute;es 
multim&eacute;dia. En plus du <span class="emphasis"><em>MMX</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">MultiMedia eXtension</em></span></span>&nbsp;&raquo;) 
d'Intel/AMD/Cyrix, il existe&nbsp;: <span class="emphasis"><em>MAX</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">MultimediA eXtension</em></span></span>&nbsp;&raquo;) sur 
l'Alpha de Digital, <span class="emphasis"><em>MAX</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Multimedia Acceleration 
eXtension</em></span></span>&nbsp;&raquo;) sur le PA-RISC de Hewlett-Packard, 
<span class="emphasis"><em>MDMX</em></span> (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Digital Media 
eXtension</em></span></span>&nbsp;&raquo;, prononc&eacute; &laquo;&nbsp;<span class="quote">Mad&nbsp;Max</span>&nbsp;&raquo;) 
sur MIPS, et <span class="emphasis"><em>VIS</em></span> (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Visual 
Instruction Set</em></span></span>&nbsp;&raquo;) sur le SPARC V9 de Sun. En 
dehors des trois constructeurs qui ont adopt&eacute; le MMX, tous ces jeux 
d'instructions sont comparables, mais incompatibles entre eux.

</p></dd><dt><span class="term">Processeur auxiliaires, d&eacute;di&eacute;s. (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Attached Processors</em></span></span>&nbsp;&raquo;)&nbsp;:</span></dt><dd><p>

Les processeurs auxiliaires sont essentiellement des calculateurs d&eacute;di&eacute;s 
&agrave; une t&acirc;che particuli&egrave;re, reli&eacute;s &agrave; un syst&egrave;me <span class="emphasis"><em>h&ocirc;te</em></span> 
et servant &agrave; acc&eacute;l&eacute;rer certains types de calculs. Par exemple, de 
nombreuses cartes vid&eacute;o et son pour PC embarquent des processeurs d&eacute;di&eacute;s 
con&ccedil;us pour acc&eacute;l&eacute;rer respectivement les op&eacute;rations graphiques et le 
<span class="emphasis"><em>DSP</em></span> audio (DSP&nbsp;: &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Digital 
Signal Processing</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Traitement 
Num&eacute;rique du Signal</span>&nbsp;&raquo;). Il existe aussi une large vari&eacute;t&eacute; de 
<span class="emphasis"><em>processeurs de tableaux</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">array processors</em></span></span>&nbsp;&raquo;), nomm&eacute;s 
ainsi car ils sont con&ccedil;us pour acc&eacute;l&eacute;rer les op&eacute;rations arithm&eacute;tiques 
sur les tableaux. &Agrave; dire vrai, un certain nombre de supercalculateurs 
commerciaux sont en r&eacute;alit&eacute; form&eacute;s de processeurs d&eacute;di&eacute;s rattach&eacute;s &agrave; des 
stations de travail h&ocirc;tes.

</p></dd><dt><span class="term">RAID&nbsp;:</span></dt><dd><p>

RAID (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Redundant Array of Inexpensive 
Disks</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Batterie Redondante de 
Disques Peu Co&ucirc;teux</span>&nbsp;&raquo;) est une technologie simple servant &agrave; 
am&eacute;liorer tant la bande passante que la fiabilit&eacute; des acc&egrave;s disque. M&ecirc;me 
si le RAID se d&eacute;cline en plusieurs variantes, toutes ont en commun deux 
concepts-cl&eacute;s&nbsp;: D'abord, chaque bloc de donn&eacute;es est 
<span class="emphasis"><em>d&eacute;coup&eacute;</em></span> en segments distribu&eacute;s &agrave; un groupe de 
<span class="emphasis"><em>n+k</em></span> disques de fa&ccedil;on &agrave; ce que chaque disque n'ait &agrave; 
lire que <span class="emphasis"><em>1/n</em></span>i&egrave;me de la donn&eacute;e&hellip; offrant ainsi 
<span class="emphasis"><em>n</em></span> fois la bande passante d'un seul disque. Ensuite, 
des donn&eacute;es redondantes sont &eacute;crites pour que les donn&eacute;es puissent &ecirc;tre 
recouvr&eacute;es si un des disques vient &agrave; d&eacute;faillir. C'est important car 
autrement, si l'un des <span class="emphasis"><em>n+k</em></span> disques tombait en 
panne, le syst&egrave;me de fichiers entier pourrait &ecirc;tre perdu. Il existe une 
bonne pr&eacute;sentation du syst&egrave;me RAID sur <a href="http://www.dpt.com/uraiddoc.html" target="_top">http://www.dpt.com/uraiddoc.html</a>, ainsi que des informations 
concernant le RAID pour Linux sur <a href="http://linas.org/linux/raid.html" target="_top">http://linas.org/linux/raid.html</a>. Hormis la prise en charge du 
mat&eacute;riel RAID sp&eacute;cialis&eacute;, Linux g&egrave;re aussi le RAID logiciel 0, 1, 4 et 5 
&agrave; travers plusieurs disques h&eacute;berg&eacute;s sur un syst&egrave;me Linux unique. 
Reportez-vous aux Software RAID mini-HOWTO et Multi-Disk System Tuning 
mini-HOWTO pour plus de d&eacute;tails. Le RAID au travers de plusieurs disques 
<span class="emphasis"><em>sur plusieurs machines en clusters</em></span> n'est pas 
directement pris en charge.

</p></dd><dt><span class="term">IA32&nbsp;:</span></dt><dd><p>

L'IA32 (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Intel 
Architecture</em></span></span>&nbsp;&raquo;, 32 bits) n'a rien &agrave; voir avec le 
traitement en parall&egrave;le, mais se r&eacute;f&egrave;re &agrave; la classe de processeurs dont 
les instructions sont compatibles avec celles de l'Intel 386. 
Concr&egrave;tement, tout processeur Intel x86 apr&egrave;s le 286 est compatible avec 
le mod&egrave;le de m&eacute;moire &laquo;&nbsp;<span class="quote">&agrave;&nbsp;plat<sup>[<a href="#ftn.N101F7" name="N101F7">1</a>]</sup></span>&nbsp;&raquo; qui caract&eacute;rise l'IA32. AMD et Cyrix font eux 
aussi une multitude de processeurs compatibles IA32. Comme Linux a 
&eacute;volu&eacute; principalement sur des processeurs IA32 et que c'est l&agrave; qu'est 
centr&eacute; le march&eacute; de la grande consommation, il est commode d'utiliser le 
terme IA32 pour distinguer ce type de processeur des PowerPC, Alpha, 
PA-RISC, MIPS, SPARC, et c&aelig;tera. La future IA64 (64 bits avec EPIC, 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Explicitly Parallel Instruction 
Computing</em></span></span>&nbsp;&raquo;) va certainement compliquer les 
choses, mais la production de Merced, le premier processeur IA64, n'est 
pas envisag&eacute;e avant 1999.

</p></dd><dt><span class="term">Produits du commerce&nbsp;:</span></dt><dd><p>

Depuis la mort de plusieurs fabricants de supercalculateurs en parall&egrave;le,
les solutions commerciales toutes faites et pr&ecirc;tes &agrave; l'emploi (en 
anglais &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Commercial Off-The-Shelf</em></span></span>&nbsp;&raquo;
ou <span class="foreignphrase"><em class="foreignphrase">COTS</em></span>, pour &laquo;&nbsp;<span class="quote">disponibles en 
rayons</span>&nbsp;&raquo;) sont couramment consid&eacute;r&eacute;es comme une n&eacute;cessit&eacute; dans le 
monde des syst&egrave;mes de calcul en parall&egrave;le. En &eacute;tant totalement puriste, les 
seuls moyens de traitement en parall&egrave;le disponibles sous forme de 
produits du commerce utilisant des PC sont des choses comme les serveurs 
Windows NT en SMP et les diff&eacute;rentes applications Windows utilisant le 
MMX. &Ecirc;tre aussi puriste ne m&egrave;ne &agrave; rien. L'id&eacute;e fondamentale de 
l'utilisation de produits du commerce est de r&eacute;duire les co&ucirc;ts et les 
temps de d&eacute;veloppement. Ainsi, une mani&egrave;re plus compl&egrave;te et plus utile 
de comprendre l'utilisation de ce type de produit serait de dire que 
la plupart des sous-syst&egrave;mes tirent profit du march&eacute; de masse mais que 
d'autres technologies sont utilis&eacute;es l&agrave; o&ugrave; elles servent vraiment. Le 
plus souvent, les produits du commerce pour le traitement en parall&egrave;le
sont utilis&eacute;s au sein d'un groupe de machines 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">cluster</em></span></span>&nbsp;&raquo;) dans lequel les 
postes sont des PC courants, mais dont l'interface r&eacute;seau et les 
logiciels ont &eacute;t&eacute; quelque peu <span class="emphasis"><em>personnalis&eacute;s</em></span>&hellip; 
classiquement, fonctionnant sous Linux avec des applications dont le 
code source est libre et disponible (par exemple sous
<span class="foreignphrase"><em class="foreignphrase">copyleft</em></span> ou dans le domaine public), mais 
pas litt&eacute;ralement des produits du commerce.

</p></dd></dl></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1021D"></a>1.3.&nbsp;Algorithme d'exemple</h3></div></div></div><p>
Afin de bien comprendre l'usage des diff&eacute;rentes approches de
programmation en parall&egrave;le mises en &eacute;vidence dans ce guide,
il est utile d'&eacute;tudier cet exemple. Bien qu'un algorithme
simple de traitement en parall&egrave;le e&ucirc;t suffi, si l'on en choisit
un qui a d&eacute;j&agrave; &eacute;t&eacute; utilis&eacute; pour faire la d&eacute;monstration d'autres
syst&egrave;mes de programmation parall&egrave;le, il devient un peu plus facile
de comparer et mettre en &eacute;vidence les caract&eacute;ristiques de ces diff&eacute;rentes approches. le livre
de M. J. Quinn, <span class="foreignphrase"><em class="foreignphrase">Parallel Computing Theory And Practice</em></span>
(&laquo;&nbsp;<span class="quote">Th&eacute;orie et Pratique du Calcul en Parall&egrave;le</span>&nbsp;&raquo;), seconde &eacute;dition, &eacute;dit&eacute;
par McGraw Hill, New York en 1994, utilise un algorithme parall&egrave;le
qui calcule la valeur de Pi pour pr&eacute;senter diff&eacute;rents environnements
de programmation sur supercalculateurs parall&egrave;les (par exemple, le
<span class="foreignphrase"><em class="foreignphrase">message passing</em></span> du nCube ou la m&eacute;moire partag&eacute;e des Sequent). Dans ce
guide, nous utiliserons le m&ecirc;me algorithme.
</p><p>
Cet algorithme calcule la valeur approch&eacute;e de Pi en faisant la
somme de l'aire situ&eacute;e sous <span class="emphasis"><em>x</em></span> au carr&eacute;. En
tant que programme C purement s&eacute;quentiel, l'algorithme ressemble &agrave;&nbsp;:
</p><p>

<pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;

main(int argc, char **argv)
{
  register double largeur, somme;
  register int intervalles, i;

  /* Lit le nombre d'intervalles d&eacute;sir&eacute; */
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;

  /* fait le calcul */
  somme = 0;
  for (i=0; i&lt;intervalles; ++i) {
    register double x = (i + 0.5) * largeur;
    somme += 4.0 / (1.0 + x * x);
  }
  somme *= largeur;

  printf("Estimation de la valeur de pi: %f\n", somme);

  return(0);
}
</pre>

</p><p>

En revanche, cet algorithme s&eacute;quentiel conduit facilement &agrave; une 
impl&eacute;mentation &laquo;&nbsp;<span class="quote">parall&egrave;le et embarrassante</span>&nbsp;&raquo;. L'aire est 
subdivis&eacute;e en intervalles, et un nombre quelconque de processeurs peut 
faire la somme de l'intervalle qui lui est assign&eacute; ind&eacute;pendamment des 
autres, sans n&eacute;cessit&eacute; d'interaction entre les processeurs. Une fois que 
les sommes locales ont toutes &eacute;t&eacute; calcul&eacute;es, elles sont additionn&eacute;es 
pour former la somme globale. Cette &eacute;tape requiert un certain niveau de 
coordination et de communication entre les diff&eacute;rents processeurs. 
Enfin, cette somme globale est renvoy&eacute;e &agrave; l'&eacute;cran par un seul 
processeur, en tant que valeur approximative de Pi.

</p><p>
Dans ce guide, les diff&eacute;rentes impl&eacute;mentations parall&egrave;les de cet algorithme
apparaissent l&agrave; ou les diff&eacute;rentes m&eacute;thodes de programmation sont trait&eacute;es.
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1023D"></a>1.4.&nbsp;Structure du document</h3></div></div></div><p>
Le reste de ce document est divis&eacute; en cinq parties. Les sections
2, 3, 4 et 5 correspondent aux trois diff&eacute;rents types de configuration
mat&eacute;rielle pouvant assumer le traitement en parall&egrave;le en utilisant
Linux.
</p><div class="itemizedlist"><ul type="disc"><li><p>

La section 2 traite des syst&egrave;mes Linux sur SMP, lesquels prennent
directement en charge l'ex&eacute;cution MIMD en utilisant la m&eacute;moire
partag&eacute;e, m&ecirc;me si les files de messages sont facilement mises en
place, elles aussi. Bien que Linux sache g&eacute;rer les configurations
SMP jusqu'&agrave; 16 processeurs, la plupart des ordinateurs SMP de type PC
sont dot&eacute;s soit de deux, soit de quatre processeurs identiques.

</p></li><li><p>

La section 3 traite des batteries d'ordinateurs en r&eacute;seau 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">clusters</em></span></span>&nbsp;&raquo;), chaque machine 
fonctionnant sous Linux. Un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> peut 
&ecirc;tre utilis&eacute; comme un syst&egrave;me de traitement en parall&egrave;le g&eacute;rant 
directement l'ex&eacute;cution en MIMD et l'&eacute;change de messages, et peut-&ecirc;tre 
m&ecirc;me aussi la m&eacute;moire partag&eacute;e logique. L'ex&eacute;cution SIMD simul&eacute;e et la 
communication des fonctions d'agr&eacute;gation peuvent aussi &ecirc;tre prises en 
charge, selon le r&eacute;seau exploit&eacute;. Le nombre de processeurs compris dans 
un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> peut s'&eacute;tendre de deux &agrave; 
plusieurs milliers, la principale limitation &eacute;tant le c&acirc;blage physique 
du r&eacute;seau. Dans certains cas, des machines de diff&eacute;rents types peuvent 
&ecirc;tre m&eacute;lang&eacute;es au sein d'un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>. Par 
exemple, un r&eacute;seau qui combinerait des Alpha DEC et des Pentium sous 
Linux serait appel&eacute; <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> 
<span class="emphasis"><em>h&eacute;t&eacute;rog&egrave;ne</em></span>.

</p></li><li><p>

La section 4 traite du SWAR, le &laquo;&nbsp;<span class="quote">SIMD dans un registre</span>&nbsp;&raquo;. 
C'est une forme tr&egrave;s restrictive d'ex&eacute;cution en parall&egrave;le, mais d'un 
autre cot&eacute;, c'est une possibilit&eacute; int&eacute;gr&eacute;e aux processeurs ordinaires. 
Ces derniers temps, les extensions MMX et autres des processeurs 
modernes ont rendu cette approche encore plus efficace.

</p></li><li><p>

La section 5 traite de l'utilisation de PC sous Linux comme h&ocirc;tes pour 
des syst&egrave;mes de calcul parall&egrave;le simples. Sous forme de carte 
d'extension ou de bo&icirc;tiers externes, les processeurs auxiliaires peuvent 
apporter &agrave; des syst&egrave;mes Linux une formidable puissance de traitement 
pour des applications sp&eacute;cifiques. Par exemple, des cartes ISA 
disponibles &agrave; peu de frais fournissent de multiples processeurs DSP, 
offrant plusieurs centaines de M&eacute;gaFLOP aux calculs de grande envergure. 
En revanche, ces cartes ne sont <span class="emphasis"><em>que</em></span> des 
processeurs. Elle n'embarquent g&eacute;n&eacute;ralement pas de syst&egrave;me 
d'exploitation, de disque dur, ou de connecteur pour terminal de 
contr&ocirc;le, et c&aelig;tera. Pour rendre ces syst&egrave;mes exploitables, 
l'&laquo;&nbsp;<span class="quote">h&ocirc;te</span>&nbsp;&raquo; Linux doit fournir ces facilit&eacute;s.

</p></li></ul></div><p>

La section finale de ce document couvre les aspects d'int&eacute;r&ecirc;t g&eacute;n&eacute;ral 
concernant le traitement en parall&egrave;le sous Linux, non sp&eacute;cifique &agrave; l'une 
des approches list&eacute;es ci-dessus.

</p><p>

En lisant ce document, gardez &agrave; l'esprit que nous n'avons pas tout 
test&eacute;, et que beaucoup de choses rapport&eacute;es dans ce document ont 
toujours &laquo;&nbsp;<span class="quote">un caract&egrave;re exp&eacute;rimental</span>&nbsp;&raquo; (une jolie mani&egrave;re de 
dire que cela ne fonctionne pas tout &agrave; fait comme esp&eacute;r&eacute; ;-) ). Cela 
dit, le traitement en parall&egrave;le sous Linux est d&eacute;sormais exploitable, et 
un groupe incroyablement vaste de personnes travaille &agrave; le rendre encore 
meilleur.

</p><p>

L'auteur de la version originale de ce guide est le Dr (Ph.D) Hank 
Dietz, actuellement Professeur Associ&eacute; de l'<span class="foreignphrase"><em class="foreignphrase">Electrical 
and Computer Engineering</em></span> &agrave; l'universit&eacute; de Purdue, West 
Lafayette, IN, 47907-1285. Dietz est propri&eacute;taire des droits sur ce 
document, conform&eacute;ment aux r&egrave;gles du <span class="foreignphrase"><em class="foreignphrase">Linux Documentation 
Project</em></span> (LDP). Bien qu'un effort ait &eacute;t&eacute; fait pour 
assurer l'exactitude de cette pr&eacute;sentation, ni Dietz ni l'Universit&eacute; de 
Purdue ne peuvent &ecirc;tre tenus responsables d'&eacute;ventuels probl&egrave;mes ou 
erreurs, et l'Universit&eacute; de Purdue n'endosse la responsabilit&eacute; d'aucun 
produit ou travaux trait&eacute;s dans ce document.

</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1027A"></a>1.5.&nbsp;Note du traducteur</h3></div></div></div><p>

Chers lecteurs, avant de poursuivre la lecture de ce guide, il est 
important de revenir, notament au vu de la date de publication de cette 
version fran&ccedil;aise, sur plusieurs points&nbsp;:

</p><div class="itemizedlist"><ul type="disc"><li><p>

Le Professeur Henry G. Dietz (dit &laquo;&nbsp;<span class="quote">Hank</span>&nbsp;&raquo;), apr&egrave;s avoir 
enseign&eacute; plusieurs ann&eacute;es &agrave; l'Universit&eacute; de Purdue et y avoir d&eacute;velopp&eacute; 
la plupart de ce qui forme ce document, <span class="emphasis"><em>m&egrave;ne aujourd'hui ses recherches &agrave; 
l'Universit&eacute; du Kentucky</em></span>. Son site personnel se trouve 
d&eacute;sormais ici: <a href="http://aggregate.org/hankd/" target="_top">http://aggregate.org/hankd/</a>. Cela signifie 
&eacute;galement que la plupart des r&eacute;f&eacute;rences &agrave; l'Universit&eacute; de Purdue sont 
d&eacute;sormais caduques. Toutefois, un certain nombre de ces r&eacute;f&eacute;rences ont 
&eacute;t&eacute; conserv&eacute;es en l'&eacute;tat dans ce guide, ce lorsque le contenu r&eacute;f&eacute;renc&eacute; 
&eacute;tait toujours disponible sur le site de l'Universit&eacute; sans avoir &eacute;t&eacute; 
transf&eacute;r&eacute; vers le nouveau site. En tout &eacute;tat de cause, dirigez-vous en 
priorit&eacute; sur le site de l'Universit&eacute; du Kentucky pour tout contact ou 
pour obtenir les informations les plus r&eacute;centes.

</p></li><li><p>

La totalit&eacute; des termes, notament techniques, employ&eacute;s dans ce documents 
ont &eacute;t&eacute; traduits en fran&ccedil;ais, &agrave; quelques exceptions pr&egrave;s. C'est par 
exemple le cas du mot 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">cluster</em></span></span>&nbsp;&raquo;, qui d&eacute;signe en 
informatique la mise en parall&egrave;le de plusieurs machines individuelles et 
coordonn&eacute;es de mani&egrave;re &agrave; les faire agir comme un seul super-ordinateur. 
Le terme fran&ccedil;ais homologue est &laquo;&nbsp;<span class="quote">grappe</span>&nbsp;&raquo;. Toutefois, la 
fr&eacute;quence &agrave; laquelle ce mot est employ&eacute; tant dans ce document (un 
chapitre entier est consacr&eacute; &agrave; ce sujet pr&eacute;cis) que dans la communaut&eacute; 
du traitement en parall&egrave;le en g&eacute;n&eacute;ral est telle que le terme original a 
&eacute;t&eacute; conserv&eacute; dans la pr&eacute;sente version fran&ccedil;aise. Dans le m&ecirc;me esprit, la 
notion de &laquo;&nbsp;<span class="quote">bande passante</span>&nbsp;&raquo; se retrouve tr&egrave;s fr&eacute;quement tout 
au long de ce guide. C'est &agrave; la base un abus de langage, mais la 
popularit&eacute; de cette formule est &eacute;galement suffisament grande pour la 
conserver en l'&eacute;tat.

</p></li><li><p>

La version originale de ce document a &eacute;t&eacute; &eacute;crite en 1998, la version 
fran&ccedil;aise est parue en 2004. Il va sans dire qu'au cours d'une aussi 
longue p&eacute;riode, le paysage informatique a beaucoup &eacute;volu&eacute;, sp&eacute;cialement 
en ce qui concerne le d&eacute;veloppement du noyau Linux. Certaines 
technologies r&eacute;seau (telles que ATM, FireWire, ou Fiber Channel) ou de 
<span class="foreignphrase"><em class="foreignphrase">clustering</em></span> (comme MOSIX), recens&eacute;es comme 
indisponibles en 1998, ont depuis int&eacute;gr&eacute; le noyau, ou sont devenues 
disponibles. En revanche, il est tr&egrave;s peu probable qu'une technologie 
connue pour fonctionner sous Linux lors de la r&eacute;daction de ce document 
soit devenue inutilisable depuis.

</p></li><li><p>

Plus encore que celui de l'industrie informatique, le paysage du 
<span class="foreignphrase"><em class="foreignphrase">World Wide Web</em></span> s'est transform&eacute; de fa&ccedil;on 
&agrave; rendre la plupart des liens propos&eacute;s obsol&egrave;tes. Un effort a &eacute;t&eacute; fait 
pour assurer leur mise &agrave; jour ou leur remplacement, ainsi que la 
pertinence de leur contenu. En d&eacute;pit de cela, un certain nombre d'entre eux,
en particulier ceux dont les projets &eacute;taient h&eacute;berg&eacute;s sur les 
pages personnelles d'&eacute;tudiants de grandes &eacute;coles, n'ont pu &ecirc;tre corrig&eacute;s 
et ont &eacute;t&eacute; retir&eacute;s du document.

</p></li></ul></div><p>

Malgr&eacute; toutes ces r&eacute;serves, les techniques couvertes par ce document 
sont suffisament g&eacute;n&eacute;rales pour rester valables au cours du temps et au 
travers des diff&eacute;rents mod&egrave;les de machines, et son contenu pr&eacute;sente 
toujours un int&eacute;r&ecirc;t &agrave; la fois p&eacute;dagogique et historique, qui restera encore 
longtemps profitable au lecteur. Tout ceci justifie une publication m&ecirc;me 
tardive.

</p><p>

Enfin, le traducteur s'est efforc&eacute; de rendre le pr&eacute;sent document aussi 
correct et fid&egrave;le &agrave; son original que possible, mais n'est pas 
infaillible. Tout signalement d'un contresens, d'une erreur technique, 
ou tout autre d&eacute;faut de traduction sera appr&eacute;ci&eacute; &agrave; sa juste valeur &agrave; 
l'adresse suivante&nbsp;:

<code class="email">&lt;<a href="mailto:dvandenbroeck CHEZ free POINT fr">dvandenbroeck CHEZ free POINT fr</a>&gt;</code>.

</p><p>

Bonne lecture&nbsp;!

</p></div></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N102AE"></a>2.&nbsp;Linux sur SMP</h2></div></div></div><p>

Ce document donne un bref aper&ccedil;u de la mani&egrave;re dont on utilise <a href="http://www.linux.org.uk/SMP/title.html" target="_top">le SMP sous Linux</a> 
pour le traitement en parall&egrave;le. L'information la plus &agrave; jour concernant 
le SMP sous Linux est fort probablement disponible via la liste de 
diffusion du SMP Linux Project (N.D.T.&nbsp;: en anglais). Envoyez un 
courrier &eacute;lectronique &agrave;

<code class="email">&lt;<a href="mailto:majordomo CHEZ vger POINT rutgers POINT edu">majordomo CHEZ vger POINT rutgers POINT edu</a>&gt;</code>

avec le texte <code class="literal">subscribe linux-smp</code> pour rejoindre la 
liste.

</p><p>

Le SMP sous Linux fonctionne-t-il vraiment&nbsp;? En juin 1996, j'ai 
fait l'achat d'un bi-Pentium 100MHz flambant neuf. Le syst&egrave;me complet et 
assembl&eacute;, comprenant les deux processeurs, la carte-m&egrave;re Asus, 256 
kilo-octets de m&eacute;moire cache, 32 m&eacute;ga-octets de RAM, le disque dur d'1.6 
giga-octet, le lecteur de CD-ROM 6X, une carte Stealth 64 et un moniteur 
15'' Acer m'a co&ucirc;t&eacute; 1800 dollars. Cela ne fait que quelques centaines de 
dollars de plus qu'un syst&egrave;me monoprocesseur. Pour faire fonctionner le 
SMP sous Linux, il a suffi d'installer le Linux monoprocesseur 
d'origine, de recompiler le noyau en d&eacute;commentant la ligne 
<code class="literal">SMP=1</code> dans le <span class="emphasis"><em>Makefile</em></span> (bien que 
je trouve le fait de mettre <code class="literal">SMP</code> &agrave; 
<code class="literal">1</code> un peu ironique&nbsp;! ;-) ), et d'informer 
<code class="literal">lilo</code> de l'existence du nouveau noyau. Ce syst&egrave;me 
pr&eacute;sente une stabilit&eacute; et des performances suffisamment bonnes pour 
qu'il me serve depuis de station de travail principale. Pour r&eacute;sumer, le 
SMP sous Linux, &ccedil;a fonctionne&nbsp;!

</p><p>

La question qui se pr&eacute;sente alors est&nbsp;: existe-t-il suffisamment 
d'API de haut niveau permettant d'&eacute;crire et d'ex&eacute;cuter des programmes en 
parall&egrave;le et utilisant la m&eacute;moire partag&eacute;e sous Linux SMP&nbsp;? Courant 
1996, il n'y en avait pas beaucoup. Les choses ont chang&eacute;. Par exemple, 
il existe d&eacute;sormais une biblioth&egrave;que POSIX de gestion des 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span><sup>[<a href="#ftn.N102D7" name="N102D7">2</a>]</sup> tr&egrave;s compl&egrave;te.

</p><p>

Bien que les performances soient moins &eacute;lev&eacute;es que celles des m&eacute;canismes 
de m&eacute;moire partag&eacute;e natifs, un syst&egrave;me Linux sur SMP peut aussi utiliser 
la plupart des logiciels de traitement en parall&egrave;le initialement 
d&eacute;velopp&eacute;s pour des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de stations 
de travail en utilisant la communication par 
<span class="foreignphrase"><em class="foreignphrase">socket</em></span>. Les <span class="emphasis"><em>sockets</em></span> 
(voir section 3.3) fonctionnent &agrave; l'int&eacute;rieur d'une machine en SMP, et 
m&ecirc;me dans un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de machines SMP 
reli&eacute;es en r&eacute;seau. Cependant, les <span class="foreignphrase"><em class="foreignphrase">sockets</em></span> 
engendrent beaucoup de pertes en temps inutiles pour du SMP. Cela 
complique le probl&egrave;me car Linux SMP n'autorise en g&eacute;n&eacute;ral qu'un seul 
processeur &agrave; la fois &agrave; se trouver dans le noyau et le contr&ocirc;leur 
d'interruption est r&eacute;gl&eacute; de fa&ccedil;on &agrave; ce que seul le processeur de 
<span class="foreignphrase"><em class="foreignphrase">boot</em></span><sup>[<a href="#ftn.N102EE" name="N102EE">3</a>]</sup> puisse traiter les interruptions. En d&eacute;pit de cela, 
l'&eacute;lectronique de communication typique des syst&egrave;mes SMP est tellement 
meilleure que la plupart des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> en 
r&eacute;seau que les logiciels pour <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> 
fonctionneront souvent mieux sur du SMP que sur le 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> pour lequel ils ont &eacute;t&eacute; con&ccedil;us.

</p><p>
Le reste de cette section traite de l'&eacute;lectronique contr&ocirc;lant le SMP,
passe en revue les m&eacute;canismes Linux de base partageant de la m&eacute;moire
&agrave; travers les diff&eacute;rents processus d'un programme en parall&egrave;le, fait
quelques remarques concernant l'atomicit&eacute;, la volatilit&eacute;, les verrous
et les lignes de cache, et donne enfin des r&eacute;f&eacute;rences vers d'autres
ressources de traitement en parall&egrave;le &agrave; m&eacute;moire partag&eacute;e.
</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N102FD"></a>2.1.&nbsp;L'&eacute;lectronique SMP</h3></div></div></div><p>

Bien que les syst&egrave;mes SMP soit r&eacute;pandus depuis de nombreuses ann&eacute;es, 
jusque tr&egrave;s r&eacute;cemment, chaque machine tendait &agrave; impl&eacute;menter les 
fonctions de base d'une mani&egrave;re suffisamment diff&eacute;rente des autres pour 
que leur gestion par le syst&egrave;me d'exploitation ne soit pas portable. Les 
choses ont chang&eacute; avec la <span class="foreignphrase"><em class="foreignphrase">Intel's MultiProcessor 
Specification</em></span> (Sp&eacute;cification MultiProcesseurs d'Intel) 
souvent d&eacute;sign&eacute;e par <span class="emphasis"><em>MPS</em></span>. La sp&eacute;cification MPS 1.4 
est actuellement disponible sous forme de document PDF sur <a href="http://www.intel.com/design/intarch/MANUALS/242016.htm" target="_top">http://www.intel.com/design/intarch/MANUALS/242016.htm</a><sup>[<a href="#ftn.N1030A" name="N1030A">4</a>]</sup>,
mais gardez &agrave; l'esprit qu'Intel r&eacute;organise souvent son site web. Un large 
panel de constructeurs fabrique des syst&egrave;mes conformes &agrave; MPS pouvant 
recevoir jusqu'&agrave; quatre processeurs, mais en th&eacute;orie, MPS admet bien 
plus de processeurs.

</p><p>

Les seuls syst&egrave;mes non MPS et non IA32 reconnus par Linux SMP sont les 
machines SPARC multiprocesseurs de Sun4m. Linux SMP prend aussi en 
charge la plupart des machines Intel conformes &agrave; MPS 1.1 ou 1.4, 
comptant jusqu'&agrave; 16 processeurs 486DX, Pentium, Pentium MMX, Pentium Pro 
ou Pentium II. Parmi les processeurs IA32 non pris en charge (N.D.T.&nbsp;: 
par le SMP), on trouve les Intel 386 et 486SX/SLC (l'absence de 
coprocesseur math&eacute;matique interf&egrave;re sur les m&eacute;canismes du SMP) et les 
processeurs AMD et Cyrix (qui n&eacute;cessitent des circuits de gestion du SMP 
diff&eacute;rents et qui ne semblent pas &ecirc;tre disponibles &agrave; l'heure o&ugrave; ce 
document est &eacute;crit).

</p><p>
Il est important de bien comprendre que les performances de diff&eacute;rents
syst&egrave;mes conformes &agrave; MPS peuvent fortement varier. Comme l'on peut s'y
attendre, une des causes de diff&eacute;rence de performance est la vitesse
du processeur&nbsp;: Une horloge plus rapide tend &agrave; rendre les syst&egrave;mes plus
rapides, et un processeur Pentium Pro est plus rapide qu'un Pentium.
En revanche, MPS ne sp&eacute;cifie pas vraiment comment le mat&eacute;riel doit
mettre en &#339;uvre la m&eacute;moire partag&eacute;e, mais seulement comment cette
impl&eacute;mentation doit fonctionner d'un point de vue logiciel. Cela
signifie que les performances d&eacute;pendent aussi de la fa&ccedil;on dont
l'impl&eacute;mentation de la m&eacute;moire partag&eacute;e interagit avec les caract&eacute;ristiques
de Linux SMP et de vos applications en particulier.
</p><p>
La principale diff&eacute;rence entre les syst&egrave;mes conformes &agrave; MPS r&eacute;side
dans la mani&egrave;re dont ils impl&eacute;mentent l'acc&egrave;s &agrave; la m&eacute;moire physiquement
partag&eacute;e.
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10317"></a>2.1.1.&nbsp;Chaque processeur poss&egrave;de-t-il sa propre m&eacute;moire cache de niveau 2 (L2)&nbsp;?</h4></div></div></div><p>
Certains syst&egrave;mes MPS &agrave; base de Pentium, et tous les syst&egrave;mes MPS
Pentium Pro et Pentium II ont des m&eacute;moires cache L2 ind&eacute;pendantes
(le cache L2 est embarqu&eacute; dans le module des Pentium Pro et Pentium II).
Les m&eacute;moires caches L2 dissoci&eacute;es sont g&eacute;n&eacute;ralement r&eacute;put&eacute;es augmenter
les performances de l'ordinateur, mais les choses ne sont pas si &eacute;videntes
sous Linux. La principale complication provient du fait que l'ordonnanceur
de Linux SMP n'essaie pas de maintenir chaque processus sur
le m&ecirc;me processeur, concept connu sous le nom d'<span class="emphasis"><em>affinit&eacute; processeur</em></span>.
Cela pourrait bient&ocirc;t changer. Un d&eacute;bat a r&eacute;cemment eu lieu sur ce sujet dans la
communaut&eacute; des d&eacute;veloppeurs Linux SMP, sous le titre &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">processor
bindings</em></span></span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote">associations de processeurs</span>&nbsp;&raquo;). Sans affinit&eacute; processeur,
des caches L2 s&eacute;par&eacute;s peuvent introduire des d&eacute;lais non n&eacute;gligeables
lorsqu'un processus se voit allouer une tranche de temps d'ex&eacute;cution
sur un processeur qui n'est pas le m&ecirc;me que celui sur lequel il
s'ex&eacute;cutait juste avant.
</p><p>
Plusieurs syst&egrave;mes relativement bon march&eacute; sont organis&eacute;s de mani&egrave;re
&agrave; ce que deux processeurs Pentium puissent partager la m&ecirc;me m&eacute;moire
cache L2. La mauvaise nouvelle, c'est que cela cr&eacute;e des conflits &agrave;
l'utilisation de ce cache, qui d&eacute;gradent s&eacute;rieusement les performances
lorsque plusieurs programmes s&eacute;quentiels ind&eacute;pendants s'ex&eacute;cutent
simultan&eacute;ment. La bonne nouvelle, c'est que bon nombre de programmes
parall&egrave;les pourraient tirer profit de la m&eacute;moire cache partag&eacute;e, car
si les deux processeurs veulent acc&eacute;der &agrave; la m&ecirc;me ligne de m&eacute;moire
partag&eacute;e, seul un processeur doit aller la rapatrier dans le cache,
et l'on &eacute;vite des conflits de bus. Le manque d'affinit&eacute; processeur
peut aussi s'av&eacute;rer moins d&eacute;sastreux avec un cache L2 partag&eacute;. Ainsi,
pour les programmes parall&egrave;les, il n'est pas vraiment certain que
partager la m&eacute;moire cache L2 soit si pr&eacute;judiciable que l'on pourrait
le penser.
</p><p>
&Agrave; l'usage, notre bi-Pentium &agrave; m&eacute;moire cache partag&eacute;e
de 256Ko pr&eacute;sente une vaste &eacute;chelle de performances, d&eacute;pendantes du
niveau d'activit&eacute; noyau requis. Au pire, le gain en vitesse
n'atteint qu'un facteur de 1,2. En revanche, nous avons aussi
constat&eacute; une acc&eacute;l&eacute;ration de 2,1 fois la vitesse d'origine, ce qui
sugg&egrave;re que les calculs intensifs &agrave; base de SPMD tirent vraiment
profit de l'effet d'&laquo;&nbsp;<span class="quote">acquisition partag&eacute;e</span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">shared fetch</em></span></span>&nbsp;&raquo;).
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10331"></a>2.1.2.&nbsp;Configuration du bus&nbsp;?</h4></div></div></div><p>
La premi&egrave;re chose &agrave; dire est que la plupart des syst&egrave;mes modernes
relient le processeur &agrave; un ou plusieurs bus PCI qui &agrave; leur tour
sont &laquo;&nbsp;<span class="quote">pont&eacute;s</span>&nbsp;&raquo; vers un ou plusieurs bus ISA ou EISA. Ces ponts
engendrent des temps de latence, et l'ISA comme l'EISA offrent
g&eacute;n&eacute;ralement des bandes passantes plus r&eacute;duites que le PCI
(ISA &eacute;tant le plus lent). C'est pourquoi les disques, cartes vid&eacute;os et
autres p&eacute;riph&eacute;riques de haute performance devraient en principe
&ecirc;tre connect&eacute;s sur un bus PCI.
</p><p>
Bien qu'un syst&egrave;me MPS puisse apporter un gain en vitesse honorable
&agrave; plusieurs programmes parall&egrave;les de calcul intensif m&ecirc;me avec un
seul bus PCI, les op&eacute;rations d'entr&eacute;es/sorties, elles, ne sont pas
meilleures que sur un syst&egrave;me monoprocesseur. Elles sont peut-&ecirc;tre m&ecirc;me
un peu moins bonnes &agrave; cause des conflits de bus entre les processeurs.
Ainsi, si votre objectif est d'acc&eacute;l&eacute;rer les entr&eacute;es/sorties, prenez
soin de choisir un syst&egrave;me MPS comportant plusieurs bus PCI ind&eacute;pendants
et plusieurs contr&ocirc;leurs d'entr&eacute;es/sorties (par exemple&nbsp;: plusieurs
cha&icirc;nes SCSI). Il vous faudra &ecirc;tre prudent, et s&ucirc;r que Linux
reconna&icirc;t tout votre mat&eacute;riel. Gardez aussi &agrave; l'esprit le fait que
Linux n'autorise qu'un seul processeur &agrave; la fois &agrave; entrer en mode
noyau, aussi devrez-vous choisir des contr&ocirc;leurs qui r&eacute;duisent au
minimum le temps noyau n&eacute;cessaire &agrave; leurs op&eacute;rations. Pour atteindre
des performances vraiment tr&egrave;s &eacute;lev&eacute;es, il se pourrait m&ecirc;me qu'il
vous faille envisager d'effectuer vous-m&ecirc;me les op&eacute;rations d'entr&eacute;e/sortie
de bas niveau directement depuis les processus utilisateurs,
sans appel syst&egrave;me&hellip; ce n'est pas forc&eacute;ment aussi difficile que
cela en a l'air, et cela permet d'&eacute;viter de compromettre la s&eacute;curit&eacute; du
syst&egrave;me (voir la section 3.3 pour une description des techniques de
base).
</p><p>
Il est important de remarquer que la relation entre vitesse du bus
et vitesse du processeur est devenue tr&egrave;s floue ces derni&egrave;res ann&eacute;es.
Bien que la plupart des syst&egrave;mes utilisent maintenant la m&ecirc;me fr&eacute;quence
de bus PCI, il n'est pas rare de trouver un processeur rapide appari&eacute;
avec un bus lent. L'exemple classique est celui du Pentium 133 qui
utilise en g&eacute;n&eacute;ral un bus plus rapide que celui du Pentium 150, produisant
des r&eacute;sultats &eacute;tranges sur les logiciels bancs de tests (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">benchmarks</em></span></span>&nbsp;&raquo;).
Ces effets sont amplifi&eacute;s sur les syst&egrave;mes SMP, o&ugrave; il est encore plus
important d'utiliser un bus rapide.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10341"></a>2.1.3.&nbsp;Interfoliage de la m&eacute;moire et technologie DRAM</h4></div></div></div><p>
L'interfoliage de la m&eacute;moire n'a en fait absolument rien &agrave; voir
avec le MPS, mais vous verrez souvent cette mention accompagner
les syst&egrave;mes MPS car ceux-ci sont typiquement gourmands en
bande passante m&eacute;moire. Concr&egrave;tement, l'interfoliage en deux ou
en quatre voies organise la RAM de fa&ccedil;on &agrave; ce que l'acc&egrave;s &agrave; un
bloc de m&eacute;moire se fasse au travers de plusieurs bancs de RAM
plut&ocirc;t qu'un seul. Ceci acc&eacute;l&egrave;re grandement les acc&egrave;s &agrave; la
m&eacute;moire, particuli&egrave;rement en ce qui concerne le chargement et
l'enregistrement du contenu des lignes de cache.
</p><p>
Il faut toutefois souligner que ce fait n'est pas aussi &eacute;vident
qu'il y parait, car la DRAM EDO et les diff&eacute;rentes technologies
m&eacute;moire tendent &agrave; optimiser ce genre d'op&eacute;rations. Un excellent
aper&ccedil;u des diff&eacute;rentes technologies DRAM est disponible sur
<a href="http://www.pcguide.com/ref/ram/tech.htm" target="_top">http://www.pcguide.com/ref/ram/tech.htm</a>.
</p><p>
Ainsi, par exemple, mieux vaut-il avoir de la m&eacute;moire DRAM EDO
interfoli&eacute;e &agrave; 2 voies, ou de la m&eacute;moire SDRAM non interfoli&eacute;e&nbsp;?
C'est une tr&egrave;s bonne question et la r&eacute;ponse n'est pas simple,
car la m&eacute;moire interfoli&eacute;e comme les technologies DRAM exotiques
ont tendance &agrave; &ecirc;tre co&ucirc;teuses. Le m&ecirc;me investissement en m&eacute;moire
plus ordinaire vous apporte en g&eacute;n&eacute;ral une m&eacute;moire centrale bien
plus vaste. M&ecirc;me la plus lente des m&eacute;moire DRAM reste autrement
plus rapide que la m&eacute;moire virtuelle par fichier d'&eacute;change&hellip;
</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1034E"></a>2.2.&nbsp;Introduction &agrave; la programmation en m&eacute;moire partag&eacute;e</h3></div></div></div><p>
<span class="foreignphrase"><em class="foreignphrase">Okay</em></span>, donc vous avez d&eacute;cid&eacute; que le traitement 
en parall&egrave;le sur SMP, c'est g&eacute;nial&hellip; Par quoi allez-vous commencer&nbsp;? Eh bien,
la premi&egrave;re &eacute;tape consiste &agrave; en apprendre un peu plus sur le fonctionnement r&eacute;el
de la communication par m&eacute;moire partag&eacute;e.
</p><p>
A premi&egrave;re vue, il suffit qu'un processeur range une valeur en m&eacute;moire
et qu'un autre la lise. Malheureusement, ce n'est pas aussi simple.
Par exemple, les relations entre processus et processeurs sont tr&egrave;s
floues. En revanche, si nous n'avons pas plus de processus actifs que
de processeurs, les termes sont &agrave; peu pr&egrave;s interchangeables. Le reste
de cette section r&eacute;sume bri&egrave;vement les cas de figure typiques qui
peuvent poser de s&eacute;rieux probl&egrave;mes, si vous ne les connaissiez pas
d&eacute;j&agrave;&nbsp;: les deux diff&eacute;rents mod&egrave;les utilis&eacute;s pour d&eacute;terminer ce qui
est partag&eacute;, les probl&egrave;mes d'atomicit&eacute;, le concept de volatilit&eacute;,
les instructions de verrouillage mat&eacute;riel, les effets de la ligne
de cache, et les probl&egrave;mes pos&eacute;s par l'ordonnanceur de
Linux.
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10358"></a>2.2.1.&nbsp;Partage Int&eacute;gral contre Partage Partiel</h4></div></div></div><p>
Il existe deux mod&egrave;les fondamentaux couramment utilis&eacute;s en programmation
en m&eacute;moire partag&eacute;e&nbsp;: le <span class="emphasis"><em>partage int&eacute;gral</em></span>
et le <span class="emphasis"><em>partage partiel</em></span>. Ces mod&egrave;les
permettent tous deux aux processeurs de communiquer en chargeant et
rangeant des donn&eacute;es depuis et dans la m&eacute;moire. La diff&eacute;rence r&eacute;side
dans le fait que le partage int&eacute;gral place toutes les structures en
m&eacute;moire partag&eacute;e, quand le partage partiel, lui, distingue les structures
qui sont potentiellement partageables et celles qui sont
<span class="emphasis"><em>priv&eacute;es</em></span>, propres &agrave; un seul processeur
(et oblige l'utilisateur &agrave; classer explicitement ses structures dans l'une
de ces cat&eacute;gories).
</p><p>
Alors quel mod&egrave;le de partage m&eacute;moire faut-il utiliser&nbsp;? C'est surtout
une affaire de chapelle. Beaucoup de gens aiment le partage int&eacute;gral
car ils n'ont pas sp&eacute;cialement besoin d'identifier les structures qui
doivent &ecirc;tre partag&eacute;es au moment de leur d&eacute;claration. On place simplement
des verrous sur les objets auxquels l'acc&egrave;s peut cr&eacute;er des conflits, pour
s'assurer qu'un seul processeur (ou processus) y acc&egrave;de &agrave; un moment donn&eacute;.
Mais l&agrave; encore, ce n'est pas aussi simple&hellip; aussi beaucoup d'autres
gens pr&eacute;f&egrave;rent, eux, le mod&egrave;le relativement s&ucirc;r du partage partiel.
</p><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N10368"></a>2.2.1.1.&nbsp;Partage int&eacute;gral</h5></div></div></div><p>
Le bon cot&eacute; du partage int&eacute;gral est que l'on peut ais&eacute;ment reprendre
un programme s&eacute;quentiel existant et le convertir progressivement en
programme parall&egrave;le en partage int&eacute;gral. Vous n'avez pas &agrave; d&eacute;terminer
au pr&eacute;alable les donn&eacute;es qui doivent &ecirc;tre accessibles aux autres
processeurs.
</p><p>
Pos&eacute; simplement, le principal probl&egrave;me avec le partage int&eacute;gral vient
du fait qu'une action effectu&eacute;e par un processeur peut affecter les
autres processeurs. Ce probl&egrave;me ressurgit de deux mani&egrave;res&nbsp;:
</p><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Plusieurs biblioth&egrave;ques utilisent des structures de donn&eacute;es qui
ne sont tout simplement pas partageables. Par exemple, la convention
UNIX stipule que la plupart des fonctions peuvent renvoyer un code
d'erreur dans une variable appel&eacute;e <code class="literal">errno</code>.
Si deux processus en partage int&eacute;gral font des appels divers, ils vont
interf&eacute;rer l'un sur l'autre car ils partagent la m&ecirc;me variable
<code class="literal">errno</code>. Bien qu'il existe d&eacute;sormais une
biblioth&egrave;que qui r&egrave;gle le probl&egrave;me de cette variable, ce probl&egrave;me se pr&eacute;sente toujours
dans la plupart des biblioth&egrave;ques comme par exemple X-Window qui, &agrave; moins de prendre
des pr&eacute;cautions tr&egrave;s sp&eacute;ciales, ne fonctionnera pas si diff&eacute;rents appels sont
pass&eacute;s depuis diff&eacute;rents processus en partage int&eacute;gral.
</p></li><li><p>
En temps normal, un programme qui utilise un pointeur ou un index
d&eacute;faillant provoque au pire l'arr&ecirc;t du processus qui contient le code corrompu.
Il peut m&ecirc;me g&eacute;n&eacute;rer un fichier <code class="literal">core</code> vous renseignant sur les conditions
dans lesquelles se sont d&eacute;roul&eacute;s les &eacute;v&eacute;nements. En programmation parall&egrave;le
&agrave; partage int&eacute;gral, il est fort probable que les acc&egrave;s ill&eacute;gaux provoquent
la <span class="emphasis"><em>fin d'un processus qui n'est pas le fautif</em></span>, rendant
la localisation et la correction de l'erreur quasiment impossibles.
</p></li></ul></div>

</p><p>
Aucun de ces deux probl&egrave;mes n'est courant dans le cas du partage
partiel, car seules sont partag&eacute;es les structures explicitement marqu&eacute;es
comme telles. De plus, il est trivial que le partage int&eacute;gral ne peut
fonctionner que si les processeurs ex&eacute;cutent exactement la m&ecirc;me image
en m&eacute;moire. On ne peut pas utiliser le partage int&eacute;gral entre des images
de code diff&eacute;rentes (c'est-&agrave;-dire que vous pourrez travailler en SPMD,
mais pas d'une mani&egrave;re g&eacute;n&eacute;rale en MIMD).
</p><p>

Les supports de programmation en partage int&eacute;gral existent le plus 
couramment sous la forme de <span class="emphasis"><em>biblioth&egrave;ques de 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span></em></span>. Les <a href="http://liinwww.ira.uka.de/bibliography/Os/threads.html" target="_top"><span class="foreignphrase"><em class="foreignphrase">threads</em></span></a> 
sont essentiellement des processus &laquo;&nbsp;<span class="quote">all&eacute;g&eacute;s</span>&nbsp;&raquo; dont 
l'ex&eacute;cution peut ne pas &ecirc;tre planifi&eacute;e comme celle des processus UNIX 
normaux et qui, c'est le plus important, partagent tous la m&ecirc;me page 
m&eacute;moire. L'adaptation des <a href="http://www.mit.edu:8001/people/proven/IAP_2000/index.html" target="_top">Pthreads</a> 
POSIX a fait l'objet de nombreux efforts. La grande question est&nbsp;: 
ces adaptations parall&eacute;lisent-elles les 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span> d'un programme en environnement 
Linux SMP (id&eacute;alement, en attribuant un processeur &agrave; chaque 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span>)&nbsp;?. L'API POSIX ne l'impose 
pas, et certaines versions comme <span class="emphasis"><em>PCthreads</em></span> semblent 
ne pas impl&eacute;menter une ex&eacute;cution en parall&egrave;le des 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span>&nbsp;: tous les 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span> d'un programme sont conserv&eacute;s &agrave; 
l'int&eacute;rieur d'un seul processus Linux.

</p><p>
La premi&egrave;re biblioth&egrave;que de <span class="foreignphrase"><em class="foreignphrase">threads</em></span> &agrave; avoir pris en charge le parall&eacute;lisme
sous Linux SMP fut la d&eacute;sormais quelque peu obsol&egrave;te biblioth&egrave;que
<span class="emphasis"><em>bb_thread</em></span>, une toute petite biblioth&egrave;que qui utilisait l'appel Linux
<code class="literal">clone()</code> pour donner naissance &agrave; de nouveaux
processus Linux, planifi&eacute;s ind&eacute;pendamment les uns des autres, tous partageant
un m&ecirc;me espace d'adressage. Les machines Linux SMP peuvent lancer plusieurs
de ces &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">threads</em></span></span>&nbsp;&raquo; car chaque &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">thread</em></span></span>&nbsp;&raquo; est un processus Linux &agrave; part enti&egrave;re.
L'inconv&eacute;nient, c'est que l'on ne peut obtenir l'ordonnancement &laquo;&nbsp;<span class="quote">poids-plume</span>&nbsp;&raquo;
apport&eacute;e par les biblioth&egrave;ques de <span class="foreignphrase"><em class="foreignphrase">threads</em></span> d'autres syst&egrave;mes
d'exploitation. La biblioth&egrave;que utilisait un peu de code assembleur int&eacute;gr&eacute;
dans un code source en langage C pour mettre en place un bloc de m&eacute;moire pour la
pile de chaque <span class="foreignphrase"><em class="foreignphrase">thread</em></span> et fournir des fonctions d'acc&egrave;s atomiques &agrave; un tableau
de verrous (les <span class="emphasis"><em>mutex</em></span>). Sa documentation se r&eacute;sumait &agrave; un fichier
<code class="literal">LISEZMOI</code> et &agrave; un court programme d'exemple.
</p><p>
Plus r&eacute;cemment, une version de <span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX utilisant <code class="literal">clone()</code>
a &eacute;t&eacute; d&eacute;velopp&eacute;e. Cette biblioth&egrave;que,
<a href="http://pauillac.inria.fr/~xleroy/linuxthreads/" target="_top">LinuxThreads</a>,
est clairement la biblioth&egrave;que en partage int&eacute;gral favorite pour l'utilisation
sous Linux SMP. Les <span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX sont bien document&eacute;s, et les documents
<a href="http://pauillac.inria.fr/~xleroy/linuxthreads/README" target="_top">LinuxThreads&nbsp;README</a>
et <a href="http://pauillac.inria.fr/~xleroy/linuxthreads/faq.html" target="_top">LinuxThreads&nbsp;FAQ</a>
sont vraiment tr&egrave;s bien r&eacute;alis&eacute;s. A pr&eacute;sent, le principal probl&egrave;me est
que les <span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX ont encore beaucoup de d&eacute;tails &agrave; r&eacute;gler, et que
LinuxThread est toujours un projet en cours d'&eacute;volution. D'autre part,
les <span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX ont &eacute;volu&eacute; pendant dans leur phase de standardisation,
aussi devrez-vous &ecirc;tre prudent pour ne pas d&eacute;velopper en suivant une version obsol&egrave;te du standard.
</p></div><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N103EE"></a>2.2.1.2.&nbsp;Partage Partiel</h5></div></div></div><p>

Le Partage Partiel consiste r&eacute;ellement &agrave; &laquo;&nbsp;<span class="quote">ne partager que ce qui 
doit &ecirc;tre partag&eacute;</span>&nbsp;&raquo;. Cette approche est valable pour le MIMD en 
g&eacute;n&eacute;ral (et pas simplement le SPMD) &agrave; condition de prendre soin 
d'allouer les objets partag&eacute;s aux m&ecirc;mes endroits dans le plan m&eacute;moire de 
chaque processeur. Plus important encore, le partage partiel facilite 
l'estimation et l'ajustage des performances, le d&eacute;bogage des sources, et 
c&aelig;tera. Les seuls probl&egrave;mes sont&nbsp;:

</p><p>

<div class="itemizedlist"><ul type="disc"><li><p>
D&eacute;terminer &agrave; l'avance ce qui doit &ecirc;tre partag&eacute; peut s'av&eacute;rer difficile.
</p></li><li><p>
L'allocation d'objets dans la m&eacute;moire partag&eacute;e peut en fait se r&eacute;v&eacute;ler
malais&eacute;, sp&eacute;cialement en ce qui concerne tout ce qui aurait du &ecirc;tre d&eacute;clar&eacute;
dans la pile. Par exemple, il peut &ecirc;tre n&eacute;cessaire d'allouer explicitement
des objets partag&eacute;s dans des segments de m&eacute;moire s&eacute;par&eacute;s, n&eacute;cessitant des
routines d'allocation m&eacute;moire s&eacute;par&eacute;es, et impliquant l'ajout de pointeurs
et d'indirections suppl&eacute;mentaires &agrave; chaque r&eacute;f&eacute;rence.
</p></li></ul></div>

</p><p>

Actuellement, il existe deux m&eacute;canismes similaires permettant aux 
groupes de processus sous Linux de poss&eacute;der des espaces m&eacute;moire 
ind&eacute;pendants, mais de tous partager un unique et relativement &eacute;troit 
segment de m&eacute;moire. En supposant que vous n'ayez pas b&ecirc;tement exclu 
l'option &laquo;&nbsp;<span class="quote">System V IPC</span>&nbsp;&raquo; lorsque que vous avez configur&eacute; 
votre syst&egrave;me Linux (N.D.T.&nbsp;: ici &agrave; la recompilation du noyau), Linux 
g&egrave;re un m&eacute;canisme tr&egrave;s portable devenu c&eacute;l&egrave;bre sous le nom de 
&laquo;&nbsp;<span class="quote">m&eacute;moire partag&eacute;e System V</span>&nbsp;&raquo;. L'autre alternative est une 
fonction de projection en m&eacute;moire dont l'impl&eacute;mentation varie grandement 
selon le syst&egrave;me UNIX utilis&eacute;&nbsp;: L'appel syst&egrave;me 
<code class="literal">mmap</code>. Vous pouvez &mdash; et devriez &mdash; 
apprendre le fonctionnement de ces primitives au travers des pages du 
manuel (<span class="foreignphrase"><em class="foreignphrase">man pages</em></span>)&hellip; mais vous 
trouverez quand m&ecirc;me un rapide aper&ccedil;u de chacune d'elles dans les 
sections 2.5 et 2.6 pour vous aider &agrave; d&eacute;marrer.

</p></div></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1040F"></a>2.2.2.&nbsp;Atomicit&eacute; et ordonnancement</h4></div></div></div><p>
Que vous utilisiez l'un ou l'autre des mod&egrave;les cit&eacute;s ci-dessus, le
r&eacute;sultat est &agrave; peu pr&egrave;s le m&ecirc;me&nbsp;: vous obtenez un pointeur sur un bloc de
m&eacute;moire en lecture/&eacute;criture accessible par tous les processus de votre
programme en parall&egrave;le. Cela signifie-t-il que je peux laisser mes
programmes acc&eacute;der aux objets partag&eacute;s comme s'ils se trouvaient en
m&eacute;moire locale ordinaire&nbsp;? Pas tout &agrave; fait&hellip;
</p><p>
L'<span class="emphasis"><em>atomicit&eacute;</em></span> d&eacute;signe une op&eacute;ration sur
un objet effectu&eacute;e en une s&eacute;quence indivisible et ininterruptible.
Malheureusement, les acc&egrave;s &agrave; la m&eacute;moire partag&eacute;e n'impliquent pas que
les toutes les op&eacute;rations sur les donn&eacute;es de cette m&eacute;moire se fassent
de mani&egrave;re atomique. A moins de prendre des pr&eacute;cautions sp&eacute;ciales, seules les
op&eacute;rations de lecture ou d'&eacute;criture s'accomplissant en une seule transaction
sur le bus (c'est-&agrave;-dire align&eacute;es sur une adresse multiple de 8, 16 ou 32
bits, &agrave; l'exclusion des op&eacute;rations 64 bits ou mal align&eacute;es) sont atomiques. Pire encore,
les compilateurs &laquo;&nbsp;<span class="quote">intelligents</span>&nbsp;&raquo; comme GCC font souvent des optimisations qui
peuvent &eacute;liminer les op&eacute;rations m&eacute;moire n&eacute;cessaires pour s'assurer que les autres processeurs
puissent voir ce que le processeur concern&eacute; a fait. Heureusement, ces probl&egrave;mes ont tous
deux une solution&hellip; en acceptant seulement de ne pas se soucier de la
relation entre l'efficacit&eacute; des acc&egrave;s m&eacute;moire et la taille de la ligne de
cache.
</p><p>
En revanche, avant de traiter de ces diff&eacute;rents cas de figure, il est
utile de pr&eacute;ciser que tout ceci part du principe que les r&eacute;f&eacute;rences &agrave;
la m&eacute;moire pour chaque processeur se produisent dans l'ordre o&ugrave; elles
ont &eacute;t&eacute; programm&eacute;es. Le Pentium fonctionne de cette mani&egrave;re, mais les
futurs processeurs d'Intel pourraient ne pas le faire. Aussi, quand
vous d&eacute;velopperez sur les processeurs &agrave; venir, gardez &agrave; l'esprit qu'il
pourrait &ecirc;tre n&eacute;cessaire d'encadrer les acc&egrave;s &agrave; la m&eacute;moire avec des
instructions provoquant l'ach&egrave;vement de toutes les acc&egrave;s &agrave; la m&eacute;moire
en suspens, provoquant ainsi leur mise en ordre.
L'instruction <code class="literal">CPUID</code> semble provoquer
cet effet.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10422"></a>2.2.3.&nbsp;Volatilit&eacute;</h4></div></div></div><p>
Pour &eacute;viter que l'optimiseur du GCC ne conserve les valeurs de la
m&eacute;moire partag&eacute;e dans les registres de processeur, tous les objets
en m&eacute;moire partag&eacute;e doivent &ecirc;tre d&eacute;clar&eacute;s avec l'attribut
<code class="literal">volatile</code>. Tous les acc&egrave;s en lecture ou &eacute;criture
ne n&eacute;cessitant l'acc&egrave;s qu'&agrave; un seul mot se feront alors de mani&egrave;re
atomique. Par exemple, en supposant que <span class="emphasis"><em>p</em></span> est un pointeur
sur un entier, et que ce pointeur comme l'entier qu'il pointe se trouvent en
m&eacute;moire partag&eacute;e, la d&eacute;claration en C ANSI ressemblera &agrave;&nbsp;:
</p><p>

<pre class="programlisting">
volatile int * volatile p;
</pre>

</p><p>

Dans ce code, le premier <code class="literal">volatile</code> concerne 
l'<code class="literal">int</code> que <code class="literal">p</code> pointe 
&eacute;ventuellement, quand le second <code class="literal">volatile</code> s'applique 
au pointeur lui-m&ecirc;me. Oui, c'est ennuyeux, mais c'est le prix &agrave; payer 
pour que GCC puisse faire des optimisations vraiment puissantes. En 
th&eacute;orie, l'option <code class="literal">-traditional</code> devrait suffire &agrave; 
produire du code correct au prix de quelques optimisations, car le 
standard C K&amp;R (N.D.T.&nbsp;: Kernigan &amp; Ritchie) pr&eacute; norme ANSI 
&eacute;tablit que toutes les variables sont volatiles si elles ne sont pas 
explicitement d&eacute;clar&eacute;es comme <code class="literal">register</code>. Ceci &eacute;tant 
dit, si vos compilations GCC ressemblent &agrave; <code class="literal">cc -O6 
&hellip;</code>, vous n'aurez r&eacute;ellement besoin de d&eacute;clarer les 
choses comme &eacute;tant volatiles qu'aux endroits o&ugrave; c'est n&eacute;cessaire.

</p><p>
Un rumeur a circul&eacute; &agrave; propos du fait que les verrous &eacute;crits en assembleur
signal&eacute;s comme modifiant tous les registres du processeur provoquaient de
la part du compilateur GCC l'enregistrement ad&eacute;quat de toutes les variables en
suspens, &eacute;vitant ainsi le code compil&eacute; &laquo;&nbsp;<span class="quote">inutile</span>&nbsp;&raquo; associ&eacute; aux objets d&eacute;clar&eacute;s
<code class="literal">volatile</code>. Cette astuce semble fonctionner pour
les variables globales statiques avec la version 2.7.0 de GCC&hellip; En revanche,
ce comportement n'est <span class="emphasis"><em>pas</em></span> une recommandation du standard
C ANSI. Pire encore, d'autres processus n'effectuant que des acc&egrave;s en lecture
pourraient conserver &eacute;ternellement les valeurs dans des registres, et ainsi
ne <span class="emphasis"><em>jamais</em></span> s'apercevoir que la vraie valeur stock&eacute;e en
m&eacute;moire partag&eacute;e a en fait chang&eacute;. En r&eacute;sum&eacute;, d&eacute;veloppez comme vous l'entendez,
mais seules les variables d&eacute;clar&eacute;es <code class="literal">volatile</code>
offrent un fonctionnement normal <span class="emphasis"><em>garanti</em></span>.
</p><p>
Notez qu'il est possible de provoquer un acc&egrave;s volatile &agrave; une variable
ordinaire en utilisant un transtypage (&laquo;&nbsp;<span class="quote"><span class="emphasis"><em>casting</em></span></span>&nbsp;&raquo;) imposant l'attribut
<code class="literal">volatile</code>. Par exemple, un
<code class="literal">int i;</code> ordinaire peut &ecirc;tre r&eacute;f&eacute;renc&eacute; en tant
que volatile par <code class="literal">*((volatile int *) &amp;i);</code> .
Ainsi, vous pouvez forcer la volatilit&eacute; et les co&ucirc;ts suppl&eacute;mentaires qu'elle engendre
seulement aux endroits o&ugrave; elle est critique.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1047A"></a>2.2.4.&nbsp;Verrous (<span class="foreignphrase"><em class="foreignphrase">Locks</em></span>)</h4></div></div></div><p>

Si vous pensiez que <code class="literal">++i;</code> aurait toujours incr&eacute;ment&eacute; 
une variable <code class="literal">i</code> sans probl&egrave;me, vous allez avoir une 
mauvaise surprise&nbsp;: m&ecirc;me cod&eacute;es en une seule instruction, le 
chargement et l'enregistrement du r&eacute;sultat sont deux transactions 
m&eacute;moire s&eacute;par&eacute;es, et d'autres processeurs peuvent acc&eacute;der &agrave; 
<code class="literal">i</code> entre ces deux transactions. Par exemple, deux 
processus effectuant chacun l'instruction <code class="literal">++i;</code> 
pourraient n'incr&eacute;menter la variable <code class="literal">i</code> que d'une 
unit&eacute; et non deux. Selon le &laquo;&nbsp;<span class="quote">Manuel de l'Architecture et de la 
Programmation</span>&nbsp;&raquo; du Pentium d'Intel, le pr&eacute;fixe 
<code class="literal">LOCK</code> peut &ecirc;tre employ&eacute; pour s'assurer que chacune des 
instructions suivantes soit atomique par rapport &agrave; l'adresse m&eacute;moire &agrave; 
laquelle elles acc&egrave;dent&nbsp;:

</p><p>

<pre class="programlisting">
BTS, BTR, BTC                     mem, reg/imm
XCHG                              reg, mem
XCHG                              mem, reg
ADD, OR, ADC, SBB, AND, SUB, XOR  mem, reg/imm
NOT, NEG, INC, DEC                mem
CMPXCHG, XADD
</pre>

</p><p>
En revanche, il n'est pas conseill&eacute; d'utiliser toutes ces op&eacute;rations.
Par exemple, <code class="literal">XADD</code> n'existait m&ecirc;me pas sur 386,
aussi l'employer en programmation peut poser des probl&egrave;mes de portabilit&eacute;.
</p><p>

L'instruction <code class="literal">XCHG</code> engendre <span class="emphasis"><em>toujours</em></span>
un verrou, m&ecirc;me sans le pr&eacute;fixe <code class="literal">LOCK</code>, et est ainsi
et indiscutablement l'op&eacute;ration atomique favorite pour construire d'autres op&eacute;rations
atomiques de plus haut niveau comme les s&eacute;maphores et les files d'attente partag&eacute;es.
Bien s&ucirc;r, on ne peut pas demander &agrave; GCC de g&eacute;n&eacute;rer cette instruction en &eacute;crivant
simplement du code C. Il vous faudra &agrave; la place &eacute;crire un peu de code assembleur en
ligne<sup>[<a href="#ftn.N104B6" name="N104B6">5</a>]</sup>. En prenant un objet volatile 
<span class="emphasis"><em>obj</em></span> et un registre du processeur 
<span class="emphasis"><em>reg</em></span>, tous deux de type <code class="literal">word</code> 
(longs de 16 bits), le code assembleur GCC sera&nbsp;:

</p><pre class="programlisting">
__asm__ __volatile__ ("xchgl %1,%0"
                      :"=r" (reg), "=m" (obj)
                      :"r" (reg), "m" (obj));
</pre><p>

Quelques exemples de programmes assembleur en ligne utilisant des 
op&eacute;rations bit-&agrave;-bit pour r&eacute;aliser des verrous sont disponibles dans le 
code source de la biblioth&egrave;que bb_threads.

</p><p>

Il est toutefois important de se souvenir que faire des transactions 
m&eacute;moire atomiques a un co&ucirc;t. Une op&eacute;ration de verrouillage engendre des 
d&eacute;lais suppl&eacute;mentaires assez importants et peut retarder l'activit&eacute; 
m&eacute;moire d'autres processeurs, quand des r&eacute;f&eacute;rences ordinaires auraient 
utilis&eacute; le cache local. Les meilleures performances s'obtiennent en 
utilisant les op&eacute;rations atomiques aussi <span class="emphasis"><em>peu</em></span> 
souvent que possible. De plus, ces instructions atomiques IA32 ne sont 
&eacute;videment pas portables vers d'autres syst&egrave;mes.

</p><p>

Il existe plusieurs alternatives permettant aux instructions ordinaires 
d'&ecirc;tre utilis&eacute;es pour mettre en &#339;uvre diff&eacute;rents types de 
synchronisation, y compris l'<span class="emphasis"><em>exclusion mutuelle</em></span>, 
qui garantit qu'au plus un seul processeur met &agrave; jour un objet partag&eacute; 
donn&eacute; &agrave; un moment pr&eacute;cis. La plupart des manuels des diff&eacute;rents syst&egrave;mes 
d'exploitation traitent d'au moins une de ces techniques. On trouve un 
tr&egrave;s bon expos&eacute; sur le sujet dans la quatri&egrave;me &eacute;dition des 
<span class="foreignphrase"><em class="foreignphrase">Operating System Concepts</em></span> (Principes des 
Syst&egrave;mes d'Exploitation), par Abraham Silberschatz et Peter B. Galvin, 
ISBN 0-201-50480-4.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N104D6"></a>2.2.5.&nbsp;Taille de la ligne de cache</h4></div></div></div><p>

Encore une chose fondamentale concernant l'atomicit&eacute; et qui peut avoir 
des cons&eacute;quence dramatiques sur les performances d'un SMP&nbsp;: la 
taille de la ligne de cache. M&ecirc;me si le standard MPS impose que les 
r&eacute;f&eacute;rences soient coh&eacute;rentes quelque soit le cache utilis&eacute;, il n'en 
reste pas moins que lorsque qu'un processeur &eacute;crit sur une ligne 
particuli&egrave;re de la m&eacute;moire, chaque copie en cache de l'ancienne ligne 
doit &ecirc;tre invalid&eacute;e ou mise &agrave; jour. Ceci implique que si au moins deux 
processeurs &eacute;crivent chacun sur des portions diff&eacute;rentes de la ligne de 
cache, cela peut provoquer un trafic important sur le bus et le cache, 
pour au final transf&eacute;rer la ligne depuis le cache vers le cache. Ce 
probl&egrave;me est connu sous le nom de <span class="emphasis"><em>faux partage</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">false sharing</em></span></span>&nbsp;&raquo;). La 
solution consiste uniquement &agrave; <span class="emphasis"><em>organiser les donn&eacute;es de telle 
mani&egrave;re que ce que les objets auxquels on acc&egrave;de en parall&egrave;le 
proviennent globalement de diff&eacute;rentes lignes de cache pour chaque 
processus</em></span>.

</p><p>

Vous pourriez penser que le faux partage n'est pas un probl&egrave;me quand on 
utilise un cache de niveau 2 partag&eacute;, mais souvenez-vous qu'il existe 
toujours des caches de niveau 1 s&eacute;par&eacute;s. L'organisation du cache et le 
nombre de niveaux s&eacute;par&eacute;s peut varier, mais la ligne de cache de premier 
niveau d'un Pentium est longue de 32 octets, et le cache externe typique 
tourne autour de 256 octets. Supposons que les adresses (physiques ou 
logiques) de deux objets soient <span class="emphasis"><em>a</em></span> et 
<span class="emphasis"><em>b</em></span>, et que la taille de la ligne de cache soit 
<span class="emphasis"><em>c</em></span>, que nous admettrons &ecirc;tre une puissance de 2. 
Pour &ecirc;tre tr&egrave;s pr&eacute;cis, si

<code class="literal">((int) a) &amp; &#732;(c-1)</code> est &eacute;gal &agrave; 
<code class="literal">((int) b) &amp; &#732;(c-1)</code>,

alors les deux r&eacute;f&eacute;rences se trouvent dans la m&ecirc;me ligne de cache. Une 
r&egrave;gle plus simple consiste &agrave; dire que si deux objets r&eacute;f&eacute;renc&eacute;s en 
parall&egrave;le sont &eacute;loign&eacute;s d'au moins <span class="emphasis"><em>c</em></span> octets, ils 
devraient se trouver dans des lignes de cache diff&eacute;rentes.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N104FB"></a>2.2.6.&nbsp;Les probl&egrave;mes de l'ordonnanceur de Linux</h4></div></div></div><p>

Bien que tout l'int&eacute;r&ecirc;t d'utiliser de la m&eacute;moire partag&eacute;e pour les 
traitements en parall&egrave;le consiste &agrave; &eacute;viter les d&eacute;lais dus au syst&egrave;me 
d'exploitation, ces d&eacute;lais peuvent parfois provenir d'autres choses que 
les communications en elles-m&ecirc;mes. Nous avons d&eacute;j&agrave; remarqu&eacute; que le 
nombre de processus que l'on devrait cr&eacute;er doit &ecirc;tre inf&eacute;rieur ou &eacute;gal 
au nombre de processeurs de la machine. Mais comment d&eacute;cide-t-on 
exactement du nombre de processus &agrave; cr&eacute;er&nbsp;?

</p><p>

Pour obtenir les meilleures performances, <span class="emphasis"><em>le nombre de 
processus de votre programme en parall&egrave;le doit &ecirc;tre &eacute;gal au nombre de 
processus qui peuvent &ecirc;tre ex&eacute;cut&eacute;s simultan&eacute;ment, chacun sur son 
processeur</em></span>. Par exemple, si un syst&egrave;me SMP &agrave; quatre 
processeurs h&eacute;berge un processus tr&egrave;s actif pour un autre usage (par 
exemple un serveur <span class="foreignphrase"><em class="foreignphrase">web</em></span>), alors votre 
programme en parall&egrave;le ne devra utiliser que trois processus. Vous 
pouvez vous faire une id&eacute;e g&eacute;n&eacute;rale du nombre de processus actifs 
ex&eacute;cut&eacute;s sur votre syst&egrave;me en consultant la &laquo;&nbsp;<span class="quote">charge 
syst&egrave;me moyenne</span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">load 
average</em></span></span>&nbsp;&raquo;) mise en &eacute;vidence par la commande 
<code class="literal">uptime</code>.

</p><p>

Vous pouvez en outre &laquo;&nbsp;<span class="quote">pousser</span>&nbsp;&raquo; la priorit&eacute; de vos processus 
de votre programme parall&egrave;le en utilisant, par exemple, la commande 
<code class="literal">renice</code> ou l'appel syst&egrave;me <code class="literal">nice()</code>. 
Vous devez &ecirc;tre privil&eacute;gi&eacute;<sup>[<a href="#ftn.N10520" name="N10520">6</a>]</sup> pour augmenter la priorit&eacute; d'un processus. L'id&eacute;e 
consiste simplement &agrave; &eacute;jecter les autres programmes des autres 
processeurs pour que votre programme puisse &ecirc;tre ex&eacute;cut&eacute; sur tous les 
processeurs simultan&eacute;ment. Ceci peut &ecirc;tre effectu&eacute; de mani&egrave;re un peu 
plus explicite en utilisant la version prototype de Linux SMP disponible 
sur <a href="http://www.fsmlabs.com/products/openrtlinux/" target="_top">http://www.fsmlabs.com/products/openrtlinux/</a> et qui 
propose un ordonnanceur en temps r&eacute;el (N.D.T.&nbsp;: il existe d&eacute;sormais un 
guide consacr&eacute; &agrave; RTLinux, accessible en ligne&nbsp;: <a href="http://www.traduc.org/docs/howto/lecture/RTLinux-HOWTO.html" target="_top">RTLinux 
HOWTO</a>).

</p><p>

Si vous n'&ecirc;tes pas le seul utilisateur employant votre syst&egrave;me SMP comme 
une machine en parall&egrave;le, il se peut que vous entriez en conflit avec 
les autres programmes en parall&egrave;le essayant de s'ex&eacute;cuter simultan&eacute;ment. 
La solution standard est l'<span class="emphasis"><em>ordonnancement de groupe</em></span> 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">gang scheduling</em></span></span>&nbsp;&raquo;), 
c'est-&agrave;-dire la manipulation de la priorit&eacute; d'ordonnancement de fa&ccedil;on &agrave; 
ce que seuls les processus d'un seul programme en parall&egrave;le s'ex&eacute;cutent 
&agrave; un moment donn&eacute;. Il est bon de rappeler, en revanche, que multiplier 
les parall&eacute;lismes tend &agrave; r&eacute;duire les retours et que l'activit&eacute; de 
l'ordonnanceur introduit des d&eacute;lais suppl&eacute;mentaires. Ainsi, par exemple, 
il sera s&ucirc;rement pr&eacute;f&eacute;rable, pour une machine &agrave; quatre processeurs, 
d'ex&eacute;cuter deux programmes contenant chacun deux processus, plut&ocirc;t que 
d'ordonnancer en groupe deux programmes de quatre processus chacun.

</p><p>

Il y a encore une chose dont il faut tenir compte. Supposons que vous 
d&eacute;veloppiez un programme sur une machine tr&egrave;s sollicit&eacute;e le jour, mais 
disponible &agrave; cent pour cent pendant la nuit pour le traitement en 
parall&egrave;le. Il vous faudra &eacute;crire et tester votre code dans les 
conditions r&eacute;elles, donc avec tous ses processus lanc&eacute;s, m&ecirc;me en sachant 
que des tests de jour risquent d'&ecirc;tre lents. Ils seront en fait 
<span class="emphasis"><em>tr&egrave;s</em></span> lents si certains de vos processus sont en 
&eacute;tat d'<span class="emphasis"><em>attente active</em></span> (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">busy 
waiting</em></span></span>&nbsp;&raquo;)<sup>[<a href="#ftn.N10544" name="N10544">7</a>]</sup>, guettant le changement de certaines valeurs en 
m&eacute;moire partag&eacute;e, changement cens&eacute; &ecirc;tre provoqu&eacute; par d'autres processus 
qui ne sont pas ex&eacute;cut&eacute;s (sur d'autres processeurs) au m&ecirc;me moment. Ce 
m&ecirc;me probl&egrave;me appara&icirc;t lorsque l'on d&eacute;veloppe et que l'on teste un 
programme sur un syst&egrave;me monoprocesseur.

</p><p>

La solution consiste &agrave; int&eacute;grer des appels syst&egrave;me &agrave; votre code l&agrave; o&ugrave; il 
peut se mettre en boucle en attendant une action d'un autre processeur, 
pour que Linux puisse donner une chance de s'ex&eacute;cuter &agrave; un autre 
processus. J'utilise pour cela une macro en langage C, appelons-la 
<code class="literal">IDLE_ME</code> (N.D.T.&nbsp;: 
<code class="literal">MetsMoiEnAttente</code>)&nbsp;: pour faire un simple test, 
compilez votre programme par

&laquo;&nbsp;<span class="quote"><code class="literal">cc -DIDLE_ME=usleep(1);&hellip;</code></span>&nbsp;&raquo;.

Pour produire un ex&eacute;cutable d&eacute;finitif, utilisez

&laquo;&nbsp;<span class="quote"><code class="literal">cc -DIDLE_ME={}&hellip;</code></span>&nbsp;&raquo;.

L'appel <code class="literal">usleep(1)</code> r&eacute;clame une pause d'une 
microseconde, qui a pour effet de permettre &agrave; l'ordonnanceur de Linux de 
choisir un nouveau processus &agrave; ex&eacute;cuter sur ce processeur. Si le nombre 
de processus d&eacute;passe le nombre de processeurs disponibles, il n'est pas 
rare de voir des programmes s'ex&eacute;cuter dix fois plus rapidement avec 
<code class="literal">usleep(1)</code> que sans.

</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10564"></a>2.3.&nbsp;bb_threads</h3></div></div></div><p>

La biblioth&egrave;que bb_threads (<span class="foreignphrase"><em class="foreignphrase">"Bare Bones" 
threads</em></span>) est une biblioth&egrave;que remarquablement simple qui 
fait la d&eacute;monstration de l'utilisation de l'appel syst&egrave;me Linux 
<code class="literal">clone()</code>. Le fichier <code class="literal">tar.gz</code> 
n'occupe que 7&nbsp;ko&nbsp;! Bien que cette biblioth&egrave;que ait &eacute;t&eacute; rendue 
pour l'essentiel obsol&egrave;te par la biblioth&egrave;que LinuxThreads, trait&eacute;e dans 
la section 2.4, bb_threads reste utilisable, et est suffisamment simple 
et peu encombrante pour former une bonne introduction &agrave; la gestion des 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span> sous Linux. Il est beaucoup moins 
effrayant de se lancer dans la lecture de ce code source que dans celui 
de LinuxThreads. En r&eacute;sum&eacute;, la biblioth&egrave;que bb_threads forme un bon 
point de d&eacute;part, mais n'est pas vraiment adapt&eacute;e &agrave; la r&eacute;alisation de 
grands projets.

</p><p>

La structure de base des programmes utilisant la biblioth&egrave;que bb_threads 
est la suivante&nbsp;:

</p><div class="orderedlist"><ol type="1"><li><p>

Lancez le programme en tant que processus unique.

</p></li><li><p>

Il vous faudra estimer l'espace maximum dans la pile qui sera n&eacute;cessaire 
&agrave; chaque <span class="foreignphrase"><em class="foreignphrase">thread</em></span>. Pr&eacute;voir large est 
relativement sage (c'est &agrave; &ccedil;&agrave; que sert la m&eacute;moire virtuelle ;-), mais 
souvenez-vous que <span class="emphasis"><em>toutes</em></span> les piles proviennent d'un 
seul espace d'adressage virtuel, aussi voir trop grand n'est pas une 
id&eacute;e formidable. La d&eacute;mo sugg&egrave;re 64Ko. Cette taille est fix&eacute;e &agrave; 
<span class="emphasis"><em>b</em></span> octets par 
<code class="literal">bb_threads_stacksize(b)</code>.

</p></li><li><p>

L'&eacute;tape suivante consiste &agrave; initialiser tous les verrous dont vous aurez 
besoin. Le m&eacute;canisme de verrouillage int&eacute;gr&eacute; &agrave; cette biblioth&egrave;que 
num&eacute;rote les verrous de 0 &agrave; <code class="literal">MAX_MUTEXES</code>, et 
initialise un verrou <span class="emphasis"><em>i</em></span> par 
<code class="literal">bb_threads_mutexcreate(i)</code>.

</p></li><li><p>

La cr&eacute;ation d'un nouveau <span class="foreignphrase"><em class="foreignphrase">thread</em></span> 
s'effectue en appelant une routine de la biblioth&egrave;que recevant en 
arguments la fonction que le nouveau 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span> doit ex&eacute;cuter, et les arguments 
qui doivent lui &ecirc;tre transmis. Pour d&eacute;marrer un nouveau 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span> ex&eacute;cutant la fonction 
<span class="emphasis"><em>f</em></span> de type <code class="literal">void</code> et attendant un 
argument <span class="emphasis"><em>arg</em></span>, l'appel ressemblera &agrave; 
<code class="literal">bb_threads_newthread (f, &amp;arg)</code>, o&ugrave; 
<span class="emphasis"><em>f</em></span> devra &ecirc;tre d&eacute;clar&eacute; comme suit&nbsp;:

</p><pre class="programlisting">
void f (void *arg, size_t dummy)
</pre><p>

Si vous avez besoin de passer plus d'un argument &agrave; votre fonction, 
utilisez un pointeur sur une structure contenant les valeurs &agrave; 
transmettre.

</p></li><li><p>

Lancement du code en parall&egrave;le, en prenant soin d'utiliser 
<code class="literal">bb_threads_lock(n)</code> et 
<code class="literal">bb_threads_unlock(n)</code> o&ugrave; <span class="emphasis"><em>n</em></span> est un 
entier indiquant le verrou &agrave; utiliser. Notez que les op&eacute;rations de 
verrouillage et d&eacute;verrouillage sont des op&eacute;rations de blocage<sup>[<a href="#ftn.N105CD" name="N105CD">8</a>]</sup> tr&egrave;s primaires et utilisant des instructions 
atomiques de verrouillage du bus, lesquelles peuvent causer des 
interf&eacute;rences d'acc&egrave;s &agrave; la m&eacute;moire, et qui n'essaient en aucun cas 
d'agir &laquo;&nbsp;<span class="quote">proprement</span>&nbsp;&raquo;.

Le programme de d&eacute;monstration fourni avec bb_threads n'utilisait pas 
correctement les verrous pour emp&ecirc;cher <code class="literal">printf()</code> 
d'&ecirc;tre ex&eacute;cut&eacute; depuis les fonctions <code class="literal">fnn</code> et 
<code class="literal">main</code>, et &agrave; cause de cela, la d&eacute;mo ne fonctionne pas 
toujours. Je ne dis pas cela pour d&eacute;molir la d&eacute;mo, mais plut&ocirc;t pour bien 
mettre en &eacute;vidence le fait que ce travail comporte <span class="emphasis"><em>beaucoup de 
pi&egrave;ges</em></span>. Ceci dit, utiliser LinuxThreads ne se r&eacute;v&egrave;le que 
l&eacute;g&egrave;rement plus facile.

</p></li><li><p>

Lorsqu'un <span class="foreignphrase"><em class="foreignphrase">thread</em></span> ex&eacute;cute 
<code class="literal">return</code>, il d&eacute;truit le processus&hellip; mais la pile 
locale n'est pas automatiquement d&eacute;sallou&eacute;e. Pour &ecirc;tre plus pr&eacute;cis, 
Linux ne g&egrave;re pas la d&eacute;sallocation, et l'espace m&eacute;moire n'est pas 
automatiquement rendu &agrave; la liste d'espace libre de 
<code class="literal">malloc()</code>. Aussi, le processus parent doit-il 
r&eacute;cup&eacute;rer l'espace m&eacute;moire de chaque processus fils mort par 
<code class="literal">bb_threads_cleanup(wait(NULL))</code>.

</p></li></ol></div><p>

Le programme suivant, &eacute;crit en langage C, utilise l'algorithme trait&eacute; 
dans la section 1.3 pour calculer la valeur de Pi en utilisant deux 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span> bb_threads.

</p><pre class="programlisting">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/wait.h&gt;
#include "bb_threads.h"

volatile double pi = 0.0;
volatile int intervalles;
volatile int pids[2];      /* Num&eacute;ros de processus Unix des threads */

void
do_pi(void *data, size_t len)
{
  register double largeur, sommelocale;
  register int i;
  register int iproc = (getpid() != pids[0]);

  /* Fixe la largeur des intervalles */
  largeur = 1.0 / intervalles;

  /* Effectue les calculs locaux */
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=2) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;

  /* Obtention des permissions, mise &agrave; jour de Pi, et d&eacute;verrouillage */
  bb_threads_lock(0);
  pi += sommelocale;
  bb_threads_unlock(0);
}

int
main(int argc, char **argv)
{
  /* R&eacute;cup&egrave;re le nombre d'intervalles */
  intervalles = atoi(argv[1]);

  /* Fixe la taille de la pile, et cr&eacute;e le verrou */
  bb_threads_stacksize(65536);
  bb_threads_mutexcreate(0);

  /* cr&eacute;e deux threads ... */
  pids[0] = bb_threads_newthread(do_pi, NULL);
  pids[1] = bb_threads_newthread(do_pi, NULL);

  /* nettoie derri&egrave;re les deux threads */
  /* (forme ainsi une barri&egrave;re de synchro) */

  bb_threads_cleanup(wait(NULL));
  bb_threads_cleanup(wait(NULL));

  /* Affiche le r&eacute;sultat */
  printf("Estimation de la valeur de Pi: %f\n", pi);

  /* Sortie avec code de SUCCES */
  exit(0);
}
</pre></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10601"></a>2.4.&nbsp;LinuxThreads</h3></div></div></div><p>

LinuxThreads (<a href="http://pauillac.inria.fr/~xleroy/linuxthreads/" target="_top">http://pauillac.inria.fr/~xleroy/linuxthreads/</a>) est une 
impl&eacute;mentation assez compl&egrave;te et bien construite en accord avec le 
standard de <span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX 1003.1c. 
Contrairement aux autres adaptations d'impl&eacute;mentations de 
<span class="foreignphrase"><em class="foreignphrase">threads</em></span> POSIX, LinuxThreads utilise 
&eacute;galement l'appel <code class="literal">clone()</code> du noyau Linux, d&eacute;j&agrave; 
employ&eacute; par bb_threads. La compatibilit&eacute; POSIX implique qu'il est 
relativement ais&eacute; de faire l'adaptation de certaines applications 
provenant d'autres syst&egrave;mes, et diff&eacute;rents tutoriels et leur support 
sont disponibles. Bref, c'est incontestablement la biblioth&egrave;que &agrave; 
utiliser pour d&eacute;velopper des applications 
<span class="foreignphrase"><em class="foreignphrase">multi-threads</em></span> &agrave; grande &eacute;chelle sous 
Linux.

</p><p>

La structure de base d'un programme utilisant LinuxThreads suit ce 
mod&egrave;le&nbsp;:

</p><div class="orderedlist"><ol type="1"><li><p>

Lancement du programme en tant que processus unique.

</p></li><li><p>

Initialisation de tous les verrous dont vous aurez besoin.
Contrairement aux verrous de bb_threads qui sont identifi&eacute;s par
des num&eacute;ros, les verrous POSIX sont d&eacute;clar&eacute;s comme des variables de
type <code class="literal">pthread_mutex_t lock</code>.
Utilisez <code class="literal">pthread_mutex_init(&amp;lock,val)</code>
pour initialiser chacun des verrous que vous utiliserez.

</p></li><li><p>

Comme avec bb_threads, la cr&eacute;ation d'un nouveau 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span> se fait par l'appel d'une fonction 
de la biblioth&egrave;que admettant des arguments sp&eacute;cifiant &agrave; leur tour la 
fonction que le nouveau <span class="foreignphrase"><em class="foreignphrase">thread</em></span> doit 
ex&eacute;cuter et les arguments que celle-ci re&ccedil;oit. Cependant, POSIX impose &agrave; 
l'utilisateur la d&eacute;claration d'une variable de type 
<code class="literal">pthread_t</code> pour identifier chaque 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span>. Pour cr&eacute;er un 
<span class="foreignphrase"><em class="foreignphrase">thread</em></span> <code class="literal">pthread_t 
thread</code> ex&eacute;cutant la fonction <code class="literal">f()</code>, on 
appelle <code class="literal">pthread_create(&amp;thread,NULL,f,&amp;arg)</code>.

</p></li><li><p>

Lancement de la partie parall&egrave;le du programme, en prenant soin d'utiliser

<code class="literal">pthread_mutex_lock(&amp;lock)</code> et

<code class="literal">pthread_mutex_unlock(&amp;lock)</code>

comme il se doit.

</p></li><li><p>

Utilisation de <code class="literal">pthread_join(thread,&amp;retval)</code>
apr&egrave;s chaque <span class="foreignphrase"><em class="foreignphrase">thread</em></span> pour tout nettoyer.

</p></li><li><p>

Utilisation de <code class="literal">-D_REENTRANT</code> &agrave; la compilation de votre 
programme en C.

</p></li></ol></div><p>

Voici l'exemple du calcul de Pi en parall&egrave;le, s'appuyant sur 
LinuxThreads. L'algorithme de la section 1.3 est utilis&eacute; et, comme pour 
l'exemple de bb_threads, deux <span class="foreignphrase"><em class="foreignphrase">threads</em></span> 
s'ex&eacute;cutent en parall&egrave;le.

</p><pre class="programlisting">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include "pthread.h"

volatile double pi = 0.0;    /* Approximation de pi (partag&eacute;e) */
pthread_mutex_t pi_lock;     /* Verrou de la variable ci-dessous */
volatile double intervalles; /* Combien d'intervalles ? */

void *
process(void *arg)
{
  register double largeur, sommelocale;
  register int i;
  register int iproc = (*((char *) arg) - '0');

  /* Fixe la largeur */
  largeur = 1.0 / intervalles;

  /* Fais les calculs locaux */
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=2) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;

  /* Verrouille la variable pi en vue d'une mise &agrave; jour,
     effectue la mise &agrave; jour, puis d&eacute;verrouille Pi. */

  pthread_mutex_lock(&amp;pi_lock);
  pi += sommelocale;
  pthread_mutex_unlock(&amp;pi_lock);

  return(NULL);
}

int
main(int argc, char **argv)
{
  pthread_t thread0, thread1;
  void * retval;

  /* R&eacute;cup&egrave;re le nombre d'intervalles */
  intervalles = atoi(argv[1]);

  /* Initialise un verrou sur pi */
  pthread_mutex_init(&amp;pi_lock, NULL);

  /* Cr&eacute;e les deux threads */
  if (pthread_create(&amp;thread0, NULL, process, "0") ||
      pthread_create(&amp;thread1, NULL, process, "1")) {
    fprintf(stderr, "%s: Cr&eacute;ation des threads impossible\n", argv[0]);
    exit(1);
  }

  /* &laquo; Joint &raquo; (d&eacute;truit) les deux threads */
  if (pthread_join(thread0, &amp;retval) ||
      pthread_join(thread1, &amp;retval)) {
    fprintf(stderr, "%s: Erreur &agrave; la fusion des threads\n", argv[0]);
    exit(1);
  }

  /* Affiche le r&eacute;sultat */
  printf("Estimation de la valeur de Pi: %f\n", pi);

  /* Sortie */
  exit(0);
}
</pre></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1066C"></a>2.5.&nbsp;La m&eacute;moire partag&eacute;e de System V</h3></div></div></div><p>

La gestion des IPC (<span class="foreignphrase"><em class="foreignphrase">Inter-Process 
Communication</em></span>) System V s'effectue au travers d'un 
certain nombre d'appels syst&egrave;me fournissant les m&eacute;canismes des files de 
message, des s&eacute;maphores et de la m&eacute;moire partag&eacute;e. Bien s&ucirc;r, ces 
m&eacute;canismes ont &eacute;t&eacute; initialement con&ccedil;us pour permettre &agrave; plusieurs 
processus de communiquer au sein d'un syst&egrave;me monoprocesseur. Cela 
signifie n&eacute;anmoins que ces m&eacute;canismes devraient aussi fonctionner dans 
un syst&egrave;me Linux SMP, quelque soit le nombre de processeurs.

</p><p>

Avant d'aller plus loin dans l'utilisation de ces appels, il est 
important de comprendre que m&ecirc;me s'il existe des appels IPC System V 
pour des choses comme les s&eacute;maphores et la transmission de messages, 
vous ne les utiliserez probablement pas. Pourquoi&nbsp;? Parce ces 
fonctions sont g&eacute;n&eacute;ralement lentes et s&eacute;rialis&eacute;es sous Linux SMP. 
Inutile de s'&eacute;tendre.

</p><p>

La marche &agrave; suivre standard pour cr&eacute;er un groupe de processus partageant 
l'acc&egrave;s &agrave; un segment de m&eacute;moire partag&eacute;e est la suivante.

</p><div class="orderedlist"><ol type="1"><li><p>

Lancement du programme en tant que processus unique.

</p></li><li><p>

En temps normal, chaque instance de votre programme en parall&egrave;le devra 
avoir son propre segment de m&eacute;moire partag&eacute;e, aussi vous faudra-t-il 
appeler <code class="literal">shmget()</code> pour cr&eacute;er un nouveau segment de la 
taille souhait&eacute;e. Mais d'autre part, cet appel peut &ecirc;tre utilis&eacute; pour 
r&eacute;cup&eacute;rer l'identifiant d'un segment de m&eacute;moire partag&eacute;e d&eacute;j&agrave; existant. 
Dans les deux cas, la valeur de retour est soit l'identifiant du segment 
de m&eacute;moire partag&eacute;e, soit -1 en cas d'erreur. Par exemple, pour cr&eacute;er un 
segment de m&eacute;moire partag&eacute;e long de <span class="emphasis"><em>b</em></span> octets, on 
passe un appel ressemblant &agrave;

<code class="literal">shmid = shmget(IPC_PRIVATE, b, (IPC_CREAT | 0666))</code>.

</p></li><li><p>

L'&eacute;tape suivante consiste &agrave; attacher ce segment de m&eacute;moire partag&eacute;e au 
processus, c'est-&agrave;-dire l'ajouter &agrave; son plan m&eacute;moire. M&ecirc;me si l'appel 
<code class="literal">shmat()</code> permet au programmeur de sp&eacute;cifier l'adresse 
virtuelle &agrave; laquelle le segment doit appara&icirc;tre, cette adresse doit &ecirc;tre 
align&eacute;e sur une page (plus pr&eacute;cis&eacute;ment &ecirc;tre un multiple de la taille 
d'une page renvoy&eacute;e par <code class="literal">getpagesize()</code>, correspondant 
&agrave; 4096 octets), et recouvrera (prendra le pas sur) tout segment de 
m&eacute;moire s'y trouvant d&eacute;j&agrave;. Ainsi est-il plus sage de laisser le syst&egrave;me 
choisir une adresse. Dans les deux cas, la valeur de retour est un 
pointeur sur l'adresse virtuelle de base du segment fra&icirc;chement install&eacute; 
dans le plan m&eacute;moire. L'instruction correspondante est la 
suivante&nbsp;:

<code class="literal">shmptr = shmat(shmid, 0, 0)</code>.

Remarquez que vous pouvez allouer toutes vos variables statiques dans ce 
segment de m&eacute;moire partag&eacute;e en d&eacute;clarant simplement vos variables 
partag&eacute;es comme &eacute;tant les membres d'une structure de type 
<code class="literal">struct</code>, et en d&eacute;clarant <span class="emphasis"><em>shmptr</em></span> 
comme &eacute;tant un pointeur vers ce type de donn&eacute;es. Avec cette technique, 
une variable partag&eacute;e <span class="emphasis"><em>x</em></span> serait accessible par 
<span class="emphasis"><em>shmptr</em></span><code class="literal">-&gt;</code><span class="emphasis"><em>x</em></span>.

</p></li><li><p>

Comme ce segment de m&eacute;moire partag&eacute;e doit &ecirc;tre d&eacute;truit quand le
dernier processus &agrave; y acc&eacute;der prend fin ou s'en d&eacute;tache, il nous
faut appeler <code class="literal">shmctl()</code> pour configurer
cette action par d&eacute;faut. Le code correspondant ressemble &agrave;
<code class="literal">shmctl(shmid, IPC_RMID, 0)</code>.

</p></li><li><p>

Utiliser l'appel Linux <code class="literal">fork()</code><sup>[<a href="#ftn.N106BE" name="N106BE">9</a>]</sup> pour cr&eacute;er le nombre d&eacute;sir&eacute; de processus. 
Chacun d'eux h&eacute;ritera du segment de m&eacute;moire partag&eacute;e.

</p></li><li><p>

Lorsqu'un processus a fini d'utiliser un segment de m&eacute;moire
partag&eacute;e, il doit s'en d&eacute;tacher. On accomplit cela par un
<code class="literal">shmdt(shmptr)</code>.

</p></li></ol></div><p>

M&ecirc;me avec si peu d'appels syst&egrave;me, une fois le segment de m&eacute;moire 
partag&eacute;e &eacute;tabli, tout changement effectu&eacute; par un processeur sur une 
valeur se trouvant dans cet espace sera automatiquement visible par les 
autres processus. Plus important, chaque op&eacute;ration de communication sera 
exon&eacute;r&eacute;e du co&ucirc;t d'un appel syst&egrave;me.

</p><p>

Ci-apr&egrave;s, un exemple de programme en langage C utilisant les segments de 
m&eacute;moire partag&eacute;e System V. Il calcule Pi, en utilisant les algorithmes 
de la section 1.3.

</p><pre class="programlisting">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;sys/ipc.h&gt;
#include &lt;sys/shm.h&gt;

volatile struct shared { double pi; int lock; } * partage;

inline extern int xchg(register int reg,
volatile int * volatile obj)
{
  /* Instruction atomique d'&eacute;change */
__asm__ __volatile__ ("xchgl %1,%0"
                      :"=r" (reg), "=m" (*obj)
                      :"r" (reg), "m" (*obj));
  return(reg);
}

main(int argc, char **argv)
{
  register double largeur, sommelocale;
  register int intervalles, i;
  register int shmid;
  register int iproc = 0;;

  /* Alloue de la m&eacute;moire partag&eacute;e */
  shmid = shmget(IPC_PRIVATE,
                 sizeof(struct shared),
                 (IPC_CREAT | 0600));
  partage = ((volatile struct shared *) shmat(shmid, 0, 0));
  shmctl(shmid, IPC_RMID, 0);

  /* Fais les inits ... */
  partage-&gt;pi = 0.0;
  partage-&gt;lock = 0;

  /* Cr&eacute;e un fils */
  if (!fork()) ++iproc;

  /* R&eacute;cup&egrave;re le nombre d'intervalles */
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;

  /* Fais les calculs locaux */
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=2) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;

  /* Verrou d'attente atomique, ajout, et d&eacute;verrouillage ... */
  while (xchg((iproc + 1), &amp;(shared-&gt;lock))) ;
  shared-&gt;pi += sommelocale;
  shared-&gt;lock = 0;

  /* Fin du processus fils (barri&egrave;re de synchro) */
  if (iproc == 0) {
    wait(NULL);
    printf("Estimation de pi: %f\n", partage-&gt;pi);
  }

  /* Sortie en bonne et due forme */
  return(0);
}
</pre><p>
Dans cet exemple, j'ai utilis&eacute; l'instruction atomique d'&eacute;change
pour mettre le verrouillage en &#339;uvre. Pour de meilleures performances,
pr&eacute;f&eacute;rez-lui une technique de synchronisation &eacute;vitant les intructions
verrouillant le bus.
</p><p>
Pendant les phases de d&eacute;bogage, il est utile de se souvenir que la
commande <code class="literal">ipcs</code> renvoie la liste des
facilit&eacute;s des IPC System V en cours d'utilisation.
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N106D8"></a>2.6.&nbsp;Projection m&eacute;moire (<span class="foreignphrase"><em class="foreignphrase">Memory Map Call</em></span>)</h3></div></div></div><p>

L'utilisation des appels syst&egrave;me pour acc&eacute;der aux fichiers (les 
entr&eacute;es/sorties) peut revenir cher. En fait, c'est la raison pour 
laquelle il existe une biblioth&egrave;que de gestion des entr&eacute;es/sorties 
g&eacute;rant un tampon dans l'espace utilisateur 
(<code class="literal">getchar()</code>, <code class="literal">fwrite()</code>, et c&aelig;tera). 
Mais les tampons utilisateur ne remplissent pas leur fonction si 
plusieurs processus acc&egrave;dent au m&ecirc;me fichier ouvert en &eacute;criture. La 
solution Unix BSD &agrave; ce probl&egrave;me fut l'ajout d'un appel syst&egrave;me 
permettant &agrave; une portion d'un fichier d'&ecirc;tre projet&eacute;e en m&eacute;moire 
utilisateur, en utilisant principalement les m&eacute;canismes de la m&eacute;moire 
virtuelle pour provoquer les mises &agrave; jour. Le m&ecirc;me m&eacute;canisme a &eacute;t&eacute; 
utilis&eacute; pendant plusieurs ann&eacute;es dans les syst&egrave;mes de Sequent comme base 
de leur gestion du traitement parall&egrave;le en m&eacute;moire partag&eacute;e. En d&eacute;pit de 
commentaires tr&egrave;s n&eacute;gatifs dans la page de manuel (assez ancienne), 
Linux semble correctement effectuer au moins quelques unes des fonctions 
de base, et sait prendre en charge l'usage d&eacute;riv&eacute; de cet appel pour 
projeter un segment anonyme de m&eacute;moire pouvant &ecirc;tre partag&eacute; par 
plusieurs processus.

</p><p>

L'impl&eacute;mentation Linux de l'appel <code class="literal">mmap()</code> est en 
elle-m&ecirc;me une solution int&eacute;gr&eacute;e de remplacement des &eacute;tapes 2, 3 et 4 du 
sch&eacute;ma classique de m&eacute;moire partag&eacute;e System V, mis en &eacute;vidence dans la 
section 2.5. Pour cr&eacute;er un segment de m&eacute;moire partag&eacute;e anonyme&nbsp;:

</p><pre class="programlisting">
shmptr =
    mmap(0,                        /* Le syst&egrave;me choisit l'adresse */
         b,                        /* Taille du segment de m&eacute;moire partag&eacute;e */
         (PROT_READ | PROT_WRITE), /* droits d'acc&egrave;s, peuvent &ecirc;tre rwx */
         (MAP_ANON | MAP_SHARED),  /* anonyme, partag&eacute; */
         0,                        /* descripteur de fichier (inutilis&eacute;) */
         0);                       /* offset fichier (inutilis&eacute;) */
</pre><p>

L'&eacute;quivalent de l'appel de m&eacute;moire partag&eacute;e System V 
<code class="literal">shmdt()</code> est <code class="literal">munmap()</code>&nbsp;:

</p><pre class="programlisting">
munmap(shmptr, b);
</pre><p>

&Agrave; mon avis, on ne gagne pas grand chose &agrave; utiliser 
<code class="literal">mmap()</code> plut&ocirc;t que les m&eacute;canismes de gestion de la 
m&eacute;moire partag&eacute;e de System V.

</p></div></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N10704"></a>3.&nbsp;<span class="foreignphrase"><em class="foreignphrase">Clusters</em></span> de syst&egrave;mes Linux</h2></div></div></div><p>

Cette section tente de donner un aper&ccedil;u de ce qu'est le traitement 
parall&egrave;le en <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> sous Linux. Les 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> sont, actuellement, &agrave; la fois 
l'approche la plus populaire et la plus vari&eacute;e, en s'&eacute;tendant du simple 
r&eacute;seau de stations de travail (<span class="foreignphrase"><em class="foreignphrase">Network Of 
Workstations</em></span>&nbsp;: <span class="emphasis"><em>NOW</em></span>) aux 
machines en parall&egrave;le essentiellement construites sur mesure, et dans 
lesquelles il arrive que l'on trouve comme &eacute;l&eacute;ments des PC sous Linux. Il 
existe &eacute;galement un grand nombre de logiciels pouvant prendre en charge 
le calcul en parall&egrave;le dans des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> 
de machines Linux.

</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1071A"></a>3.1.&nbsp;Pourquoi un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>&nbsp;?</h3></div></div></div><p>

Le traitement en parall&egrave;le au travers d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> offre plusieurs avantages 
majeurs&nbsp;:

</p><div class="itemizedlist"><ul type="disc"><li><p>

Chacune des machines d'un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> peut 
&ecirc;tre un syst&egrave;me complet, utilisable avec une large gamme d'applications 
de calcul. Cela conduit beaucoup de gens &agrave; sugg&eacute;rer l'id&eacute;e que ce type 
de <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> pourrait faire usage de tous 
les &laquo;&nbsp;<span class="quote">cycles machine perdus</span>&nbsp;&raquo; sur les ordinateurs tournant &agrave; 
ne rien faire dans les bureaux. Il n'est pas si facile de r&eacute;cup&eacute;rer ces 
cycles, et cela risque de ralentir l'&eacute;cran de veille de votre coll&egrave;gue, 
mais cela peut &ecirc;tre fait.

</p></li><li><p>

L'explosion actuelle des syst&egrave;mes en r&eacute;seau signifie que la plupart du 
mat&eacute;riel n&eacute;cessaire &agrave; la construction d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> se vend d&eacute;j&agrave; en grande quantit&eacute; 
et aux prix r&eacute;duits inh&eacute;rents &agrave; la &laquo;&nbsp;<span class="quote">grande distribution</span>&nbsp;&raquo;. 
On peut &eacute;conomiser encore un peu plus en partant du principe qu'une 
seule carte vid&eacute;o, un seul moniteur et un seul clavier soient 
n&eacute;cessaires pour chaque <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> (bien 
qu'il vous faille tout de m&ecirc;me les passer d'une machine &agrave; une autre pour 
faire l'installation initiale de Linux, une fois lanc&eacute;, un PC sous Linux 
typique n'a pas besoin de &laquo;&nbsp;<span class="quote">console</span>&nbsp;&raquo;). En comparaison, les 
syst&egrave;mes SMP et processeurs auxiliaires repr&eacute;sentent des march&eacute;s plus 
r&eacute;duits, tendant &agrave; proposer des prix plus &eacute;lev&eacute;s par performances &agrave; 
l'unit&eacute;.

</p></li><li><p>

Les calculateurs en <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> peuvent 
<span class="emphasis"><em>&eacute;voluer en de tr&egrave;s grands syst&egrave;mes</em></span>. Alors qu'il est 
actuellement difficile de trouver un ordinateur SMP &agrave; plus de quatre 
processeurs qui soit compatible avec Linux, la plupart du mat&eacute;riel 
r&eacute;seau disponible permet de b&acirc;tir facilement un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> incluant jusqu'&agrave; 16 machines. 
Avec un peu d'huile de coude, on peut mettre en r&eacute;seau des centaines 
voire des milliers de machines. En fait, Internet tout entier pourrait 
&ecirc;tre assimil&eacute; &agrave; un seul immense <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>.

</p></li><li><p>

Le fait que remplacer une &laquo;&nbsp;<span class="quote">mauvaise machine</span>&nbsp;&raquo; au sein d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> soit une op&eacute;ration triviale 
compar&eacute; &agrave; la r&eacute;paration d'un ordinateur SMP partiellement d&eacute;faillant 
garantit une disponibilit&eacute; bien plus &eacute;lev&eacute;e aux configurations de 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> soign&eacute;es. Cela devient important 
non seulement pour les applications qui ne peuvent tol&eacute;rer des 
interruptions de service trop importantes, mais aussi dans l'utilisation 
en g&eacute;n&eacute;ral de syst&egrave;mes qui contiennent un nombre suffisant de 
processeurs pour que la panne d'une machine en particulier soit assez 
courante (par exemple, m&ecirc;me si la dur&eacute;e moyenne avant la premi&egrave;re panne 
d'un PC est environ deux ans, dans un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de 32 machines, la probabilit&eacute; 
qu'au moins une machine tombe en panne dans les six premiers mois est 
assez &eacute;lev&eacute;e).

</p></li></ul></div><p>

Tr&egrave;s bien. Si les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> sont gratuits 
ou tr&egrave;s peu on&eacute;reux, peuvent devenir tr&egrave;s grands et offrir une haute 
disponibilit&eacute;&hellip; pourquoi tout le monde n'utilise pas un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span>&nbsp;? Eh bien, parce qu'il y a 
aussi des probl&egrave;mes&nbsp;:

</p><div class="itemizedlist"><ul type="disc"><li><p>

&Agrave; quelques exceptions pr&egrave;s, le mat&eacute;riel r&eacute;seau n'est pas con&ccedil;u pour le 
traitement en parall&egrave;le. Typiquement, les temps de latence sont &eacute;lev&eacute;s 
et la bande passante r&eacute;duite compar&eacute;s aux syst&egrave;mes SMP et processeurs 
auxiliaires. Par exemple, si les temps de latence d'un SMP n'exc&egrave;dent 
g&eacute;n&eacute;ralement pas plus de quelques microsecondes, ils atteignent 
couramment des centaines, voire des milliers de microsecondes dans un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span>. La bande passante des 
communications dans un syst&egrave;me SMP d&eacute;passe souvent 100 m&eacute;gaoctets par 
seconde. Bien que le mat&eacute;riel r&eacute;seau le plus rapide (c'est-&agrave;-dire le 
&laquo;&nbsp;<span class="quote">Gigabit Ethernet</span>&nbsp;&raquo;) pr&eacute;sente une vitesse comparable, la 
plupart des r&eacute;seaux sont de 10 &agrave; 1000 fois plus lents.

La performance d'un mat&eacute;riel r&eacute;seau est d&eacute;j&agrave; suffisamment m&eacute;diocre dans 
un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> <span class="emphasis"><em>isol&eacute;</em></span>. Si 
le r&eacute;seau n'est pas isol&eacute; du reste du trafic, ce qui est souvent le cas 
lorsque l'on utilise des &laquo;&nbsp;<span class="quote">machines qui sont en r&eacute;seau</span>&nbsp;&raquo; 
plut&ocirc;t qu'un syst&egrave;me con&ccedil;u pour &ecirc;tre un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span>, les performances peuvent &ecirc;tre 
bien pires encore.

</p></li><li><p>

La gestion logicielle des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> en 
tant que syst&egrave;me unique est tr&egrave;s mince. Par exemple, la commande 
<code class="literal">ps</code> ne fait &eacute;tat que des processus s'ex&eacute;cutant sur un 
seul syst&egrave;me Linux, pas des processus s'ex&eacute;cutant &agrave; travers le 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> Linux entier.

</p></li></ul></div><p>

Moralit&eacute;, les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> ont un tr&egrave;s grand 
potentiel, mais ce potentiel risque d'&ecirc;tre tr&egrave;s difficile &agrave; concr&eacute;tiser 
pour la plupart des applications. La bonne nouvelle, c'est qu'il existe 
un soutien logiciel tr&egrave;s d&eacute;velopp&eacute; pour vous aider &agrave; obtenir de bonnes 
performances avec les programmes adapt&eacute;s &agrave; cet environnement, et qu'il 
existe &eacute;galement des r&eacute;seaux con&ccedil;us sp&eacute;cialement pour &eacute;largir la palette 
de programmes pouvant avoir de bonnes performances.

</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1078F"></a>3.2.&nbsp;Le mat&eacute;riel r&eacute;seau</h3></div></div></div><p>

Les r&eacute;seaux informatiques sont en pleine explosion&hellip; mais vous le 
savez d&eacute;j&agrave;. Un nombre toujours grandissant de technologies et produits 
r&eacute;seau a &eacute;t&eacute; d&eacute;velopp&eacute;, et la plupart d'entre eux sont disponibles sous 
une forme qui peut &ecirc;tre utilis&eacute;e pour faire d'un groupe de machines (des 
PC fonctionnant chacun sous Linux) un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de traitement en parall&egrave;le.

</p><p>

Malheureusement, aucune technologie r&eacute;seau ne r&eacute;sout compl&egrave;tement tous 
les probl&egrave;mes. &Agrave; vrai dire, le nombre d'approches diff&eacute;rentes, en co&ucirc;t 
et en performances, est &agrave; premi&egrave;re vue assez difficile &agrave; croire. Par 
exemple, le co&ucirc;t par machine mise en r&eacute;seau s'&eacute;tend de moins de 5 
dollars jusqu'&agrave; plus de 4000. La bande passante et les temps de latence 
varient aussi selon quatre ordres de grandeur.

</p><p>

Avant d'en apprendre plus sur les sp&eacute;cificit&eacute;s de certains r&eacute;seaux, il 
est important de remarquer que ces choses &eacute;voluent tr&egrave;s fr&eacute;quemment 
(voir <a href="http://www.linux.org.uk/NetNews.html" target="_top">http://www.linux.org.uk/NetNews.html</a> pour avoir des 
nouvelles fra&icirc;ches concernant les r&eacute;seaux sous Linux), et qu'il est tr&egrave;s 
difficile d'obtenir des infos pr&eacute;cises concernant certains r&eacute;seaux.

</p><p>

J'ai plac&eacute; un &laquo;&nbsp;<span class="quote"><span class="emphasis"><em>?</em></span></span>&nbsp;&raquo; aux endroits 
incertains. J'ai aussi pass&eacute; beaucoup de temps &agrave; faire des recherches 
sur ce sujet, mais je reste s&ucirc;r que ce r&eacute;sum&eacute; est plein d'erreurs et 
d'omissions importantes. Si vous avez des corrections ou des ajouts &agrave; y 
apporter, merci de m'envoyer un courrier &eacute;lectronique en anglais &agrave; 
l'adresse suivante&nbsp;: <code class="email">&lt;<a href="mailto:hankd CHEZ engr POINT uky POINT edu">hankd CHEZ engr POINT uky POINT edu</a>&gt;</code>.

</p><p>

Les r&eacute;sum&eacute;s comme le <a href="http://www.mcgeoch.com/other/lan-technology.html" target="_top">LAN Technology 
Scorecard</a><sup>[<a href="#ftn.N107AC" name="N107AC">10</a>]</sup> donnent les caract&eacute;ristiques de nombreux et 
diff&eacute;rents types de r&eacute;seaux et de standards de r&eacute;seaux locaux (LAN). 
Cependant, l'essentiel de ce guide pratique est centr&eacute; sur les propri&eacute;t&eacute;s les 
plus indiqu&eacute;es &agrave; la construction d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> Linux. Chaque section d&eacute;crivant 
un type de r&eacute;seau d&eacute;bute par une courte liste de caract&eacute;ristiques. Ci 
dessous, la d&eacute;finition de chaque entr&eacute;e.

</p><div class="variablelist"><dl><dt><span class="term">Prise en charge sous Linux&nbsp;:</span></dt><dd><p>

Si la r&eacute;ponse est <span class="emphasis"><em>non</em></span>, la signification est claire. 
Les autres r&eacute;ponses tentent de d&eacute;crire l'interface de base utilis&eacute;e pour 
acc&eacute;der au r&eacute;seau. La plupart du mat&eacute;riel r&eacute;seau est interfac&eacute; via un 
pilote de p&eacute;riph&eacute;rique du noyau, sachant typiquement g&eacute;rer les 
communications TCP/UDP. D'autres r&eacute;seaux utilisent des interfaces plus 
directes (par exemple des biblioth&egrave;ques) pour r&eacute;duire les temps de 
latence en &eacute;vitant d'avoir &agrave; passer par le noyau.

</p><p>

Il y a quelques ann&eacute;es, il &eacute;tait consid&eacute;r&eacute; comme parfaitement acceptable 
d'acc&eacute;der au coprocesseur math&eacute;matique (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">floating 
point unit</em></span></span>&nbsp;&raquo;) par un appel syst&egrave;me mais 
aujourd'hui, c'est clairement ridicule. &Agrave; mon avis, n&eacute;cessiter un appel 
syst&egrave;me pour chaque communication entre les processeurs ex&eacute;cutant un 
programme en parall&egrave;le est peu commode. Le probl&egrave;me est que les 
ordinateurs n'ont pas encore int&eacute;gr&eacute; ces m&eacute;canismes de communication, et 
que les approches non &laquo;&nbsp;<span class="quote">orient&eacute;es noyau</span>&nbsp;&raquo; tendent &agrave; pr&eacute;senter 
des probl&egrave;mes de portabilit&eacute;. Vous allez entendre beaucoup parler de ce 
sujet dans un avenir proche, principalement sous la forme de la nouvelle 
<span class="emphasis"><em> Virtual Interface (VI) Architecture</em></span> (<a href="http://www.intel.com/intelpress/sum_via.htm" target="_top">http://www.intel.com/intelpress/sum_via.htm</a>) m&eacute;thode 
standardis&eacute;e pour les op&eacute;rations des interfaces r&eacute;seau et servant &agrave; 
contourner les couches des syst&egrave;mes d'exploitation usuels. Le standard 
VI est appuy&eacute; par Compaq, Intel et Microsoft, et aura assur&eacute;ment un 
impact important sur la conception des SAN (<span class="foreignphrase"><em class="foreignphrase">System Area 
Network</em></span>) dans les prochaines ann&eacute;es.

</p></dd><dt><span class="term">Bande passante maximum&nbsp;:</span></dt><dd><p>

C'est le chiffre dont tout le monde se soucie. J'ai g&eacute;n&eacute;ralement repris 
les d&eacute;bits maximum th&eacute;oriques. Votre moyenne, elle, 
<span class="emphasis"><em>va</em></span> varier.

</p></dd><dt><span class="term">Temps de latence minimum&nbsp;:</span></dt><dd><p>

&Agrave; mon avis, c'est le chiffre dont tout le monde devrait se soucier, plus 
encore que de la bande passante. L&agrave; encore, j'ai utilis&eacute; les 
(irr&eacute;alistes) valeurs th&eacute;oriques id&eacute;ales, mais au moins ces nombres 
prennent en compte <span class="emphasis"><em>toutes</em></span> les sources de latence, 
tant logicielles que mat&eacute;rielles. Dans la plupart des cas, les temps de 
latence r&eacute;seau ne durent que quelques microsecondes. Des nombres 
beaucoup plus grands refl&egrave;tent les couches des mat&eacute;riels et logiciels 
inefficaces.

</p></dd><dt><span class="term">Disponibilit&eacute;&nbsp;:</span></dt><dd><p>

Reprise telle quelle, cette ligne d&eacute;crit la forme sous laquelle vous 
pouvez acqu&eacute;rir ce type de mat&eacute;riel. Le mat&eacute;riel de la grande 
distribution est disponible largement et de plusieurs fabricants, le 
prix &eacute;tant alors le premier crit&egrave;re de choix. Les choses propos&eacute;es par 
&laquo;&nbsp;<span class="quote">diff&eacute;rents fabricants</span>&nbsp;&raquo; sont disponibles chez plus d'un 
seul concurrent, mais il existe des diff&eacute;rences significatives, et des 
probl&egrave;mes potentiels d'interop&eacute;rabilit&eacute;. Les r&eacute;seaux produits par un 
&laquo;&nbsp;<span class="quote">fabricant exclusif</span>&nbsp;&raquo; vous laissent &agrave; la merci de ce 
fournisseur, aussi bienveillant soit-il. &laquo;&nbsp;<span class="quote">Domaine public</span>&nbsp;&raquo; 
signifie que m&ecirc;me si vous ne pouvez trouver quelqu'un pour vous vendre 
ce type de mat&eacute;riel, vous ou n'importe qui d'autre pouvez acheter les 
composants et en fabriquer un exemplaire. C'est typiquement le cas des 
prototypes de recherche. Ils ne sont en g&eacute;n&eacute;ral ni pr&ecirc;ts &agrave; &ecirc;tre utilis&eacute;s 
par le public, ni disponibles &agrave; celui-ci.

</p></dd><dt><span class="term">Port ou bus utilis&eacute;&nbsp;:</span></dt><dd><p>

La mani&egrave;re dont on relie l'interface r&eacute;seau &agrave; l'ordinateur. Les 
meilleures performances (et d&eacute;sormais les plus courantes) s'obtiennent 
avec le bus PCI. Il existe aussi des cartes pour bus EISA, VESA local 
bus (VLB), et ISA. ISA fut le premier, et il est toujours utilis&eacute; par 
les cartes aux basses performances. On trouve toujours l'EISA comme bus 
secondaire dans les machines PCI. Ces derniers temps, on ne trouve plus 
beaucoup de mat&eacute;riel &agrave; base de VLB (m&ecirc;me si la <a href="http://www.vesa.org" target="_top">Video Electronics Standards 
Association</a> voit les choses autrement).

</p><p>

Bien s&ucirc;r, toute interface que vous pouvez utiliser sans avoir &agrave; ouvrir 
le bo&icirc;tier de votre PC est plus qu'attrayante. les interfaces IrDA 
(N.D.T.&nbsp;: port infrarouge) et USB font leur apparition de plus en 
plus fr&eacute;quemment. Le Port Parall&egrave;le Standard (SPP) est longtemps rest&eacute; 
&laquo;&nbsp;<span class="quote">ce sur quoi vous branchiez votre imprimante</span>&nbsp;&raquo;, mais s'est 
av&eacute;r&eacute; derni&egrave;rement &ecirc;tre tr&egrave;s utile comme extension du bus ISA. Cette 
nouvelle fonction est am&eacute;lior&eacute;e par le standard IEEE 1284, qui sp&eacute;cifie 
les optimisations EPP et ECP. Il y a aussi le bon vieux port s&eacute;rie 
RS232, lent mais fiable. Je n'ai eu vent d'aucun t&eacute;moignage concernant 
l'interconnexion de machines par le biais des connecteurs vid&eacute;o (VGA), 
clavier, souris ou joystick&hellip;

</p></dd><dt><span class="term">Structure du r&eacute;seau&nbsp;:</span></dt><dd><p>

Un bus est un fil, un ensemble de fils, ou une fibre (optique). Un 
<span class="foreignphrase"><em class="foreignphrase">hub</em></span> (&laquo;&nbsp;<span class="quote">concentrateur</span>&nbsp;&raquo; ou 
&laquo;&nbsp;<span class="quote">plaque tournante</span>&nbsp;&raquo;) est une petite boite qui peut recevoir 
diff&eacute;rents types de fils ou de fibres. Les commutateurs 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">switched hubs</em></span></span>&nbsp;&raquo;) permettent 
&agrave; plusieurs connexions de transmettre activement et simultan&eacute;ment leurs 
donn&eacute;es.

</p></dd><dt><span class="term">Co&ucirc;t par machine reli&eacute;e&nbsp;:</span></dt><dd><p>

Voici comment lire ces nombres. Supposons que, en dehors des connexions 
r&eacute;seau, acheter un PC comme unit&eacute; de votre 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> vous co&ucirc;te 2000 dollars. L'ajout 
de Fast Ethernet porte le prix &agrave; l'unit&eacute; &agrave; environ 2400 dollars. L'ajout 
de Myrinet m&egrave;ne ce prix &agrave; 3800 dollars environ. Si vous avez 20&nbsp;000 
dollars &agrave; d&eacute;penser, vous pouvez alors avoir soit 8 machines reli&eacute;es par 
du Fast Ethernet, soit 5 machines reli&eacute;es par du Myrinet. Il est 
&eacute;galement tr&egrave;s raisonnable d'utiliser plusieurs types de r&eacute;seaux. Par 
exemple, avec 20&nbsp;000 dollars, vous pouvez vous offrir 8 machines 
reli&eacute;es entre elles par Fast Ethernet et TTL_PAPERS. Choisissez un 
r&eacute;seau &mdash; ou un ensemble de r&eacute;seaux &mdash; qui permettra &agrave; votre 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> d'ex&eacute;cuter votre application le 
plus rapidement possible.

</p><p>

Au moment o&ugrave; vous lirez ces lignes, ces chiffres seront faux&hellip; En 
fait, ils le sont s&ucirc;rement d&eacute;j&agrave;. Il peut aussi y avoir un grand nombre 
de r&eacute;ductions, d'offres sp&eacute;ciales, et c&aelig;tera. Mais en tout cas, les prix 
cit&eacute;s ici ne seront jamais suffisamment erron&eacute;s pour vous conduire &agrave; 
faire un choix totalement inappropri&eacute;. Nul besoin d'avoir un doctorat 
(m&ecirc;me si c'est mon cas ;-) pour constater que l'utilisation de r&eacute;seaux 
on&eacute;reux n'a de sens que si votre application a r&eacute;ellement besoin de 
leurs propri&eacute;t&eacute;s ou si les PC utilis&eacute;s dans votre 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> sont eux aussi relativement 
chers.

</p></dd></dl></div><p>
Maintenant que vous &ecirc;tes avertis, place au spectacle&hellip;
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10825"></a>3.2.1.&nbsp;ArcNet</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>2,5 Mbits/s</em></span>
</p></li><li><p>
Temps de latence minimum&nbsp;: <span class="emphasis"><em>1000 microsecondes&nbsp;?</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>
</p></li><li><p>
Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>ISA</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em><span class="foreignphrase"><em class="foreignphrase">Hub</em></span> ou bus non commut&eacute;s (anneau logique)</em></span>
</p></li><li><p>
Co&ucirc;t par machine reli&eacute;e&nbsp;: <span class="emphasis"><em>200 dollars</em></span>
</p></li></ul></div>

</p><p>
ARCNET est un r&eacute;seau local principalement destin&eacute; &agrave; &ecirc;tre utilis&eacute;
dans les syst&egrave;mes de contr&ocirc;le temps r&eacute;el embarqu&eacute;s. Comme Ethernet,
le r&eacute;seau est physiquement organis&eacute; en prises le long d'un bus d'un
ou plusieurs <span class="foreignphrase"><em class="foreignphrase">hubs</em></span>. En revanche, et contrairement
&agrave; Ethernet, il utilise un protocole &agrave; base de jetons qui structure de mani&egrave;re
logique le r&eacute;seau comme un anneau. Les ent&ecirc;tes des paquets sont r&eacute;duites (3 ou 4
octets) et les messages peuvent &ecirc;tre tr&egrave;s courts (jusqu'&agrave; un seul octet).
De fait, ARCNET est plus efficace qu'Ethernet. Malheureusement, il est
aussi plus lent, et moins populaire ce qui le rend plus cher. Vous trouvez
plus d'informations sur le site de l'<a href="http://www.arcnet.com/" target="_top">ARCNET Trade Association</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10861"></a>3.2.2.&nbsp;ATM</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau, biblioth&egrave;ques AAL*</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>155 Mbits/s</em></span> (bient&ocirc;t, <span class="emphasis"><em>1200 Mbits/s</em></span>)
</p></li><li><p>
Temps de latence minimum&nbsp;: <span class="emphasis"><em>120 microsecondes</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>
</p></li><li><p>
Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em><span class="foreignphrase"><em class="foreignphrase">hubs</em></span> commut&eacute;s</em></span>
</p></li><li><p>
Co&ucirc;t par machine reli&eacute;e&nbsp;: <span class="emphasis"><em>3000 dollars</em></span>
</p></li></ul></div>

</p><p>
A moins d'avoir &eacute;t&eacute; dans le coma ces derni&egrave;res ann&eacute;es, vous avez
s&ucirc;rement beaucoup entendu dire qu'ATM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Asynchronous Transfer Mode</em></span></span>&nbsp;&raquo;)
<span class="emphasis"><em>est</em></span> l'avenir&hellip; Eh bien, en quelque sorte.
ATM est meilleur march&eacute; que HiPPI et plus rapide que Fast Ethernet,
et il peut &ecirc;tre utilis&eacute; sur de tr&egrave;s longues distances, ce qui
int&eacute;resse beaucoup les op&eacute;rateurs t&eacute;l&eacute;phoniques. Le protocole du
r&eacute;seau ATM est &eacute;galement con&ccedil;u pour fournir une interface logicielle
aux temps d'acc&egrave;s r&eacute;duits et g&eacute;rant plus efficacement les messages
courts et les communications en temps r&eacute;el (c'est-&agrave;-dire les transmissions
audio et vid&eacute;o num&eacute;riques). C'est aussi l'un des r&eacute;seaux aux plus hauts
d&eacute;bits que Linux prenne actuellement en charge. La mauvaise nouvelle,
c'est qu'ATM n'est pas vraiment bon march&eacute;, et qu'il demeure encore des
probl&egrave;mes de compatibilit&eacute; entre fabricants. Un aper&ccedil;u du d&eacute;veloppement
ATM sous Linux est disponible sur <a href="http://linux-atm.sourceforge.net/" target="_top">http://linux-atm.sourceforge.net/</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N108A4"></a>3.2.3.&nbsp;CAPERS</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>biblioth&egrave;que AFAPI</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>1,2 Mbit/s</em></span>
</p></li><li><p>
Temps de latence maximum&nbsp;: <span class="emphasis"><em>3 microsecondes</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>grande distribution</em></span>
</p></li><li><p>
Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>SPP (port parall&egrave;le)</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>c&acirc;ble entre 2 machines</em></span>
</p></li><li><p>
Co&ucirc;t par machine reli&eacute;e&nbsp;: <span class="emphasis"><em>2 dollars</em></span>
</p></li></ul></div>

</p><p>

CAPERS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Cable Adapter for Parallel Execution and 
Rapid Synchronisation</em></span></span>&nbsp;&raquo;&nbsp;: &laquo;&nbsp;<span class="quote">Adaptateur 
par C&acirc;ble pour l'Ex&eacute;cution en Parall&egrave;le et la Synchronisation 
Rapide</span>&nbsp;&raquo;) est un produit d&eacute;riv&eacute; du projet PAPERS, de 
l'<span class="foreignphrase"><em class="foreignphrase">University School of Electrical and Computer 
Engineering</em></span> de Purdue. En substance, ce projet d&eacute;finit 
un protocole logiciel permettant d'utiliser une simple liaison 
point-&agrave;-point par le port parall&egrave;le, type &laquo;&nbsp;<span class="quote">LapLink</span>&nbsp;&raquo;, pour 
impl&eacute;menter la biblioth&egrave;que PAPERS sur deux PC sous Linux. L'id&eacute;e n'est 
exploitable &agrave; grande &eacute;chelle, mais le prix est imbattable. Tout comme avec 
TTL_PAPERS, pour am&eacute;liorer la s&eacute;curit&eacute; du syst&egrave;me, il est recommand&eacute;, 
mais pas n&eacute;cessaire, d'appliquer un correctif mineur au noyau.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N108E4"></a>3.2.4.&nbsp;Ethernet</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>10 Mbits/s</em></span>
</p></li><li><p>
Latence minimum&nbsp;: <span class="emphasis"><em>100 microsecondes</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>grande distribution</em></span>
</p></li><li><p>
Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>commutateurs ou concentrateurs, ou m&ecirc;me bus simple</em></span>
</p></li><li><p>
Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>100 dollars</em></span> (sans concentrateur, <span class="emphasis"><em>50 dollars</em></span>)
</p></li></ul></div>

</p><p>

Depuis plusieurs ann&eacute;es maintenant, l'Ethernet &agrave; 10Mbits/s est le 
standard des technologies r&eacute;seau. Une bonne carte Ethernet s'ach&egrave;te pour 
moins de 50 dollars, et bon nombre de PC en sont aujourd'hui &eacute;quip&eacute;s de 
s&eacute;rie, l'interface &eacute;tant int&eacute;gr&eacute;e &agrave; la carte-m&egrave;re. Les r&eacute;seaux &agrave; base 
Ethernet dont la charge n'est pas tr&egrave;s &eacute;lev&eacute;e peuvent &ecirc;tre organis&eacute;s 
comme un long bus &agrave; prises multiples sans concentrateur 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">hub</em></span></span>&nbsp;&raquo;). Une telle 
configuration peut servir jusqu'&agrave; 200 machines pour un co&ucirc;t minimal, 
mais n'est pas appropri&eacute;e au traitement en parall&egrave;le. Ajouter un 
concentrateur non commut&eacute; n'am&eacute;liore pas beaucoup les performances. En 
revanche, les commutateurs 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">switches</em></span></span>&nbsp;&raquo;), qui peuvent 
offrir une bande passante maximum &agrave; plusieurs connexions simultan&eacute;ment 
ne co&ucirc;tent qu'aux alentours de 100 dollars par port. Linux prend en 
charge un nombre impressionnant d'interfaces Ethernet diff&eacute;rentes, mais 
il est important de garder &agrave; l'esprit que les variations entre ces 
mat&eacute;riels peuvent engendrer de grandes diff&eacute;rences de performance. 
Consultez le <a href="http://www.traduc.org/docs/howto/lecture/Hardware-HOWTO.html" target="_top">Guide pratique de 
la compatibilit&eacute; mat&eacute;rielle avec Linux</a> (N.D.T.&nbsp;: version 
fran&ccedil;aise du <a href="http://www.tldp.org/HOWTO/Hardware-HOWTO/" target="_top"> 
<span class="foreignphrase"><em class="foreignphrase">Hardware&nbsp;Compatibility&nbsp;HOWTO</em></span> 
</a>) pour plus d'informations concernant les diff&eacute;rents &eacute;quipements 
fonctionnant sous Linux, et pour avoir une id&eacute;e de leur qualit&eacute;.

</p><p>

Le <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> Linux &agrave; 16 machines r&eacute;alis&eacute; 
dans le cadre du projet &laquo;&nbsp;<span class="quote"><a href="http://www.beowulf.org" target="_top">Beowulf</a></span>&nbsp;&raquo; (initialement 
d&eacute;velopp&eacute; au CESDIS de la NASA) est une mani&egrave;re int&eacute;ressante d'augmenter 
ses performances. C'est &agrave; Donald Becker, auteur de plusieurs pilotes de 
cartes Ethernet, que l'on doit la prise en charge de la r&eacute;partition du 
trafic au travers de plusieurs cartes r&eacute;seau s'&eacute;clipsant mutuellement 
(autrement dit, partageant les m&ecirc;mes adresses r&eacute;seau). Cette fonction 
est int&eacute;gr&eacute;e en standard dans Linux, et s'effectue de mani&egrave;re invisible 
en dessous du niveau des op&eacute;rations sur les 
<span class="foreignphrase"><em class="foreignphrase">sockets</em></span>. Le co&ucirc;t d'un hub n'&eacute;tant pas 
n&eacute;gligeable, relier chaque machine &agrave; deux (ou plus) r&eacute;seaux Ethernet, 
sans <span class="foreignphrase"><em class="foreignphrase">hubs</em></span> ni 
<span class="foreignphrase"><em class="foreignphrase">switches</em></span>, pour am&eacute;liorer les performances 
peut s'av&eacute;rer financi&egrave;rement tr&egrave;s rentable. D'une mani&egrave;re g&eacute;n&eacute;rale, dans 
les situations o&ugrave; une machine est le goulet d'&eacute;tranglement d'un r&eacute;seau, 
la r&eacute;partition de charge &agrave; travers plusieurs r&eacute;seaux 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">shadow networks</em></span></span>&nbsp;&raquo;) se 
r&eacute;v&egrave;le bien plus efficace que l'usage d'un r&eacute;seau &eacute;quip&eacute; d'un 
commutateur seul.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10944"></a>3.2.5.&nbsp;Ethernet (Fast Ethernet)</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>
</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>100 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>80 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>grande distribution</em></span>

</p></li><li><p>

Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>concentrateurs ou commutateurs 
(<span class="foreignphrase" lang="en"><em class="foreignphrase">hubs</em></span> ou <span class="foreignphrase" lang="en"><em class="foreignphrase">switches</em></span>)</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>400 dollars (?)</em></span>

</p></li></ul></div>

</p><p>

Bien qu'il existe un certain nombre de technologies diff&eacute;rentes nomm&eacute;es 
&laquo;&nbsp;<span class="quote">Fast Ethernet</span>&nbsp;&raquo;, elles se r&eacute;f&egrave;rent souvent &agrave; un r&eacute;seau 
Ethernet 100 Mbits/s en grande partie compatible avec les anciens c&acirc;bles 
et p&eacute;riph&eacute;riques 10 Mbits/s type &laquo;&nbsp;<span class="quote">10 BaseT</span>&nbsp;&raquo;. Comme on peut 
s'y attendre, tout ce qui s'appelle Ethernet b&eacute;n&eacute;ficie des prix de la 
vente de masse, et ces interfaces ne co&ucirc;tent g&eacute;n&eacute;ralement qu'une 
fraction du prix des cartes ATM &agrave; 155 Mbits/s. Le probl&egrave;me est qu'une 
collection de machines partageant tous le m&ecirc;me &laquo;&nbsp;<span class="quote">bus</span>&nbsp;&raquo; &agrave; 100 
Mbits/s (&agrave; l'aide d'un <span class="foreignphrase"><em class="foreignphrase">hub</em></span> non commut&eacute;) 
peut pr&eacute;senter des performances n'atteignant en moyenne m&ecirc;me pas celles 
d'un r&eacute;seau 10 Mbits/s utilisant un commutateur fournissant &agrave; chaque 
machine une connexion 10 Mbits/s compl&egrave;te.

</p><p>
Les commutateurs pouvant fournir une connexion 100 Mbits &agrave; chaque
machine simultan&eacute;ment sont chers, mais les prix chutent chaque jour,
et ces commutateurs offrent une bande passante autrement plus &eacute;lev&eacute;e
que de simples <span class="foreignphrase"><em class="foreignphrase">hubs</em></span> non commut&eacute;s. Ce qui rend les commutateurs ATM
si on&eacute;reux est la n&eacute;cessit&eacute; de commuter chaque cellule ATM, cellule
relativement courte. Certains commutateurs Ethernet parient sur une
fr&eacute;quence de commutation attendue relativement lente et en tirent
profit en utilisant des techniques aux temps de latence r&eacute;duits &agrave;
l'int&eacute;rieur du commutateur, mais n&eacute;cessitant quelques millisecondes
pour changer de voie. Si l'itin&eacute;raire de votre trafic r&eacute;seau change
fr&eacute;quemment, &eacute;vitez ce type d'&eacute;quipement.
</p><p>
Notez aussi que, comme pour Ethernet, le projet Beowulf
(<a href="http://www.beowulf.org" target="_top">http://www.beowulf.org</a>) de la NASA d&eacute;veloppe des pilotes aux performances sup&eacute;rieures
car utilisant la r&eacute;partition de charge entre plusieurs cartes
Fast Ethernet.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10996"></a>3.2.6.&nbsp;Ethernet (Gigabit Ethernet)</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>1000 Mb/s</em></span>
</p></li><li><p>
Temps de latence minimum&nbsp;: <span class="emphasis"><em>300 microsecondes (?)</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>
</p></li><li><p>
Port ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>
</p></li><li><p>
Structure r&eacute;seau&nbsp;: <span class="emphasis"><em>commutateurs ou FDR</em></span>
</p></li><li><p>
Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>2500 dollars (?)</em></span>
</p></li></ul></div>

</p><p>
Je ne suis pas s&ucirc;r que <a href="http://www.gigabit-ethernet.org/" target="_top">Gigabit Ethernet</a>
ait une raison technologique valable
de s'appeler Ethernet&hellip; mais son nom inspire le fait que
Gigabit Ethernet est con&ccedil;u pour &ecirc;tre une technologie r&eacute;seau
bon march&eacute; et de grande distribution, avec une prise en charge
native de l'IP. En revanche, les prix actuels refl&egrave;tent le
fait que cela reste un produit difficile &agrave; fabriquer.
</p><p>

Contrairement aux autres technologies Ethernet, Gigabit Ethernet apporte 
un contr&ocirc;le du flux, ce qui devrait en faire un r&eacute;seau plus fiable. Les 
FDR, ou &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Full-Duplex 
Repeaters</em></span></span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote">r&eacute;p&eacute;teurs bidirectionnels 
simultan&eacute;s</span>&nbsp;&raquo;), se contentent de multiplexer les lignes, en 
utilisant des m&eacute;moires tampons et des contr&ocirc;les de flux localis&eacute;s pour 
am&eacute;liorer les performances. La plupart des commutateurs sont construits 
comme de nouveaux modules pour les diff&eacute;rents mod&egrave;les de commutateurs 
compatible Gigabit d&eacute;j&agrave; existants. Les commutateurs ou FDR sont 
distribu&eacute;s ou annonc&eacute;s par au moins Acacianet, <a href="http://www.baynetworks.com/" target="_top">Bay networks</a>, <a href="http://www.cabletron.com/" target="_top">Cabletron</a> (d&eacute;sormais Enterasys. 
Page fran&ccedil;aise&nbsp;: <a href="http://www.enterasys.com/fr/" target="_top">http://www.enterasys.com/fr/</a>), 
Networks digital, <a href="http://www.extremenetworks.com/homepage_french.asp" target="_top">Extreme 
networks</a>, <a href="http://www.foundrynet.com/" target="_top">Foundry 
networks</a>, Gigalabs.com, Packet engines, <a href="http://www.plaintree.com/" target="_top">Plaintree systems</a>, <a href="http://www.prominet.com/" target="_top">Prominet</a>, <a href="http://fr.sun.com/" target="_top">Sun microsystems</a>, et Xlnt.

</p><p>

Il existe un pilote pour Linux pour les &laquo;&nbsp;<span class="quote">Yellowfin</span>&nbsp;&raquo; G-NIC 
de Packet Engines<sup>[<a href="#ftn.N109FA" name="N109FA">11</a>]</sup>. Les premiers essais sous Linux ont fait &eacute;tat d'un 
taux de transfert environ 2,5 fois sup&eacute;rieur &agrave; la plus rapide des cartes 
Fast Ethernet &agrave; 100 Mbits/s. Avec les r&eacute;seaux Gigabit, une configuration 
soigneuse du bus PCI est un facteur critique. Il reste toujours un doute 
quant &agrave; la poursuite des am&eacute;liorations de ce pilote, et du d&eacute;veloppement 
des pilotes Linux des autres cartes r&eacute;seau.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N109FE"></a>3.2.7.&nbsp;FC (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Fibre Channel</em></span></span>&nbsp;&raquo;)</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: 
<span class="emphasis"><em>non</em></span><sup>[<a href="#ftn.N10A0B" name="N10A0B">12</a>]</sup>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>1062 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI (?)</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li></ul></div><p>

L'objectif du FC (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Fibre 
Channel</em></span></span>&nbsp;&raquo;) est de fournir un m&eacute;dium 
d'entr&eacute;e/sortie de type bloc aux performances &eacute;lev&eacute;es (une trame FC 
transporte un bloc de donn&eacute;es d'une longueur forfaitaire de 2048 
octets), particuli&egrave;rement adapt&eacute; aux partages de disques et autres 
p&eacute;riph&eacute;riques de stockage, qui peuvent alors &ecirc;tre reli&eacute;s directement au 
r&eacute;seau FC plut&ocirc;t qu'&agrave; travers un ordinateur. Niveau bande passante, le 
FC est pr&eacute;sent&eacute; comme &eacute;tant relativement rapide, avec un taux s'&eacute;tendant 
de 133 &agrave; 1062 Mbits/s. Si le FC tend &agrave; devenir populaire en tant que 
solution haut-de-gamme de remplacement du SCSI, il pourrait rapidement 
devenir une technologie tr&egrave;s abordable. Pour le moment, ce n'est pas 
abordable, et ce n'est pas pris en charge par Linux. La Fibre Channel 
Association tient une importante collection de r&eacute;f&eacute;rences au FC, sur 
<a href="http://www.fibrechannel.org" target="_top">http://www.fibrechannel.org</a><sup>[<a href="#ftn.N10A3B" name="N10A3B">13</a>]</sup>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10A48"></a>3.2.8.&nbsp;FireWire (IEEE 1394)</h4></div></div></div><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>non</em></span><sup>[<a href="#ftn.N10A53" name="N10A53">14</a>]</sup>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>196,608 Mbits/s</em></span> (bient&ocirc;t, <span class="emphasis"><em>393,216 Mbits/s</em></span>)
</p></li><li><p>
Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>
</p></li><li><p>
Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>al&eacute;atoire, sans cycles (auto-configur&eacute;)</em></span>
</p></li><li><p>
Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>600 dollars</em></span>
</p></li></ul></div>

</p><p>
FireWire, ou standard IEEE 1394-1995, est vou&eacute; &agrave; &ecirc;tre le r&eacute;seau num&eacute;rique
&agrave; grande vitesse et &agrave; prix r&eacute;duit des appareils &eacute;lectroniques domestiques.
Son application-phare est la connexion des cam&eacute;scopes num&eacute;riques aux
ordinateurs, mais FireWire est destin&eacute; &agrave; &ecirc;tre utilis&eacute; dans des domaines
s'&eacute;tendant de l'alternative au SCSI jusqu'&agrave; l'interconnexion des diff&eacute;rents
composants de votre &laquo;&nbsp;<span class="quote">Home Cin&eacute;ma</span>&nbsp;&raquo;. Il vous permet de relier plus de
64000 p&eacute;riph&eacute;riques dans une topologie utilisant des bus et des ponts
mais ne formant pas de boucle, et d&eacute;tecte automatiquement la configuration
des p&eacute;riph&eacute;riques lorsqu'ils sont ajout&eacute;s ou retir&eacute;s. Les messages courts
(les &laquo;&nbsp;<span class="quote">quadlets</span>&nbsp;&raquo;, longs de quatre octets) sont &eacute;galement pris en charge,
de m&ecirc;me que les transmissions isochrones sur le mod&egrave;le de l'ATM (utilis&eacute;es
pour pr&eacute;server le synchronisme des messages multim&eacute;dia). Adaptec propose
des produits FireWire permettant de relier jusqu'&agrave; 63 p&eacute;riph&eacute;riques
&agrave; une seule carte PCI, et tient aussi un bon site d'information g&eacute;n&eacute;rale
concernant le FireWire sur
<a href="http://www.adaptec.com/worldwide/product/prodtechindex.html?cat=%2fTechnology%2fFireWire-1394&prodkey=1394_summary" target="_top">http://www.adaptec.com</a>.
</p><p>
Bien que le FireWire ne soit pas le r&eacute;seau le plus rapide disponible
actuellement, le march&eacute; de la grande distribution (qui tire les prix
vers le bas) et les temps de latence r&eacute;duits pourraient en faire d'ici
l'an prochain la meilleure interface r&eacute;seau pour le <span class="emphasis"><em><span class="foreignphrase"><em class="foreignphrase">message passing</em></span></em></span> dans
un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de PC sous Linux.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10A94"></a>3.2.9.&nbsp;HiPPI et Serial HiPPI</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: 
<span class="emphasis"><em>non</em></span><sup>[<a href="#ftn.N10A9D" name="N10A9D">15</a>]</sup>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>1600 Mbits/s</em></span> 
(<span class="emphasis"><em>1200 Mb/s</em></span> pour Serial HiPPI)

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>EISA, PCI</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>commutateurs</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>3500 dollars</em></span> 
(<span class="emphasis"><em>4,500</em></span> dollars pour Serial HiPPI)

</p></li></ul></div><p>

HiPPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">High Performance Parallel 
Interface</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Interface Parall&egrave;le aux 
Performances &Eacute;lev&eacute;es</span>&nbsp;&raquo;) &eacute;tait initialement cens&eacute;e fournir un taux 
de transfert &eacute;lev&eacute; pour l'&eacute;change d'immenses blocs de donn&eacute;es entre un 
supercalculateur et une autre machine (un autre supercalculateur, un 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">frame buffer</em></span></span>&nbsp;&raquo;, une batterie 
de disques, et c&aelig;tera), et est devenu le standard dominant dans le monde 
des supercalculateurs. Bien que ce soit un oxymoron, <span class="emphasis"><em>Serial 
HiPPI</em></span> devient &eacute;galement tr&egrave;s populaire en utilisant 
typiquement de la fibre optique &agrave; la place des c&acirc;bles HiPPI standard de 
32 bits de large (donc parall&egrave;les). Ces derni&egrave;res ann&eacute;es, les 
commutateurs HiPPI en croix sont devenus courants et les prix ont 
s&eacute;rieusement chut&eacute;. Malheureusement, les &eacute;quipement HiPPI S&eacute;rie, eux, 
sont encore tr&egrave;s on&eacute;reux, et sont en g&eacute;n&eacute;ral les seuls pris en charge 
par le bus PCI. Pire, Linux ne g&egrave;re pas encore HiPPI. Le CERN tient une 
bonne pr&eacute;sentation d'HiPPI sur <a href="http://www.cern.ch/HSI/hippi/" target="_top">http://www.cern.ch/HSI/hippi/</a>. Ils tiennent aussi une liste 
assez longue de distributeurs proposant le HiPPI sur <a href="http://www.cern.ch/HSI/hippi/procintf/manufact.htm" target="_top">http://www.cern.ch/HSI/hippi/procintf/manufact.htm</a>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10AE1"></a>3.2.10.&nbsp;IrDA (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Infrared Data Association</em></span></span>&nbsp;&raquo;)</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>non 
(?)</em></span><sup>[<a href="#ftn.N10AEE" name="N10AEE">16</a>]</sup>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>1,15 Mbits/s</em></span> et 
<span class="emphasis"><em>4 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>Diff&eacute;rents fabricants</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>IrDA</em></span>

</p></li><li><p>

Structure r&eacute;seau&nbsp;: <span class="emphasis"><em>Air libre</em></span> ;-)

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>0</em></span>

</p></li></ul></div><p>

L'IrDA (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Infrared Data 
Association</em></span></span>&nbsp;&raquo; ou &laquo;&nbsp;<span class="quote">Association de Donn&eacute;es par 
Infrarouges</span>&nbsp;&raquo;, sur <a href="http://www.irda.org" target="_top">http://www.irda.org</a>), c'est ce 
petit appareil &agrave; infrarouges sur le cot&eacute; des ordinateurs portables. Il 
reste assez difficile, par conception, de relier plus de deux 
ordinateurs par ce biais, aussi l'IrDA ne se pr&ecirc;te-t-il gu&egrave;re &agrave; la 
&laquo;&nbsp;<span class="quote">clusterisation</span>&nbsp;&raquo;, ou mise en parall&egrave;le massive de 
nombreuses machines. Don Becker est toutefois l'auteur de quelques 
travaux pr&eacute;liminaires sur l'IrDA.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10B28"></a>3.2.11.&nbsp;Myrinet</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>biblioth&egrave;ques</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>1280 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>9 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>mat&eacute;riel propri&eacute;taire.</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;s&nbsp;: <span class="emphasis"><em>PCI</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>commutateurs</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>1800 dollars</em></span>

</p></li></ul></div><p>

<a href="http://www.myri.com" target="_top">Myrinet</a> est un r&eacute;seau local 
(LAN&nbsp;: &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Local Area 
Network</em></span></span>&nbsp;&raquo;) con&ccedil;u pour servir &eacute;galement de r&eacute;seau 
syst&egrave;me <sup>[<a href="#ftn.N10B60" name="N10B60">17</a>]</sup>. Les versions LAN et SAN utilisent des m&eacute;dias 
physiques distincts et leur caract&eacute;ristiques sont sensiblement 
diff&eacute;rentes. La version SAN est g&eacute;n&eacute;ralement utilis&eacute;e au sein d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span>.

</p><p>

La structure de Myrinet est tr&egrave;s conventionnelle, mais a la r&eacute;putation
d'&ecirc;tre tr&egrave;s bien impl&eacute;ment&eacute;e. Les pilotes pour Linux sont connus pour
donner de tr&egrave;s bons r&eacute;sultats, bien qu'il e&ucirc;t &eacute;t&eacute; fait &eacute;tat de frappantes
diff&eacute;rences de performances d'une impl&eacute;mentation du bus PCI &agrave; l'autre.

</p><p>

Actuellement, Myrinet est assur&eacute;ment le r&eacute;seau favori des responsables 
de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> n'&eacute;tant pas trop s&eacute;v&egrave;rement 
limit&eacute; au niveau budg&eacute;taire. Si, pour vous, un PC Linux typique est un 
Pentium Pro dernier cri ou un Pentium II avec au moins 256&nbsp;Mo de m&eacute;moire 
vive, et un disque RAID et SCSI, alors le co&ucirc;t de Myrinet appara&icirc;t 
raisonnable. En revanche, avec des machines plus conventionnelles, il 
vous faudra probablement choisir entre relier <span class="emphasis"><em>N</em></span> 
machines avec Myrinet, ou <span class="emphasis"><em>2N</em></span> machines avec 
plusieurs &eacute;quipements de type &laquo;&nbsp;<span class="quote">Fast Ethernet</span>&nbsp;&raquo; ou 
&laquo;&nbsp;<span class="quote">TTL_PAPERS</span>&nbsp;&raquo;. Tout cela d&eacute;pend r&eacute;ellement de votre budget 
et du type de calcul qui vous importe le plus.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10B84"></a>3.2.12.&nbsp;Parastation</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>couches d'abstraction 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">HAL</em></span></span>&nbsp;&raquo;) ou biblioth&egrave;ques 
r&eacute;seau</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>125 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>2 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>fabricant exclusif</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>maillage, sans concentrateur</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>plus de 1000 dollars</em></span>

</p></li></ul></div><p>

Le projet ParaStation (<a href="http://wwwipd.ira.uka.de/parastation" target="_top">http://wwwipd.ira.uka.de/parastation</a>) de la section informatique 
de l'Universit&eacute; de Karlsruhe est en train de mettre sur pieds un r&eacute;seau 
&laquo;&nbsp;<span class="quote">maison</span>&nbsp;&raquo; compatible PVM et aux temps de latence r&eacute;duits. 
Ils ont d'abord construit un prototype de ParaPC biprocesseur en 
utilisant une carte EISA con&ccedil;ue sur mesure et des PC fonctionnant sous 
Unix BSD, puis ont b&acirc;ti de plus grands 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> compos&eacute;s de machines Alpha DEC. 
Depuis Janvier 1997, Parastation est disponible sous Linux. Les cartes 
PCI sont produites en coop&eacute;ration avec une soci&eacute;t&eacute; nomm&eacute;e <a href="http://www.hitex.com" target="_top">Hitex</a>. Le mat&eacute;riel de Parastation 
impl&eacute;mente &agrave; la fois un syst&egrave;me de transmission de messages rapide et 
fiable, et des barri&egrave;res de synchronisation simples.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10BC5"></a>3.2.13.&nbsp;PLIP</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>
</p></li><li><p>
Bande passante maximum&nbsp;: <span class="emphasis"><em>1,2 Mbits/s</em></span>
</p></li><li><p>
Temps de latence minimum&nbsp;: <span class="emphasis"><em>1000 microsecondes&nbsp;?</em></span>
</p></li><li><p>
Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>grande distribution</em></span>
</p></li><li><p>
Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>SPP</em></span>
</p></li><li><p>
Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>c&acirc;ble entre 2 machines</em></span>
</p></li><li><p>
Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>2 dollars</em></span>
</p></li></ul></div><p>

Pour le seul co&ucirc;t d'un c&acirc;ble &laquo;&nbsp;<span class="quote">LapLink</span>&nbsp;&raquo; (N.D.T.&nbsp;: c&acirc;ble 
parall&egrave;le crois&eacute;), PLIP (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Line Interface 
Protocol</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Protocole d'Interface par 
Ligne Parall&egrave;le</span>&nbsp;&raquo;) permet &agrave; deux machines Linux de communiquer par 
le port parall&egrave;le en utilisant les couches logicielles standard bas&eacute;es 
sur la communication par &laquo;&nbsp;<span class="quote">sockets</span>&nbsp;&raquo;. En termes de bande 
passante, de temps de latence et d'&eacute;volutivit&eacute;, il ne s'agit pas d'une 
technologie r&eacute;seau s&eacute;rieuse. En revanche, le co&ucirc;t de revient quasi-nul 
et la compatibilit&eacute; logicielle s'av&egrave;rent &ecirc;tre tr&egrave;s utiles. Le pilote est 
partie int&eacute;grante du noyau Linux standard.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10C02"></a>3.2.14.&nbsp;SCI</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>non</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>4000 Mbit/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>2,7 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants.</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI et propri&eacute;taire</em></span>

</p></li><li><p>

Structure r&eacute;seau&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>+ de 1000 dollars</em></span>

</p></li></ul></div><p>

L'objectif de SCI (Scalable Coherent Interconnect, ANSI/IEEE 1596-1992) 
consiste essentiellement &agrave; fournir un m&eacute;canisme de haute performance 
pouvant assurer des acc&egrave;s coh&eacute;rents &agrave; la m&eacute;moire partag&eacute;e au travers 
d'un grand nombre de machines. On peut dire sans se mouiller que la 
bande passante et les temps de latences de SCI sont &laquo;&nbsp;<span class="quote">tr&egrave;s 
impressionnants</span>&nbsp;&raquo; compar&eacute;s &agrave; la plupart des autres technologies 
r&eacute;seau. Le probl&egrave;me est que SCI n'est pas tr&egrave;s r&eacute;pandu et reste donc 
assez on&eacute;reux, et que ce mat&eacute;riel n'est pas pris en charge par Linux.

</p><p>

SCI est principalement utilis&eacute; dans diverses impl&eacute;mentations 
propri&eacute;taires pour des machines &agrave; m&eacute;moire partag&eacute;e logiquement et 
distribu&eacute;e physiquement, comme le HP/Convex Exemplar SPP et le Sequent 
NUMA-Q 2000 (voir <a href="http://www.sequent.com" target="_top">http://www.sequent.com</a><sup>[<a href="#ftn.N10C39" name="N10C39">18</a>]</sup>). Ceci dit, SCI est disponible sous forme de 
carte PCI et de commutateurs quatre ports de Dolphin (on peut relier 
ainsi jusqu'&agrave; 16 machines en montant ces commutateurs en cascade), 
<a href="http://www.dolphinics.com" target="_top">http://www.dolphinics.com</a>, sous la s&eacute;rie "Clustar". Le 
CERN tient &agrave; jour une bonne collection de liens concernant SCI sur 
<a href="http://www.cern.ch/HSI/sci/sci.html" target="_top">http://www.cern.ch/HSI/sci/sci.html</a>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10C43"></a>3.2.15.&nbsp;SCSI</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: de <span class="emphasis"><em>5 Mbits/s</em></span> &agrave; plus 
de <span class="emphasis"><em>20 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>diff&eacute;rents fabricants</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>cartes PCI, EISA ou 
ISA</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>bus inter-machines partageant des 
p&eacute;riph&eacute;riques SCSI</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li></ul></div><p>

Le SCSI (Small Computer Systems Interconnect) consiste essentiellement 
en un bus d'entr&eacute;e/sortie utilis&eacute; par les disques durs, les lecteurs de 
CD-ROM, les num&eacute;riseurs 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">scanners</em></span></span>&nbsp;&raquo;), et c&aelig;tera. Il 
existe trois standards distincts&nbsp;: SCSI-1, SCSI-2 et SCSI-3, en 
vitesses "Fast" et "Ultra" et en largeur de bus de 8, 16 ou 32 bits 
(avec compatibilit&eacute; FireWire annonc&eacute;e pour SCSI-3). Tout cela est plut&ocirc;t 
confus, mais nous savons tous qu'un bon SCSI est bien plus rapide que 
l'EIDE, et peut g&eacute;rer plus de p&eacute;riph&eacute;riques, plus efficacement.

</p><p>

Ce que beaucoup de gens ne r&eacute;alisent pas, c'est qu'il est tr&egrave;s simple de 
partager le m&ecirc;me bus SCSI entre deux ordinateurs. Ce type de 
configuration est tr&egrave;s utile pour partager des disques entre deux 
machines et mettre en place un syst&egrave;me de 
<span class="emphasis"><em><span class="foreignphrase"><em class="foreignphrase">fail-over</em></span></em></span>, de fa&ccedil;on 
&agrave; ce qu'une machine prenne &agrave; sa charge les requ&ecirc;tes &agrave; une base de 
donn&eacute;es lorsque l'autre machine tombe en panne. C'est actuellement le 
seul m&eacute;canisme reconnu par le <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> PC 
de Microsoft&nbsp;: WolfPack. En revanche, l'incapacit&eacute; du SCSI &agrave; 
&eacute;voluer vers de plus grands syst&egrave;mes le rend en g&eacute;n&eacute;ral inint&eacute;ressant 
pour le traitement en parall&egrave;le.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10C83"></a>3.2.16.&nbsp;ServerNet</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>non</em></span>

</p></li><li><p>

Maximum bandwidth&nbsp;: <span class="emphasis"><em>400 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>3 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>fabricant exclusif</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>PCI</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>arbre hexagonal / concentrateurs en 
mailles t&eacute;tra&eacute;driques</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li></ul></div><p>

ServerNet est la solution r&eacute;seau de haute performance propos&eacute;e par 
Tandem (<a href="http://www.tandem.com" target="_top">http://www.tandem.com</a>). Dans le monde du 
traitement des transactions en ligne (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">OnLine 
Transation Processing</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">OLTP</span>&nbsp;&raquo;) 
en particulier, Tandem est r&eacute;put&eacute; &ecirc;tre l'un des premiers fabricants de 
syst&egrave;mes de haute fiabilit&eacute;, aussi n'est-il pas surprenant que leurs 
r&eacute;seaux ne revendiquent pas simplement la haute performance, mais aussi 
la &laquo;&nbsp;<span class="quote">haute fiabilit&eacute; et int&eacute;grit&eacute; des donn&eacute;es</span>&nbsp;&raquo;. Une autre 
facette int&eacute;ressante de ServerNet&nbsp;: ce mat&eacute;riel serait capable de 
transf&eacute;rer des donn&eacute;es directement de p&eacute;riph&eacute;rique &agrave; p&eacute;riph&eacute;rique, pas 
simplement entre processeurs, mais &eacute;galement entre disques durs, et 
c&aelig;tera, dans un style unilat&eacute;ral similaire &agrave; ce qui a &eacute;t&eacute; sugg&eacute;r&eacute; pour 
les m&eacute;canismes d'acc&egrave;s &agrave; distance &agrave; la m&eacute;moire du MPI, d&eacute;crits dans la 
section 3.5. Un dernier mot &agrave; propos de Servernet&nbsp;: Bien qu'il n'y 
ait qu'un seul fabricant, celui-ci est suffisamment puissant pour faire 
&eacute;tablir potentiellement Servernet en tant que standard majeur&nbsp;: 
Tandem appartient &agrave; Compaq<sup>[<a href="#ftn.N10CC0" name="N10CC0">19</a>]</sup>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10CC4"></a>3.2.17.&nbsp;SHRIMP</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>interface utilisateur &agrave; 
m&eacute;moire mapp&eacute;e</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>180 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>5 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>prototype exp&eacute;rimental</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>EISA</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>Fond de panier en maille (comme 
pour le Paragon d'Intel)</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li></ul></div><p>

Le projet <a href="http://www.cs.princeton.edu/shrimp/" target="_top">SHRIMP</a> de la section 
des sciences des ordinateurs de l'Universit&eacute; de Princeton, met sur pieds 
un ordinateur parall&egrave;le en utilisant dont les &eacute;l&eacute;ments de traitement 
sont des ordinateurs PC sous Linux. Le premier SHRIMP 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Scalable, High-Performance, Really Inexpensive 
Multi-Processor</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">multiprocesseur 
&eacute;volutif et de hautes performances vraiment bon march&eacute;</span>&nbsp;&raquo;) &eacute;tait un 
simple prototype biprocesseur utilisant une m&eacute;moire partag&eacute;e sur une 
carte EISA d&eacute;velopp&eacute;e pour l'occasion. Il existe d&eacute;sormais un prototype 
pouvant &eacute;voluer vers de plus larges configurations en utilisant une 
interface &laquo;&nbsp;<span class="quote">maison</span>&nbsp;&raquo; pour se connecter &agrave; une sorte de 
concentrateur, essentiellement con&ccedil;u comme le r&eacute;seau de routage en 
mailles utilis&eacute; dans le Paragon d'Intel. Des efforts consid&eacute;rables ont 
&eacute;t&eacute; faits pour d&eacute;velopper une &eacute;lectronique de &laquo;&nbsp;<span class="quote">communication 
mapp&eacute;e en m&eacute;moire virtuelle</span>&nbsp;&raquo; aux 
<span class="foreignphrase"><em class="foreignphrase">overheads</em></span> r&eacute;duits, avec sa couche 
logicielle.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10D08"></a>3.2.18.&nbsp;SLIP</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>0,1 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>1000 microsecondes&nbsp;?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>grande distribution</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>RS232C</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>c&acirc;ble entre deux machines</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>2 dollars</em></span>

</p></li></ul></div><p>

M&ecirc;me si SLIP (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Serial Line Interface 
Protocol</em></span></span>&nbsp;&raquo;) se situe d&eacute;finitivement au pied de 
l'&eacute;chelle des performances, ce protocole (tout comme CSLIP ou PPP) 
permet &agrave; deux machines de communiquer en utilisant les 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">sockets</em></span></span>&nbsp;&raquo; et ce au travers 
d'un c&acirc;ble RS232 ordinaire. Les ports RS232 peuvent &ecirc;tre reli&eacute;s &agrave; l'aide 
d'un c&acirc;ble s&eacute;rie type NULL-MODEM, ou m&ecirc;me au travers d'une ligne 
t&eacute;l&eacute;phonique en utilisant des modems. Dans tous les cas, les temps de 
latence sont &eacute;lev&eacute;s, et la bande passante r&eacute;duite. Aussi, SLIP ne 
devrait &ecirc;tre utilis&eacute; qu'en dernier recours. En revanche, la plupart des 
PC sont dot&eacute;s de deux ports RS232. Il doit donc &ecirc;tre possible de relier 
un groupe de machines sous forme de r&eacute;seau lin&eacute;aire ou d'anneau. Il 
existe m&ecirc;me un logiciel de r&eacute;partition de la charge appel&eacute; EQL.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10D40"></a>3.2.19.&nbsp;TTL_PAPERS</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>biblioth&egrave;que AFAPI</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>1,6 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>3 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>conception dans le domaine public, 
fabricant exclusif</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>SPP (port parall&egrave;le)</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>arbre de concentrateurs</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>100 dollars</em></span>

</p></li></ul></div><p>

Le projet PAPERS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Purdue's Adapter for Parallel 
Execution and Rapid Synchronization</em></span></span>&nbsp;&raquo;, soit 
&laquo;&nbsp;<span class="quote">Adaptateur pour l'Ex&eacute;cution en Parall&egrave;le et la Synchronisation 
Rapide de l'universit&eacute; de Purdue</span>&nbsp;&raquo;), men&eacute; par la Purdue University 
School of Electrical and Computer Engineering (&laquo;&nbsp;<span class="quote">&Eacute;cole Sup&eacute;rieure 
d'&Eacute;lectricit&eacute; et d'Ing&eacute;nierie en Informatique</span>&nbsp;&raquo;), d&eacute;veloppe un 
ensemble logiciel et mat&eacute;riel &eacute;volutif et aux temps de latence r&eacute;duits 
pour les communications des fonctions d'agr&eacute;gation, permettant de mettre 
sur pieds un supercalculateur en parall&egrave;le utilisant comme n&#339;uds 
des PC d'origine, non modifi&eacute;s.

</p><p>

Plus d'une douzaine de versions de cartes &laquo;&nbsp;<span class="quote">PAPERS</span>&nbsp;&raquo;, reli&eacute;es 
au PC &agrave; la station de travail via le port parall&egrave;le standard (SPP&nbsp;: 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Standard Parallel Port</em></span></span>&nbsp;&raquo;), 
ont &eacute;t&eacute; construites, en suivant globalement deux grands axes. Les 
versions estampill&eacute;es &laquo;&nbsp;<span class="quote">PAPERS</span>&nbsp;&raquo; visent les hautes 
performances, quelle que soit la technologie la plus indiqu&eacute;e, la 
version actuelle utilisant des FPGA (N.D.T.&nbsp;: famille de r&eacute;seaux 
logiques programmables), et des mod&egrave;les d'interfaces PCI &agrave; haut d&eacute;bit 
sont actuellement &agrave; l'&eacute;tude. Par opposition, les versions nomm&eacute;es 
&laquo;&nbsp;<span class="quote">TTL_PAPERS</span>&nbsp;&raquo; sont con&ccedil;ues pour &ecirc;tre facilement reproduites 
hors de l'universit&eacute; de Purdue, et s'appuient sur des mod&egrave;les du domaine 
public remarquablement simples et qui peuvent &ecirc;tre mis en place en 
utilisant de la logique TTL ordinaire. L'une de ces versions est 
produite commercialement.

</p><p>

Contrairement au mat&eacute;riel sur mesure con&ccedil;u par d'autres universit&eacute;s, des 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> TTL_PAPERS ont &eacute;t&eacute; assembl&eacute;s 
dans plusieurs &eacute;coles depuis les &Eacute;tats-Unis jusqu'en Cor&eacute;e du Sud. La 
bande passante est s&eacute;v&egrave;rement limit&eacute;e par la connectivit&eacute; du port 
parall&egrave;le, mais PAPERS met en &#339;uvre des fonctions d'agr&eacute;gation aux 
temps de latence tr&egrave;s r&eacute;duits. M&ecirc;me les syst&egrave;mes orient&eacute;s messages les 
plus rapides ne peuvent offrir de telles performances sur ces fonctions 
d'agr&eacute;gation. Ainsi, PAPERS est particuli&egrave;rement performant dans la 
synchronisation des diff&eacute;rents &eacute;crans d'un mur vid&eacute;o (&agrave; d&eacute;battre dans le 
<span class="foreignphrase"><em class="foreignphrase">Video-Wall-HOWTO</em></span> actuellement en 
pr&eacute;paration), pour planifier les acc&egrave;s &agrave; un r&eacute;seau &agrave; haut d&eacute;bit, pour 
&eacute;valuer les probabilit&eacute;s en recherche g&eacute;n&eacute;tique, et c&aelig;tera. M&ecirc;me si des 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> PAPERS ont &eacute;t&eacute; construits en 
utilisant AIX d'IBM sur PowerPC, des DEC Alpha OSF/1, ou HP-UX sur HP 
PA-RISC, le PC sous Linux reste la plate-forme la mieux prise en charge.

</p><p>

Les programmes utilisateur utilisant l'AFAPI de TTL_PAPERS attaquent 
directement les registres mat&eacute;riels du port parall&egrave;le sous Linux, sans 
effectuer d'appel syst&egrave;me &agrave; chaque acc&egrave;s. Pour ce faire, l'AFAPI demande 
d'abord les droits d'acc&egrave;s au port parall&egrave;le en utilisant soit 
<code class="literal">iopl()</code>, soit <code class="literal">ioperm()</code>. Le probl&egrave;me 
est que, l'un comme l'autre, ces appels obligent le programme appelant &agrave; 
&ecirc;tre privil&eacute;gi&eacute;, ce qui introduit une faille de s&eacute;curit&eacute; potentielle. La 
solution r&eacute;side en un correctif optionnel &agrave; appliquer au noyau Linux et 
permettant &agrave; un processus privil&eacute;gi&eacute; de contr&ocirc;ler les permissions 
d'acc&egrave;s aux ports d'entr&eacute;e/sortie pour n'importe quel autre processus.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10D9E"></a>3.2.20.&nbsp;USB (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Universal Serial Bus</em></span></span>&nbsp;&raquo;)</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>pilotes du noyau</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>12 Mbits/s</em></span>

</p></li><li><p>

Temps de latence minimum&nbsp;: <span class="emphasis"><em>?</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>dans le commerce</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>USB</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>bus</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>5 dollars</em></span>

</p></li></ul></div><p>

USB (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Universal Serial Bus</em></span></span>&nbsp;&raquo;, 
<a href="http://www.usb.org" target="_top">http://www.usb.org</a>) est un bus fonctionnant &agrave; la vitesse 
de l'Ethernet conventionnel, dont les p&eacute;riph&eacute;riques qui s'y rattachent 
peuvent &ecirc;tre connect&eacute;s &agrave; chaud 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Hot-Plug</em></span></span>&nbsp;&raquo;&nbsp;: sans 
imposer la mise hors tension pr&eacute;alable du bus) et pouvant accueillir simultan&eacute;ment 
jusqu'&agrave; 127 de ces p&eacute;riph&eacute;riques pouvant s'&eacute;tendre du clavier &agrave; la 
cam&eacute;ra de vid&eacute;o-conf&eacute;rence. La mani&egrave;re dont on relie plusieurs 
ordinateurs par le biais de l'USB n'est pas clairement d&eacute;finie. Quoi 
qu'il en soit, les ports USB sont en train de s'&eacute;tablir tr&egrave;s rapidement 
en standard sur les cartes-m&egrave;res, au m&ecirc;me titre que le port s&eacute;rie RS232 
ou le port parall&egrave;le, aussi ne soyez pas surpris si vous voyez 
appara&icirc;tre un ou deux ports USB <sup>[<a href="#ftn.N10DDD" name="N10DDD">20</a>]</sup> sur votre prochain PC.

</p><p>
D'une certaine mani&egrave;re, l'USB est pratiquement la version basse performance
&agrave; prix nul du FireWire que l'on peut se procurer aujourd'hui.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10DE3"></a>3.2.21.&nbsp;WAPERS</h4></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>

Prise en charge par Linux&nbsp;: <span class="emphasis"><em>biblioth&egrave;que AFAPI</em></span>

</p></li><li><p>

Bande passante maximum&nbsp;: <span class="emphasis"><em>0,4 Mbits/s</em></span>

</p></li><li><p>

Temps de latence&nbsp;: <span class="emphasis"><em>3 microsecondes</em></span>

</p></li><li><p>

Disponibilit&eacute;&nbsp;: <span class="emphasis"><em>mod&egrave;le dans le domaine public</em></span>

</p></li><li><p>

Interface ou bus utilis&eacute;&nbsp;: <span class="emphasis"><em>SPP (Port Parall&egrave;le Standard)</em></span>

</p></li><li><p>

Structure du r&eacute;seau&nbsp;: <span class="emphasis"><em>mod&egrave;le de c&acirc;blage entre 2 &agrave; 64 machines</em></span>

</p></li><li><p>

Co&ucirc;t par machine connect&eacute;e&nbsp;: <span class="emphasis"><em>5 dollars</em></span>

</p></li></ul></div><p>

WAPERS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Wired-AND Adapter for Parallel Execution 
and Rapid Synchronization</em></span></span>&nbsp;&raquo;, soit 
&laquo;&nbsp;<span class="quote">Adaptateur par ET C&acirc;bl&eacute; pour l'Ex&eacute;cution en Parall&egrave;le et la 
Synchronisation Rapide</span>&nbsp;&raquo;) est une des facettes du projet PAPERS, 
de la Purdue University School of Electrical and Computer Engineering 
(&laquo;&nbsp;<span class="quote">&Eacute;cole Sup&eacute;rieure d'&Eacute;lectricit&eacute; et d'Ing&eacute;nierie en 
Informatique</span>&nbsp;&raquo;). S'il est construit proprement, le port parall&egrave;le 
poss&egrave;de quatre bits de sortie &agrave; collecteur ouvert qui peuvent &ecirc;tre 
c&acirc;bl&eacute;s entre eux pour former un ET c&acirc;bl&eacute; de 4 bits de large. Ce ET c&acirc;bl&eacute; 
est assez sensible &eacute;lectriquement, et le nombre maximum de machines qui 
peuvent y &ecirc;tre reli&eacute;es d&eacute;pend de fa&ccedil;on critique des propri&eacute;t&eacute;s 
analogiques des ports (les limites maximum des puits de courant et les 
valeurs des r&eacute;sistances de <span class="foreignphrase"><em class="foreignphrase">pull-up</em></span>). On 
peut typiquement relier 7 &agrave; 8 machines en r&eacute;seau de cette fa&ccedil;on, avec 
WAPERS. Bien que les co&ucirc;ts et les temps de latences soient tr&egrave;s r&eacute;duits, 
la bande passante l'est aussi. WAPERS est donc bien plus indiqu&eacute; comme 
r&eacute;seau secondaire d&eacute;di&eacute; aux fonctions d'agr&eacute;gations que comme unique 
r&eacute;seau d'un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>. Comme pour 
TTL_PAPERS, il existe un correctif noyau visant &agrave; am&eacute;liorer la s&eacute;curit&eacute;, 
recommand&eacute; mais non requis.

</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10E23"></a>3.3.&nbsp;Interface Logicielle R&eacute;seau</h3></div></div></div><p>

Avant d'explorer les ressources logicielles existantes en mati&egrave;re de 
traitement en parall&egrave;le, il est utile de couvrir rapidement les bases de 
l'interface logicielle de bas niveau g&eacute;rant l'&eacute;lectronique du r&eacute;seau. Il 
n'existe en r&eacute;alit&eacute; que trois options de base&nbsp;: les 
<span class="foreignphrase"><em class="foreignphrase">sockets</em></span>, les pilotes de p&eacute;riph&eacute;riques et 
les biblioth&egrave;ques utilisateur.

</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10E2B"></a>3.3.1.&nbsp;Les <span class="foreignphrase"><em class="foreignphrase">sockets</em></span></h4></div></div></div><p>

Le <span class="foreignphrase"><em class="foreignphrase">socket</em></span> est de loin la plus courante 
des interfaces r&eacute;seau de bas niveau. Les 
<span class="foreignphrase"><em class="foreignphrase">sockets</em></span> sont partie int&eacute;grante d'Unix 
depuis plus d'une d&eacute;cennie et la plupart des standards dans le domaine 
du mat&eacute;riel &eacute;lectronique de r&eacute;seau est con&ccedil;ue pour prendre en charge au 
moins deux types de protocoles de 
<span class="foreignphrase"><em class="foreignphrase">socket</em></span>&nbsp;: TCP et UDP. Ces deux types 
de <span class="foreignphrase"><em class="foreignphrase">sockets</em></span> vous permettent d'envoyer des 
blocs de donn&eacute;es d'une longueur arbitraire d'une machine &agrave; l'autre, mais 
il existe plusieurs diff&eacute;rences importantes. Typiquement, ils engendrent 
tous deux un temps de latence minimum d'environ 1000 microsecondes, m&ecirc;me 
si les performances peuvent bien pires encore en fonction du trafic.

</p><p>

Ces types de <span class="foreignphrase"><em class="foreignphrase">sockets</em></span> constituent 
l'interface logicielle r&eacute;seau de base pour la majorit&eacute; des logiciels de 
traitement en parall&egrave;le portables et de plus haut niveau. Par exemple, 
PVM utilise une combinaison de l'UDP et du TCP, aussi en conna&icirc;tre les 
diff&eacute;rences vous aidera &agrave; affiner les performances de votre syst&egrave;me. Ce 
qui suit n'est qu'un aper&ccedil;u de TCP et UDP. R&eacute;f&eacute;rez-vous aux pages du 
manuel et &agrave; un bon livre de programmation pour plus de d&eacute;tails.

</p><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N10E43"></a>3.3.1.1.&nbsp;Le protocole UDP (SOCK_DGRAM)</h5></div></div></div><p>

<span class="emphasis"><em>UDP</em></span> signifie &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">User Datagram 
Protocol</em></span></span>&nbsp;&raquo; ou &laquo;&nbsp;<span class="quote">Protocole de Datagrammes 
Utilisateur</span>&nbsp;&raquo; mais il est plus facile de se souvenir des 
propri&eacute;t&eacute;s d'UDP en tant que &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Unreliable Datagram 
Processing</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">Traitement des Datagrammes 
Peu fiable</span>&nbsp;&raquo;. En d'autres termes, UDP permet &agrave; chaque bloc d'&ecirc;tre 
&eacute;mis comme un message individuel, mais un message peut &ecirc;tre perdu 
pendant la transmission. De fait, selon l'&eacute;tat du trafic sur le r&eacute;seau, 
certains messages UDP peuvent &ecirc;tre perdus, arriver plusieurs fois, ou 
arriver dans un ordre diff&eacute;rent de celui dans lequel ils ont &eacute;t&eacute; &eacute;mis. 
L'exp&eacute;diteur d'un message UDP ne re&ccedil;oit pas syst&eacute;matiquement d'accus&eacute; de 
r&eacute;ception, et c'est donc au programme &eacute;crit par l'utilisateur qu'il 
appartient de d&eacute;tecter et compenser ces probl&egrave;mes. Heureusement, le 
protocole UDP garantit que si un message arrive, son contenu sera intact 
(c'est-&agrave;-dire que vous ne recevrez jamais un message incomplet).

</p><p>

Le bon cot&eacute; de l'UDP est qu'il tend &agrave; &ecirc;tre le plus rapide des protocoles 
des <span class="foreignphrase"><em class="foreignphrase">socket</em></span>. En outre, UDP est 
&laquo;&nbsp;<span class="quote">orient&eacute; hors connexion</span>&nbsp;&raquo; 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">connectionless</em></span></span>&nbsp;&raquo;), ce qui 
signifie que chaque message est essentiellement ind&eacute;pendant des autres. 
On peut comparer chaque message &agrave; une lettre &agrave; La Poste. Vous pouvez 
envoyer plusieurs lettres &agrave; la m&ecirc;me adresse, mais chacune d'entre elles 
est ind&eacute;pendante des autres, et vous n'&ecirc;tes pas limit&eacute; quant aux nombre 
de personnes &agrave; qui vous pouvez en envoyer.

</p></div><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N10E65"></a>3.3.1.2.&nbsp;Le protocole TCP (SOCK_STREAM)</h5></div></div></div><p>

Contrairement &agrave; l'UDP, le <span class="emphasis"><em>TCP</em></span> est un protocole 
fiable et orient&eacute; connexion. Chaque bloc est consid&eacute;r&eacute; non pas comme un 
message, mais comme un bloc de donn&eacute;es appartenant &agrave; un flot d'octets 
voyageant au travers d'une connexion &eacute;tablie entre l'exp&eacute;diteur et le 
destinataire. Ce principe est tr&egrave;s diff&eacute;rent du syst&egrave;me de messages de 
l'UDP car chaque bloc n'est qu'une partie du flot d'octets, et il 
appartient au programme utilisateur de trouver le moyen de les isoler 
car il n'y a aucune marque de s&eacute;paration pour les distinguer. De plus, 
les connexions sont plus vuln&eacute;rables aux perturbations du r&eacute;seau, et 
seul un nombre limit&eacute; de connexions simultan&eacute;es peut exister au sein 
d'un m&ecirc;me processus. Parce qu'il est fiable, le TCP engendre souvent des 
<span class="foreignphrase"><em class="foreignphrase">overheads</em></span> plus importants que l'UDP.

</p><p>

Le TCP r&eacute;serve en revanche quelques bonnes surprises. Par exemple, si 
plusieurs messages sont envoy&eacute;s &agrave; travers une connexion, TCP est capable 
de les rassembler dans une m&eacute;moire tampon pour mieux correspondre aux 
tailles standard des paquets de l'&eacute;lectronique du r&eacute;seau, ce qui peut 
donner de meilleurs r&eacute;sultats que l'UDP dans le cas de groupes de 
messages courts ou de taille inhabituelle. Un autre avantage&nbsp;: Les 
r&eacute;seaux b&acirc;tis sur des connexions physiques directes entre deux machines 
peuvent facilement et efficacement &ecirc;tre assimil&eacute;s &agrave; des connexions TCP. 
Ce fut le cas pour la &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Socket 
Library</em></span></span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote">biblioth&egrave;que de gestion de 
<span class="foreignphrase"><em class="foreignphrase">Sockets</em></span></span>&nbsp;&raquo;), pr&eacute;sentant une gestion 
compatible TCP au niveau de l'utilisateur qui ne diff&eacute;rait des appels 
syst&egrave;mes TCP standard que par le pr&eacute;fixe <code class="literal">PSS</code>, que 
l'on rajoutait au d&eacute;but du nom des fonctions &agrave; invoquer.

</p></div></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10E7F"></a>3.3.2.&nbsp;Les pilotes de p&eacute;riph&eacute;riques</h4></div></div></div><p>

Lorsque l'on en arrive au stade o&ugrave; il faut effectivement injecter des 
donn&eacute;es sur le r&eacute;seau ou les y en extraire, l'interface logicielle des 
Unix standard fait partie du noyau et s'appelle &laquo;&nbsp;<span class="quote">pilote</span>&nbsp;&raquo; 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">driver</em></span></span>&nbsp;&raquo;). UDP et TCP ne se 
contentent pas de transporter des donn&eacute;es, ils s'accompagnent &eacute;galement 
d'importants <span class="foreignphrase"><em class="foreignphrase">overheads</em></span> dus &agrave; la gestion 
des <span class="foreignphrase"><em class="foreignphrase">sockets</em></span>. Par exemple, il faut que 
quelque chose s'occupe du fait que plusieurs connexions TCP peuvent 
partager la m&ecirc;me interface r&eacute;seau physique. Par opposition, un pilote de 
p&eacute;riph&eacute;rique d&eacute;di&eacute; &agrave; une interface r&eacute;seau n'a besoin de mettre en 
&#339;uvre qu'un petit nombre de fonctions de transport &eacute;l&eacute;mentaires. 
Ces pilotes peuvent alors &ecirc;tre invoqu&eacute;s &agrave; l'aide de l'appel 
<code class="literal">open()</code> pour identifier le p&eacute;riph&eacute;rique ad&eacute;quat, puis 
en utilisant par exemple <code class="literal">read()</code> et 
<code class="literal">write()</code> sur le &laquo;&nbsp;<span class="quote">fichier</span>&nbsp;&raquo; ouvert. Ainsi, 
chaque op&eacute;ration peut transporter un bloc de donn&eacute;es en co&ucirc;tant &agrave; peine 
plus cher qu'un appel syst&egrave;me, ce qui permet d'atteindre des d&eacute;lais de 
l'ordre de quelques dizaines de microsecondes.

</p><p>

&Eacute;crire un pilote de p&eacute;riph&eacute;rique pour Linux n'est pas difficile&hellip; 
pourvu que vous sachiez <span class="emphasis"><em>parfaitement</em></span> comme 
fonctionne votre p&eacute;riph&eacute;rique. Si vous n'en &ecirc;tes pas s&ucirc;r, n'essayez pas 
de le deviner. D&eacute;boguer un pilote de p&eacute;riph&eacute;rique n'est pas une chose 
amusante, et faire des erreurs peut co&ucirc;ter la vie &agrave; votre mat&eacute;riel. En 
revanche, si ces risques ne vous effraient pas, il est possible d'&eacute;crire 
un pilote pour, par exemple, utiliser des cartes Ethernet d&eacute;di&eacute;es comme 
des connexions machine-vers-machine &laquo;&nbsp;<span class="quote">b&ecirc;tes</span>&nbsp;&raquo; mais tr&egrave;s 
rapides car exon&eacute;r&eacute;es du protocole Ethernet habituel. Pour &ecirc;tre exact, 
c'est pratiquement la technique utilis&eacute;e par les premiers 
supercalculateurs Intel. R&eacute;f&eacute;rez-vous au 
<span class="foreignphrase"><em class="foreignphrase">Device-Driver-HOWTO</em></span> pour plus 
d'informations.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N10EAB"></a>3.3.3.&nbsp;Biblioth&egrave;ques utilisateurs</h4></div></div></div><p>

Si vous avez pris des cours de programmation syst&egrave;me, on a d&ugrave; vous y 
apprendre qu'acc&eacute;der directement aux registres mat&eacute;riels des 
p&eacute;riph&eacute;riques &agrave; partir d'un programme utilisateur &eacute;tait l'exemple 
typique de ce qu'il ne faut pas faire, parce que l'un des principes m&ecirc;me 
d'un syst&egrave;me d'exploitation est de contr&ocirc;ler l'acc&egrave;s aux p&eacute;riph&eacute;riques. 
Cependant, le simple fait de passer un appel syst&egrave;me co&ucirc;te au minimum 
quelques dizaines de microsecondes. Dans le cas d'interfaces r&eacute;seau 
b&acirc;ties sur mesure comme TTL_PAPERS, qui peut effectuer des op&eacute;rations de 
base sur un r&eacute;seau en seulement 3 microsecondes, un tel surco&ucirc;t pour un 
appel syst&egrave;me est intol&eacute;rable. Le seul moyen d'&eacute;viter ce temps d'attente 
est de faire en sorte que du code s'ex&eacute;cutant au niveau de 
l'utilisateur, donc une &laquo;&nbsp;<span class="quote">biblioth&egrave;que au niveau de 
l'utilisateur</span>&nbsp;&raquo; <sup>[<a href="#ftn.N10EB3" name="N10EB3">21</a>]</sup>, puisse acc&eacute;der directement au mat&eacute;riel, mais sans 
remettre en cause la souverainet&eacute; du syst&egrave;me d'exploitation sur la 
gestion des droits d'acc&egrave;s aux ressources mat&eacute;rielles.

</p><p>
Sur un syst&egrave;me typique, les seuls moyens, pour une biblioth&egrave;que utilisateur,
d'acc&eacute;der directement aux registres du mat&eacute;riel sont les suivants&nbsp;:
</p><div class="orderedlist"><ol type="1"><li><p>

Au lancement du programme utilisateur, faire un appel syst&egrave;me pour 
mapper l'espace d'adressage qui contient les registres du p&eacute;riph&eacute;rique 
dans le plan m&eacute;moire du processus utilisateur. Sur certains syst&egrave;mes, 
l'appel syst&egrave;me <code class="literal">mmap()</code> (trait&eacute; pour la premi&egrave;re fois 
dans la section 2.6) peut &ecirc;tre utilis&eacute; pour mapper un fichier sp&eacute;cial 
repr&eacute;sentant les adresses de la page de m&eacute;moire physique du 
p&eacute;riph&eacute;rique. Il est en m&ecirc;me temps relativement simple d'&eacute;crire un 
pilote de p&eacute;riph&eacute;rique effectuant cette op&eacute;ration. De plus, ce pilote 
peut obtenir l'acc&egrave;s en ne mappant que la ou les pages qui contiennent 
les registres n&eacute;cessaires, en maintenant ainsi le contr&ocirc;le des droits 
d'acc&egrave;s sous la coupe du syst&egrave;me d'exploitation.

</p></li><li><p>

Acc&eacute;der ensuite aux registres du p&eacute;riph&eacute;rique sans passer par un appel 
syst&egrave;me en se contentant de charger ou de ranger des valeurs sur la 
plage d'adressage mapp&eacute;e. Par exemple, un

<code class="literal">*((char *) 0x1234) = 5;</code>

d&eacute;posera un octet de valeur 5 &agrave; l'adresse m&eacute;moire 1234 en hexad&eacute;cimal.

</p></li></ol></div><p>
Par bonheur, il se trouve que Linux pour Intel 386 et compatibles
offre une solution meilleure encore&nbsp;:
</p><div class="orderedlist"><ol type="1"><li><p>

En invoquant l'appel syst&egrave;me <code class="literal">ioperm()</code>
depuis un processus privil&eacute;gi&eacute;, obtenir la permission d'acc&eacute;der
aux ports d'entr&eacute;e/sortie correspondant pr&eacute;cis&eacute;ment aux registres
du p&eacute;riph&eacute;rique. Parall&egrave;lement, ces permissions peuvent &ecirc;tre g&eacute;r&eacute;es
par un processus utilisateur privil&eacute;gi&eacute; et ind&eacute;pendant (autrement
dit&nbsp;: un &laquo;&nbsp;<span class="quote">m&eacute;ta-syst&egrave;me d'exploitation</span>&nbsp;&raquo;) en utilisant l'appel
 syst&egrave;me <span class="emphasis"><em>giveioperm()</em></span>, disponible sous la
forme d'un correctif &agrave; appliquer au noyau
Linux.

</p></li><li><p>

Acc&eacute;der aux registres du p&eacute;riph&eacute;rique sans appel syst&egrave;me en
utilisant les instructions assembleur d'acc&egrave;s aux ports
d'entr&eacute;e/sortie du 386.

</p></li></ol></div><p>
Cette seconde solution est pr&eacute;f&eacute;rable car il arrive souvent que
les registres de plusieurs p&eacute;riph&eacute;riques soient r&eacute;unis sur une
m&ecirc;me page, auquel cas la premi&egrave;re m&eacute;thode ne pourrait offrir
de protection contre l'acc&egrave;s aux autres registres r&eacute;sidant dans
la m&ecirc;me page que ceux appartenant au p&eacute;riph&eacute;rique concern&eacute;.
L'inconv&eacute;nient est que ces instructions ne peuvent bien s&ucirc;r pas
&ecirc;tre &eacute;crites en langage C. Il vous faudra &agrave; la place utiliser
un peu d'assembleur. La fonction utilisant l'assembleur en ligne
int&eacute;gr&eacute; &agrave; GCC (donc utilisable dans les programmes C) permettant
de lire un octet depuis un port est la suivante&nbsp;:
</p><pre class="programlisting">
extern inline unsigned char
inb(unsigned short port)
{
    unsigned char _v;
__asm__ __volatile__ ("inb %w1,%b0"
                      :"=a" (_v)
                      :"d" (port), "0" (0));
    return _v;
}
</pre><p>
La fonction sym&eacute;trique permettant l'&eacute;mission d'un octet est&nbsp;:
</p><p>

<pre class="programlisting">
extern inline void
outb(unsigned char value,
unsigned short port)
{
__asm__ __volatile__ ("outb %b0,%w1"
                      :/* pas de valeur retourn&eacute;e */
                      :"a" (value), "d" (port));
}
</pre>

</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10EEF"></a>3.4.&nbsp;PVM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Virtual Machine</em></span></span>&nbsp;&raquo;)</h3></div></div></div><p>

PVM (pour &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Virtual 
Machine</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Machine Virtuelle en 
Parall&egrave;le</span>&nbsp;&raquo;) est une biblioth&egrave;que de 
<span class="foreignphrase"><em class="foreignphrase">message-passing</em></span> portable et disponible 
gratuitement, s'appuyant g&eacute;n&eacute;ralement directement sur les 
<span class="foreignphrase"><em class="foreignphrase">sockets</em></span>. Cette biblioth&egrave;que s'est 
incontestablement &eacute;tablie comme le standard <span class="foreignphrase"><em class="foreignphrase">de 
facto</em></span> dans le domaine du traitement en parall&egrave;le &agrave; 
l'aide de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> &agrave; transmission de 
messages.

</p><p>

PVM prend en charge les machines Linux monoprocesseur et SMP, comme les 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de machines Linux reli&eacute;es entre 
elles &agrave; l'aide par des r&eacute;seaux reconnaissant les 
<span class="foreignphrase"><em class="foreignphrase">sockets</em></span> (donc SLIP, PLIP, Ethernet, ATM). 
En fait, PVM fonctionnera m&ecirc;me &agrave; travers un groupe de machines utilisant 
diff&eacute;rents types de processeurs, de configurations et de r&eacute;seaux 
physiques &mdash; Un <span class="emphasis"><em>cluster h&eacute;t&eacute;rog&egrave;ne</em></span> &mdash; 
m&ecirc;me si l'envergure de ce <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> est de 
l'ordre de la mise en parall&egrave;le de machines en utilisant Internet pour 
les relier entre elles. PVM offre m&ecirc;me des facilit&eacute;s de contr&ocirc;les de 
t&acirc;ches (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">jobs</em></span></span>&nbsp;&raquo;) en parall&egrave;le 
au travers d'un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>. Cerise sur le 
g&acirc;teau, PVM est disponible gratuitement et depuis longtemps 
(actuellement sur <a href="http://www.epm.ornl.gov/pvm/pvm_home.html" target="_top">http://www.epm.ornl.gov/pvm/pvm_home.html</a>), ce qui a conduit bon 
nombre de langages de programmation, de compilateurs, et d'outils de 
d&eacute;bogage ou autres &agrave; l'adopter comme leur &laquo;&nbsp;<span class="quote">biblioth&egrave;que cible 
portable de <span class="foreignphrase"><em class="foreignphrase">message-passing</em></span></span>&nbsp;&raquo;. Il 
existe &eacute;galement un groupe de discussion&nbsp;: <a href="news:comp.parallel.pvm" target="_top">news:comp.parallel.pvm</a>.

</p><p>

Il est important de remarquer, en revanche, que les appels PVM ajoutent 
g&eacute;n&eacute;ralement aux op&eacute;rations <span class="foreignphrase"><em class="foreignphrase">socket</em></span> un 
<span class="foreignphrase"><em class="foreignphrase">overhead</em></span> non n&eacute;gligeable alors que les 
temps de latence de celles-ci sont d&eacute;j&agrave; importants. En outre, les appels 
eux-m&ecirc;mes ne sont pas ais&eacute;s &agrave; manipuler.

</p><p>
Appliqu&eacute;e au m&ecirc;me exemple de calcul de Pi d&eacute;crit en section 1.3,
la version PVM du programme en langage C est la suivante&nbsp;:
</p><pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;pvm3.h&gt;

#define NPROC	4

main(int argc, char **argv)
{
  register double sommelocale, largeur;
  double somme;
  register int intervalles, i;
  int mytid, iproc, msgtag = 4;
  int tids[NPROC];  /* Tableau des num&eacute;ros des t&acirc;ches */

  /* D&eacute;but du traitement avec PVM */
  mytid = pvm_mytid();

  /* &laquo; Je rejoins le groupe et, si je suis la premi&egrave;re instance,
     iproc=0, je cr&eacute;e plusieurs copies de moi-m&ecirc;me. &raquo;
  */
  iproc = pvm_joingroup("pi");

  if (iproc == 0) {
    tids[0] = pvm_mytid();
    pvm_spawn("pvm_pi", &amp;argv[1], 0, NULL, NPROC-1, &amp;tids[1]);
  }
  /* On s'assure que tous les processus sont pr&ecirc;ts  */
  pvm_barrier("pi", NPROC);

  /* R&eacute;cup&egrave;re le nombre d'intervalles */
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;

  sommelocale = 0.0;
  for (i = iproc; i&lt;intervalles; i+=NPROC) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }

  /* On ajuste les r&eacute;sultats locaux en fonction de la largeur */
  somme = sommlocale * largeur;
  pvm_reduce(PvmSum, &amp;sum, 1, PVM_DOUBLE, msgtag, "pi", 0);

  /* Seul le processus rattach&eacute; &agrave; la console renvoie le r&eacute;sultat */
  if (iproc == 0) {
    printf("Estimation de la valeur de pi: %f\n", somme);
  }

  /* On attend que le programme soit termin&eacute;,
     on quitte le groupe et
	 on sort de PVM. */
  pvm_barrier("pi", NPROC);
  pvm_lvgroup("pi");
  pvm_exit();
  return(0);
}
</pre></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10F38"></a>3.5.&nbsp;MPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Message Passing Interface</em></span></span>&nbsp;&raquo;)</h3></div></div></div><p>

Bien que PVM soit le standard de fait en mati&egrave;re de biblioth&egrave;que de 
<span class="foreignphrase"><em class="foreignphrase">message-passing</em></span>, MPI 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Message Passing 
Interface</em></span></span>&nbsp;&raquo;) tend &agrave; devenir le nouveau standard 
officiel. Le site du standard MPI se trouve sur <a href="http://www.mcs.anl.gov:80/mpi/" target="_top">http://www.mcs.anl.gov:80/mpi/</a> et le groupe de discussion 
correspondant sur <a href="news:comp.parallel.mpi" target="_top">news:comp.parallel.mpi</a>.

</p><p>
En revanche, avant d'explorer MPI, je me sens oblig&eacute; de parler
rapidement de la guerre de religion qui oppose PVM &agrave; MPI et qui
dure depuis quelques ann&eacute;es. Je ne penche ni pour l'un ni pour
l'autre. Voici un r&eacute;sum&eacute; aussi impartial que possible des
diff&eacute;rences entre les deux interfaces&nbsp;:
</p><div class="variablelist"><dl><dt><span class="term">Environnement de contr&ocirc;le de l'ex&eacute;cution</span></dt><dd><p>
Pour faire simple, PVM en a un, et MPI ne pr&eacute;cise pas
s'il existe, ni comment il doit &ecirc;tre impl&eacute;ment&eacute;. Cela
signifie que certaines choses comme lancer l'ex&eacute;cution
d'un programme PVM se fait de la m&ecirc;me mani&egrave;re partout,
alors que pour MPI, cela d&eacute;pend de l'impl&eacute;mentation
utilis&eacute;e.
</p></dd><dt><span class="term">Prise en charge des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> h&eacute;t&eacute;rog&egrave;nes.</span></dt><dd><p>
PVM a grandi dans le monde de la collecte des cycles machines
inutilis&eacute;s sur les stations de travail, et sait donc g&eacute;rer donc
directement les m&eacute;langes h&eacute;t&eacute;rog&egrave;nes de machines et de syst&egrave;mes
d'exploitation. A contrario, MPI part du principe g&eacute;n&eacute;ral que
la cible est un MPP (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Massively Parallel Processor</em></span></span>&nbsp;&raquo;, soit
&laquo;&nbsp;<span class="quote">Processeur Massivement Parall&egrave;le</span>&nbsp;&raquo;) ou un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> d&eacute;di&eacute;
de stations de travail pratiquement toutes identiques.
</p></dd><dt><span class="term">Syndrome de l'&eacute;vier.</span></dt><dd><p>
PVM se r&eacute;v&egrave;le &ecirc;tre con&ccedil;u pour une cat&eacute;gorie d'utilisation bien
d&eacute;finie, ce que MPI&nbsp;2.0 ne fait pas. Le nouveau standard MPI&nbsp;2.0
inclut une vari&eacute;t&eacute; de fonctionnalit&eacute;s qui s'&eacute;tendent bien au del&agrave;
du simple mod&egrave;le de <span class="foreignphrase"><em class="foreignphrase">message-passing</em></span>, comme le RMA
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Remote Memory Access</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Acc&egrave;s M&eacute;moire &agrave; Distance</span>&nbsp;&raquo;) ou les
op&eacute;rations d'entr&eacute;e/sortie en parall&egrave;le sur les fichiers. Toutes
ces choses sont-elles bien utiles&nbsp;? Assur&eacute;ment&hellip; mais assimiler
MPI 2.0 est comparable &agrave; r&eacute;apprendre depuis z&eacute;ro un langage de
programmation totalement nouveau.
</p></dd><dt><span class="term">Conception de l'interface utilisateur.</span></dt><dd><p>
MPI a &eacute;t&eacute; con&ccedil;u apr&egrave;s PVM, et en a incontestablement tir&eacute; les le&ccedil;ons.
MPI offre une gestion des tampons plus simple et plus efficace et
une couche d'abstraction de haut-niveau permettant de transmettre
des donn&eacute;es d&eacute;finies par l'utilisateur comme des messages.
</p></dd><dt><span class="term">Force de loi.</span></dt><dd><p>
Pour ce que j'ai pu en voir, il existe toujours plus d'applications
con&ccedil;ues autour de PVM qu'autour de MPI. N&eacute;anmoins, porter celles-ci
vers MPI est chose facile, et le fait que MPI soit soutenu par un
standard formel tr&egrave;s r&eacute;pandu signifie que MPI est, pour un certain nombre
d'institutions, une question de vision des choses.
</p></dd></dl></div><p>
Conclusion&nbsp;? Disons qu'il existe au moins trois versions de MPI
d&eacute;velopp&eacute;es de fa&ccedil;on ind&eacute;pendante et disponibles gratuitement pouvant
fonctionner sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de machines Linux (et j'ai &eacute;crit l'un
d'eux)&nbsp;:
</p><div class="itemizedlist"><ul type="disc"><li><p>
LAM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Local Area Multicomputer</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">MultiOrdinateur Local</span>&nbsp;&raquo;)
est une mise en &#339;uvre compl&egrave;te du standard 1.1. Il permet aux programmes
MPI de s'ex&eacute;cuter sur un syst&egrave;me Linux individuel ou au travers d'un
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de syst&egrave;mes Linux communiquant par le biais de <span class="foreignphrase"><em class="foreignphrase">sockets</em></span> TCP/UDP.
Le syst&egrave;me inclut des facilit&eacute;s de base de contr&ocirc;le de l'ex&eacute;cution, ainsi que
toute une gamme d'outils de d&eacute;veloppement et de d&eacute;bogage de programmes.
</p></li><li><p>

MPICH (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">MPI CHameleon</em></span></span>&nbsp;&raquo;) est 
con&ccedil;u pour &ecirc;tre une impl&eacute;mentation compl&egrave;te et hautement portable du 
standard MPI 1.1. Tout comme LAM, il permet aux programmes MPI d'&ecirc;tre 
ex&eacute;cut&eacute;s sur des syst&egrave;mes Linux individuels ou en 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> via une communication par 
<span class="foreignphrase"><em class="foreignphrase">socket</em></span> TCP/UDP. En revanche, l'accent est 
port&eacute; sur la promotion de MPI en fournissant une impl&eacute;mentation efficace 
et facilement repositionnable. Pour porter cette impl&eacute;mentation, il faut 
r&eacute;impl&eacute;menter soit les cinq fonctions de la &laquo;&nbsp;<span class="quote">channel 
interface</span>&nbsp;&raquo;, soit, pour de meilleures performances, la totalit&eacute; de 
l'ADI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Abstract Device 
Interface</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Interface P&eacute;riph&eacute;rique 
Abstraite</span>&nbsp;&raquo;). MPICH, et beaucoup d'informations concernant ce 
sujet et la fa&ccedil;on de le porter, sont disponibles sur <a href="http://www.mcs.anl.gov/mpi/mpich/" target="_top">http://www.mcs.anl.gov/mpi/mpich/</a>.

</p></li><li><p>

AFMPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Aggregate Function 
MPI</em></span></span>&nbsp;&raquo;) est une sous-impl&eacute;mentation du standard MPI 
2.0. C'est celle que j'ai &eacute;crite. S'appuyant sur AFAPI, elle est con&ccedil;ue 
pour &ecirc;tre la vitrine des RMA et des fonctions de communications 
collectives, et n'offre donc qu'un soutien minimal des types MPI, de ses 
syst&egrave;mes de communication, et c&aelig;tera. Elle permet &agrave; des programmes C 
utilisant MPI d'&ecirc;tre ex&eacute;cut&eacute;s sur un syst&egrave;me Linux individuel ou au 
travers d'un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> mis en r&eacute;seau par du 
mat&eacute;riel pouvant prendre en charge l'AFAPI.

</p></li></ul></div><p>
Quelque soit l'impl&eacute;mentation MPI utilis&eacute;e, il est toujours tr&egrave;s simple
d'effectuer la plupart des types de communication.
</p><p>
En revanche, MPI 2.0 incorpore plusieurs paradigmes de communication
suffisamment diff&eacute;rents entre eux fondamentalement pour qu'un programmeur
utilisant l'un d'entre eux puisse ne m&ecirc;me pas reconna&icirc;tre les autres
comme &eacute;tant des styles de programmation MPI. Aussi, plut&ocirc;t que d'explorer
un seul exemple de programme, il est utile de passer en revue un exemple de chacun
des (diff&eacute;rents) paradigmes de communication de MPI. Tous les programmes
qui suivent emploient le m&ecirc;me algorithme (celui de la section 1.3)
utilis&eacute; pour calculer Pi.
</p><p>
Le premier programme MPI utilise les appels de <span class="foreignphrase"><em class="foreignphrase">message-passing</em></span> MPI sur
chaque processeur pour que celui-ci renvoie son r&eacute;sultat partiel au
processeur 0, qui fait la somme de tous ces r&eacute;sultats et la renvoie &agrave;
l'&eacute;cran&nbsp;:
</p><p>

<pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;mpi.h&gt;

main(int argc, char **argv)
{
  register double largeur;
  double somme, sommelocale;
  register int intervalles, i;
  int nproc, iproc;
  MPI_Status status;

  if (MPI_Init(&amp;argc, &amp;argv) != MPI_SUCCESS) exit(1);
  MPI_Comm_size(MPI_COMM_WORLD, &amp;nproc);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;iproc);
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=nproc) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;
  if (iproc != 0) {
    MPI_Send(&amp;lbuf, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
  } else {
    somme = sommelocale;
    for (i=1; i&lt;nproc; ++i) {
      MPI_Recv(&amp;lbuf, 1, MPI_DOUBLE, MPI_ANY_SOURCE,
               MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status);
      somme += sommelocale;
    }
    printf("Estimation de la valeur de pi: %f\n", somme);
  }
  MPI_Finalize();
  return(0);
}
</pre>

</p><p>
Le second programme MPI utilise les communications collectives (qui,
pour ce cas pr&eacute;cis, sont incontestablement les plus appropri&eacute;es)&nbsp;:
</p><p>

<pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;mpi.h&gt;

main(int argc, char **argv)
{
  register double largeur;
  double somme, sommelocale;
  register int intervalles, i;
  int nproc, iproc;

  if (MPI_Init(&amp;argc, &amp;argv) != MPI_SUCCESS) exit(1);
  MPI_Comm_size(MPI_COMM_WORLD, &amp;nproc);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;iproc);
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=nproc) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;
  MPI_Reduce(&amp;sommelocale, &amp;somme, 1, MPI_DOUBLE,
             MPI_SUM, 0, MPI_COMM_WORLD);
  if (iproc == 0) {
    printf("Estimation de la valeur de pi: %f\n", somme);
  }
  MPI_Finalize();
  return(0);
}
</pre>

</p><p>
La troisi&egrave;me version MPI utilise le m&eacute;canisme RMA de MPI&nbsp;2.0 sur chaque processeur
pour ajouter la valeur locale de la variable <code class="literal">sommelocale</code> de ce dernier
&agrave; la variable <code class="literal">somme</code> du processeur 0&nbsp;:
</p><p>

<pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;mpi.h&gt;

main(int argc, char **argv)
{
  register double largeur;
  double somme = 0, sommelocale;
  register int intervalles, i;
  int nproc, iproc;
  MPI_Win somme_fen;

  if (MPI_Init(&amp;argc, &amp;argv) != MPI_SUCCESS) exit(1);
  MPI_Comm_size(MPI_COMM_WORLD, &amp;nproc);
  MPI_Comm_rank(MPI_COMM_WORLD, &amp;iproc);
  MPI_Win_create(&amp;somme, sizeof(somme), sizeof(somme),
                 0, MPI_COMM_WORLD, &amp;somme_fen);
  MPI_Win_fence(0, somme_fen);
  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;
  sommelocale = 0;
  for (i=iproc; i&lt;intervalles; i+=nproc) {
    register double x = (i + 0.5) * largeur;
    sommelocale += 4.0 / (1.0 + x * x);
  }
  sommelocale *= largeur;
  MPI_Accumulate(&amp;sommelocale, 1, MPI_DOUBLE, 0, 0,
                 1, MPI_DOUBLE, MPI_SUM, somme_fen);
  MPI_Win_fence(0, somme_fen);
  if (iproc == 0) {
    printf("Estimation de la valeur de pi: %f\n", somme);
  }
  MPI_Finalize();
  return(0);
}
</pre>

</p><p>
Il est utile de pr&eacute;ciser que le m&eacute;canisme RMA de MPI 2.0 pr&eacute;vient
de fa&ccedil;on remarquable tout probl&egrave;me de structures de donn&eacute;es se trouvant
&agrave; des adresses m&eacute;moires diff&eacute;rentes selon les processeurs, en se r&eacute;f&eacute;rant
&agrave; une "fen&ecirc;tre" incluant l'adresse de base, une protection contre les
acc&egrave;s m&eacute;moire hors de port&eacute;e, et m&ecirc;me le r&eacute;&eacute;chelonnement d'adresse. &Agrave; une
impl&eacute;mentation efficace, s'ajoute le fait qu'un traitement RMA peut-&ecirc;tre
report&eacute; jusqu'&agrave; la prochaine <code class="literal">MPI__Win_fence</code>.
Pour faire simple, le m&eacute;canisme RMA est un &eacute;trange croisement entre
m&eacute;moire partag&eacute;e distribu&eacute;e et <span class="foreignphrase"><em class="foreignphrase">message passing</em></span>,
mais reste une interface tr&egrave;s propre pouvant g&eacute;n&eacute;rer des communications tr&egrave;s efficaces.
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N10FF0"></a>3.6.&nbsp;AFAPI (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Aggregate Function API</em></span></span>&nbsp;&raquo;)</h3></div></div></div><p>

Contrairement &agrave; PVM, MPI, et c&aelig;tera, l'interface AFAPI 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Aggregate Function API</em></span></span>&nbsp;&raquo;, 
ou &laquo;&nbsp;<span class="quote">Interface &agrave; Fonctions d'Agr&eacute;gation</span>&nbsp;&raquo;) n'a pas d&eacute;but&eacute; sa 
vie en tant que couche d'abstraction portable s'appuyant sur un r&eacute;seau 
mat&eacute;riel ou logiciel existant. AFAPI &eacute;tait plut&ocirc;t la biblioth&egrave;que de 
gestion bas niveau d'un mat&eacute;riel sp&eacute;cifique pour PAPERS 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Purdue's Adapter for Parallel Execution and Rapid 
Synchronization</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Adaptateur pour 
l'Ex&eacute;cution en Parall&egrave;le et la Synchronisation Rapide de l'universit&eacute; de 
Purdue</span>&nbsp;&raquo;).

</p><p>
PAPERS a &eacute;t&eacute; rapidement pr&eacute;sent&eacute; dans la section 3.2. Il s'agit d'un
r&eacute;seau &agrave; fonction d'agr&eacute;gations con&ccedil;u sur mesure dont le mod&egrave;le est dans
domaine public et qui pr&eacute;sente des temps de latence inf&eacute;rieurs &agrave; quelques
microsecondes. Mais surtout, il s'agit de la tentative de construction
d'un supercalculateur formant une meilleure cible pour la technologie des
compilateurs que les supercalculateurs d&eacute;j&agrave; existants. Il se distingue
en qualit&eacute; de la plupart des efforts en mati&egrave;re de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux et de
PVM/MPI, qui s'attachent g&eacute;n&eacute;ralement &agrave; essayer d'exploiter les r&eacute;seaux
standard au profit des rares applications en parall&egrave;le pr&eacute;sentant une
granularit&eacute; suffisante. Le fait que les &eacute;l&eacute;ments de PAPERS soient des
machines PC sous Linux ne sert qu'&agrave; permettre l'impl&eacute;mentation de
prototypes &agrave; des co&ucirc;ts les plus avantageux possibles.
</p><p>
La n&eacute;cessit&eacute; d'avoir une interface logicielle de bas niveau commune &agrave;
plus d'une douzaine d'impl&eacute;mentations diff&eacute;rentes d'un prototype a
conduit la biblioth&egrave;que PAPERS &agrave; &ecirc;tre standardis&eacute;e sous le nom d'AFAPI.
Mais le mod&egrave;le utilis&eacute; par AFAPI est simple en lui-m&ecirc;me et bien plus
adapt&eacute; aux interactions &agrave; la granularit&eacute; plus fine, typiquement du code
compil&eacute; par des compilateurs parall&eacute;lis&eacute;s, ou &eacute;crit pour des architectures
SIMD. Non seulement la simplicit&eacute; du mod&egrave;le rend les machines PAPERS
ais&eacute;es &agrave; construire, mais elle apporte &eacute;galement une efficacit&eacute; surprenante
aux ports d'AFAPI sur diff&eacute;rents types de syst&egrave;me, tels que les SMP.
</p><p>
AFAPI fonctionne actuellement sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux utilisant TTL_PAPERS,
CAPERS ou WAPERS. Elle fonctionne &eacute;galement (sans appel syst&egrave;me ni m&ecirc;me
instruction de verrouillage de bus, voir la section 2.2) sur les machines
SMP utilisant une biblioth&egrave;que de gestion de m&eacute;moire partag&eacute;e type System V
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">System V Shared Memory</em></span></span>&nbsp;&raquo;) appel&eacute;e SHMAPERS. Une version fonctionnant sur
des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux utilisant la diffusion UDP sur des r&eacute;seaux conventionnels
(Ex&nbsp;: Ethernet) est en cours de d&eacute;veloppement. Toutes les versions d'AFAPI sont &eacute;crites
pour &ecirc;tre appel&eacute;es &agrave; partir des langages C ou C++.
</p><p>
L'exemple suivant est la version AFAPI du programme de calcul de
Pi d&eacute;crit dans la section 1.3.
</p><p>

<pre class="programlisting">
#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include "afapi.h"

main(int argc, char **argv)
{
  register double largeur, somme;
  register int intervalles, i;

  if (p_init()) exit(1);

  intervalles = atoi(argv[1]);
  largeur = 1.0 / intervalles;

  sum = 0;
  for (i=IPROC; i&lt;intervalles; i+=NPROC) {
    register double x = (i + 0.5) * largeur;
    somme += 4.0 / (1.0 + x * x);
  }

  somme = p_reduceAdd64f(somme) * largeur;

  if (IPROC == CPROC) {
    printf("Estimation de la valeur de pi: %f\n", somme);
  }

  p_exit();
  return(0);
}
</pre>

</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N11022"></a>3.7.&nbsp;Autres biblioth&egrave;ques de gestion de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span></h3></div></div></div><p>

Outre PVM, MPI et AFAPI, les biblioth&egrave;ques suivantes proposent des 
services qui peuvent s'av&eacute;rer utiles au travers de grappes de machines 
Linux. Ces syst&egrave;mes sont trait&eacute;s ici de mani&egrave;re moins approfondie 
simplement parce que, contrairement &agrave; PVM, MPI et AFAPI, je n'ai que 
peu, voire aucune exp&eacute;rience pratique de l'utilisation de ceux-ci sur 
des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux. Si l'une de ces 
biblioth&egrave;ques (ou m&ecirc;me d'autres) vous est particuli&egrave;rement utile, merci 
de m'envoyer un courrier &eacute;lectronique en anglais &agrave;

<code class="email">&lt;<a href="mailto:hankd CHEZ engr POINT uky POINT edu">hankd CHEZ engr POINT uky POINT edu</a>&gt;</code>

en me d&eacute;taillant vos d&eacute;couvertes. J'envisagerai alors d'ajouter une 
section plus compl&egrave;te &agrave; son sujet.

</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1102F"></a>3.7.1.&nbsp;Condor (migration de processus)</h4></div></div></div><p>

Condor est un syst&egrave;me de gestion de ressources distribu&eacute;es qui peut 
diriger de vastes <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de stations de 
travail h&eacute;t&eacute;rog&egrave;nes. Sa conception a &eacute;t&eacute; motiv&eacute;e par les besoins des 
utilisateurs souhaitant utiliser la puissance inexploit&eacute;e de tels 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> au profit de leurs t&acirc;ches aux 
temps d'ex&eacute;cution prolong&eacute;s et aux calculs intensifs. Condor reproduit 
dans une large mesure l'environnement de la machine initiale sur celle 
qui ex&eacute;cute le processus, m&ecirc;me si ces deux machines ne partagent pas un 
syst&egrave;me de fichier ou un m&eacute;canisme de mot de passe communs. Les t&acirc;ches 
sous Condor qui se r&eacute;sument &agrave; un processus unique sont automatiquement 
intercept&eacute;es et d&eacute;plac&eacute;es entre les diff&eacute;rentes stations en fonctions 
des besoins pour les mener &agrave; terme.

</p><p>

Condor est disponible sur <a href="http://www.cs.wisc.edu/condor/" target="_top">http://www.cs.wisc.edu/condor/</a>. 
Une version Linux existe &eacute;galement. Contactez l'administrateur du site,

<code class="email">&lt;<a href="mailto:condor TIRET admin CHEZ cs POINT wisc POINT edu">condor TIRET admin CHEZ cs POINT wisc POINT edu</a>&gt;</code>,

pour plus de d&eacute;tails.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11042"></a>3.7.2.&nbsp;DFN-RPC (R&eacute;seau Allemand de la Recherche &mdash; &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Remote Procedure Call</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

Le DFN-RPC (un outil du R&eacute;seau Allemand de la Recherche &mdash; 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Remote Procedure Call</em></span></span>&nbsp;&raquo;) a 
&eacute;t&eacute; d&eacute;velopp&eacute; pour distribuer et parall&eacute;liser des applications d'int&eacute;ret 
scientifique ou technique entre une station de travail et un serveur de 
calcul ou un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>. L'interface est 
optimis&eacute;e pour les applications &eacute;crites en Fortran, mais le DFN-RPC peut 
aussi &ecirc;tre utilis&eacute; dans un environnement de langage C. Une version Linux 
a &eacute;t&eacute; &eacute;crite. Plus d'information sur <a href="ftp://ftp.uni-stuttgart.de/pub/rus/dfn_rpc/README_dfnrpc.html" target="_top">ftp://ftp.uni-stuttgart.de/pub/rus/dfn_rpc/README_dfnrpc.html</a>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11055"></a>3.7.3.&nbsp;DQS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Distributed Queueing System</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

Pas vraiment une biblioth&egrave;que, DQS 3.0 (&laquo;&nbsp;<span class="quote">Distributed Queueing 
System</span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">Syst&egrave;me de Files d'attente 
Distribu&eacute;es</span>&nbsp;&raquo;) est un syst&egrave;me de mise en file d'attente des t&acirc;ches 
qui a &eacute;t&eacute; d&eacute;velopp&eacute; et test&eacute; sous Linux. Ce syst&egrave;me a &eacute;t&eacute; con&ccedil;u pour 
permettre &agrave; la fois l'utilisation et l'administration d'un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de machines h&eacute;t&eacute;rog&egrave;nes comme une 
seule entit&eacute;. Disponible sur <a href="http://www.scri.fsu.edu/~pasko/dqs.html" target="_top">http://www.scri.fsu.edu/~pasko/dqs.html</a>.

</p><p>
Il existe aussi une version commerciale nomm&eacute;e CODINE 4.1.1 (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">COmputing in
DIstributed Network Environments</em></span></span>&nbsp;&raquo;, soit &laquo;&nbsp;<span class="quote">CAlcul en Environnement R&eacute;seau Distribu&eacute;</span>&nbsp;&raquo;).
</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N11073"></a>3.8.&nbsp;R&eacute;f&eacute;rences g&eacute;n&eacute;rales aux <span class="foreignphrase"><em class="foreignphrase">clusters</em></span></h3></div></div></div><p>
Les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> peuvent &ecirc;tre construits et utilis&eacute;s de tellement
de mani&egrave;res diff&eacute;rentes que certains groupes ont apport&eacute; des
contributions particuli&egrave;rement int&eacute;ressantes. Ce qui suit fait r&eacute;f&eacute;rence aux
diff&eacute;rents projets li&eacute;s &agrave; la mise en place de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> pouvant
avoir un int&eacute;r&ecirc;t d'ordre g&eacute;n&eacute;ral. Ceci inclut un m&eacute;lange de
r&eacute;f&eacute;rences &agrave; des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> sp&eacute;cifiques &agrave; Linux et &agrave; des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>
g&eacute;n&eacute;riques. Cette liste est pr&eacute;sent&eacute;e dans l'ordre alphab&eacute;tique.
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11086"></a>3.8.1.&nbsp;Beowulf</h4></div></div></div><p>

Le projet <a href="http://www.beowulf.org/" target="_top">Beowulf</a>, se 
focalise sur la production de logiciels pour une utilisation de stations 
de travail imm&eacute;diatement disponibles bas&eacute;e sur du mat&eacute;riel PC de grande 
distribution, un r&eacute;seau &agrave; haut d&eacute;bit interne au 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span>, et le syst&egrave;me d'exploitation 
Linux.

</p><p>
Thomas Sterling a &eacute;t&eacute; le principal acteur de Beowulf, et continue
d'&ecirc;tre un promoteur franc et &eacute;loquent de l'utilisation de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>
Linux dans le domaine du calcul scientifique en g&eacute;n&eacute;ral. &Agrave; vrai dire,
plusieurs groupes parlent &agrave; pr&eacute;sent de leur <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> comme de syst&egrave;me
de &laquo;&nbsp;<span class="quote">classe Beowulf</span>&nbsp;&raquo;, et ce m&ecirc;me si la conception de ce <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>
s'&eacute;loigne du mod&egrave;le Beowulf officiel.
</p><p>
Don Becker, apportant son appui au projet Beowulf, a produit nombre
des pilotes r&eacute;seau utilis&eacute;s par Linux en g&eacute;n&eacute;ral. Plusieurs de ces
pilotes ont m&ecirc;me &eacute;t&eacute; adapt&eacute;s pour &ecirc;tre utilis&eacute;s sous BSD. C'est
&eacute;galement &agrave; Don que l'on doit la possibilit&eacute;, pour certains pilotes,
de r&eacute;partir le trafic r&eacute;seau &agrave; travers plusieurs connexions parall&egrave;les
pour augmenter les taux de transfert sans utiliser d'on&eacute;reux commutateurs.
Ce type de r&eacute;partition de la charge r&eacute;seau &eacute;tait le principal atout des
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Beowulf.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N110A5"></a>3.8.2.&nbsp;Linux/AP+</h4></div></div></div><p>

Le projet <a href="http://cap.anu.edu.au/cap/projects/linux" target="_top">Linux/AP+</a> ne 
concerne pas exactement le <span class="emphasis"><em>clustering</em></span> sous Linux, 
mais s'attache &agrave; faire fonctionner Linux sur l'AP1000+ de Fujitsu, et &agrave; 
y apporter les am&eacute;liorations appropri&eacute;es en mati&egrave;re de traitement en 
parall&egrave;le. L'AP1000+ est une machine en parall&egrave;le &agrave; base de SPARC et 
disponible dans le commerce, utilisant un r&eacute;seau sp&eacute;cifique avec une 
topologie en tore, un taux de transfert de 25Mo/s et un temps de latence 
de 10 microsecondes&hellip; Pour faire court, cela ressemble beaucoup &agrave; 
un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> Linux SPARC.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N110B4"></a>3.8.3.&nbsp;Locust</h4></div></div></div><p>
Le projet Locust est en train de mettre au point un syst&egrave;me de m&eacute;moire partag&eacute;e
virtuelle qui utilise les informations obtenues &agrave; la compilation pour
masquer les temps de latence des messages et r&eacute;duire le trafic r&eacute;seau
lors de l'ex&eacute;cution. &laquo;&nbsp;<span class="quote">Pupa</span>&nbsp;&raquo; forme la base du syst&egrave;me de communication de
Locust, et est impl&eacute;ment&eacute; &agrave; l'aide d'un r&eacute;seau Ethernet reliant des
machines PC 486 sous FreeBSD. Et Linux&nbsp;?
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N110BC"></a>3.8.4.&nbsp;Midway DSM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Distributed Shared Memory</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>
<a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/midway/WWW/HomePage.html" target="_top">Midway</a> est une DSM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Distributed Shared Memory</em></span></span>&nbsp;&raquo;,
soit &laquo;&nbsp;<span class="quote">M&eacute;moire Partag&eacute;e Distribu&eacute;e</span>&nbsp;&raquo;) logicielle, similaire &agrave; TreadMarks.
Le bon cot&eacute; r&eacute;side en l'utilisation d'indications &agrave; la compilation plut&ocirc;t que de relativement
lents m&eacute;canismes d'erreur de page, et en sa gratuit&eacute;. Le mauvais cot&eacute; est cela ne fonctionne
pas sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N110D3"></a>3.8.5.&nbsp;Mosix</h4></div></div></div><p>

MOSIX apporte des modifications au syst&egrave;me d'exploitation BSD 
&laquo;&nbsp;<span class="quote">BSDI</span>&nbsp;&raquo; pour proposer une r&eacute;partition de charge r&eacute;seau 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">load balancing</em></span></span>&nbsp;&raquo;) dynamique 
et une migration de processus pr&eacute;emptive au travers d'un groupe de PC 
mis en r&eacute;seau. C'est un syst&egrave;me tr&egrave;s utile non seulement pour le 
traitement en parall&egrave;le, mais d'une mani&egrave;re g&eacute;n&eacute;rale pour utiliser un 
<span class="foreignphrase"><em class="foreignphrase">cluster</em></span> comme une machine SMP &eacute;volutive. 
Y aura-t-il une version Linux&nbsp;? Voyez <a href="http://www.mosix.org" target="_top">http://www.mosix.org</a> pour plus d'informations<sup>[<a href="#ftn.N110E5" name="N110E5">22</a>]</sup>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N110E9"></a>3.8.6.&nbsp;NOW (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Network Of Workstations</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>
Le projet NOW (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Network Of Workstations</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">R&eacute;seau de Stations de Travail</span>&nbsp;&raquo;) de l'universit&eacute; de Berkeley (<a href="http://now.cs.berkeley.edu/" target="_top">http://now.cs.berkeley.edu</a>) a conduit dans une large mesure l'effort pour le calcul en
parall&egrave;le en utilisant des r&eacute;seaux de stations de travail. Bon
nombre de travaux sont men&eacute;s l&agrave;-bas, tous tourn&eacute;s vers la
&laquo;&nbsp;<span class="quote">d&eacute;monstration en pratique d'un syst&egrave;me &agrave; 100 processeurs dans
les prochaines ann&eacute;es</span>&nbsp;&raquo;. H&eacute;las, ils n'utilisent pas Linux.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11100"></a>3.8.7.&nbsp;Traitement en parall&egrave;le avec Linux</h4></div></div></div><p>
Le site web du &laquo;&nbsp;<span class="quote">Traitement en parall&egrave;le avec Linux</span>&nbsp;&raquo;
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel processing using Linux</em></span></span>&nbsp;&raquo;), sur <a href="http://yara.ecn.purdue.edu/~pplinux" target="_top">http://yara.ecn.purdue.edu/~pplinux</a>&gt;, est le site officiel
de ce guide pratique et de plusieurs documents en rapport avec ce th&egrave;me,
y compris des pr&eacute;sentations en ligne.
Parall&egrave;lement aux travaux du projet PAPERS, l'&Eacute;cole Sup&eacute;rieure
d'&Eacute;lectricit&eacute; et d'Informatique de Purdue (&laquo;&nbsp;<span class="quote">Purdue
University School of Electrical and Computer Engineering</span>&nbsp;&raquo;) reste
un leader en mati&egrave;re de traitement en parall&egrave;le. Ce site a &eacute;t&eacute;
mis en place pour aider les autres &agrave; utiliser des PC sous Linux
pour faire du traitement en parall&egrave;le.
</p><p>
Depuis l'assemblage du premier <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> de PC Linux en f&eacute;vrier 1994,
bien d'autres furent &eacute;galement assembl&eacute;s &agrave; Purdue, dont plusieurs
&eacute;quip&eacute;s de murs vid&eacute;os. Bien que ces <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> s'appuyaient sur des
machines &agrave; base de microprocesseurs 386, 486 ou Pentium (mais pas de
Pentium Pro), Intel a r&eacute;cemment accord&eacute; &agrave; Purdue une donation qui lui
permettra de construire plusieurs grands <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de syst&egrave;mes &agrave;
Pentium II (avec pas moins de 165 machines par <span class="foreignphrase"><em class="foreignphrase">cluster</em></span>). M&ecirc;me si
tous ces <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> sont ou seront &eacute;quip&eacute;s de r&eacute;seaux PAPERS, la plupart
sont &eacute;galement dot&eacute;s de r&eacute;seaux conventionnels.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11124"></a>3.8.8.&nbsp;Pentium Pro Cluster Workshop</h4></div></div></div><p>
Les 10 et 11 avril 1997, le laboratoire AMES a tenu &agrave; Des Moines,
dans l'&eacute;tat de l'Iowa aux &Eacute;tats-Unis, le &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Pentium Pro Cluster
Workshop</em></span></span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote">Atelier de <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Pentium Pro</span>&nbsp;&raquo;). Le site web
de cet atelier, <a href="http://www.scl.ameslab.gov" target="_top">http://www.scl.ameslab.gov</a>, renferme une mine d'informations concernant les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> PC,
glan&eacute;es aupr&egrave;s de tous les participants.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1113A"></a>3.8.9.&nbsp;TreadMarks DSM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Distributed Shared Memory</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

La DSM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Distributed Shared 
Memory</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">M&eacute;moire Partag&eacute;e 
Distribu&eacute;e</span>&nbsp;&raquo;) est une technique avec laquelle un syst&egrave;me de 
<span class="foreignphrase"><em class="foreignphrase">message-passing</em></span> peut se pr&eacute;senter et agir 
comme un SMP. Il existe quelques syst&egrave;mes de ce genre, la plupart 
utilisant les m&eacute;canismes d'erreur de page du syst&egrave;me d'exploitation pour 
d&eacute;clencher la transmission des messages. <a href="http://www.cs.rice.edu/~willy/TreadMarks/overview.html" target="_top">TreadMarks</a> 
est l'un des plus efficaces, et fonctionne sur les 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux. La mauvaise nouvelle est 
que &laquo;&nbsp;<span class="quote">TreadMarks est distribu&eacute; &agrave; un co&ucirc;t r&eacute;duit aux universit&eacute;s et 
organisations &agrave; but non lucratif</span>&nbsp;&raquo;. Pour plus d'informations 
concernant le logiciel, prenez contact (en anglais) avec

<code class="email">&lt;<a href="mailto:tmk CHEZ cs POINT rice POINT edu">tmk CHEZ cs POINT rice POINT edu</a>&gt;</code>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1115A"></a>3.8.10.&nbsp;U-Net (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">User-level NETwork interface architecture</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

Le projet U-Net (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">User-level NETwork interface 
architecture</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">Architecture d'interface 
R&eacute;seau au Niveau Utilisateur</span>&nbsp;&raquo;), accessible sur <a href="http://www.eecs.harvard.edu/~mdw/proj/old/unet" target="_top">http://www.eecs.harvard.edu/~mdw/proj/old/unet</a>, tente d'apporter 
temps de latence r&eacute;duits et taux de transfert &eacute;lev&eacute;s sur du mat&eacute;riel 
r&eacute;seau du commerce en virtualisant les interfaces r&eacute;seau de mani&egrave;re &agrave; ce 
que les applications puissent envoyer et recevoir des messages sans 
passer par un appel syst&egrave;me. U-Net fonctionne sur des PC Linux en 
utilisant du mat&eacute;riel Fast Ethernet bas&eacute; sur une puce DEC DC21140, ou 
une carte ATM Fore Systems PCA-200 (mais pas PCA-200E).

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1116D"></a>3.8.11.&nbsp;WWT (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Wisconsin Wind Tunnel</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

On trouve bon nombre de projets relatifs &agrave; l'utilisation de 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> dans le Wisconsin. Le projet WWT 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Wisconsin Wind Tunnel</em></span></span>&nbsp;&raquo;, ou 
&laquo;&nbsp;<span class="quote">Soufflerie du Wisconsin</span>&nbsp;&raquo;), sur <a href="http://www.cs.wisc.edu/~wwt/" target="_top">http://www.cs.wisc.edu/~wwt/</a>, m&egrave;ne toutes sortes de travaux 
orient&eacute;s vers le d&eacute;veloppement d'une interface &laquo;&nbsp;<span class="quote">standard</span>&nbsp;&raquo; 
entre les compilateurs et le mat&eacute;riel r&eacute;seau sur lequel ils s'appuient. 
Il existe le &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Wisconsin COW</em></span></span>&nbsp;&raquo; 
(pour &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Cluster Of 
Workstation</em></span></span>&nbsp;&raquo;), &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Cooperative 
Shared Memory</em></span></span>&nbsp;&raquo; et 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Tempest</em></span></span>&nbsp;&raquo;, le 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Paradyn Parallel Performance 
Tools</em></span></span>&nbsp;&raquo;, et c&aelig;tera. Malheureusement, il n'y a pas 
grand chose concernant Linux.

</p></div></div></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N1119A"></a>4.&nbsp;SIMD <span class="foreignphrase"><em class="foreignphrase">Within A Register</em></span>&nbsp;: SWAR (Ex&nbsp;: utilisation de MMX)</h2></div></div></div><p>

Le SIMD (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Single Instruction stream, Multiple Data 
stream</em></span></span>&nbsp;&raquo; ou &laquo;&nbsp;<span class="quote">Un seul flux d'instruction, 
plusieurs flux de donn&eacute;es</span>&nbsp;&raquo;) &agrave; l'Int&eacute;rieur d'Un Registre (ou 
&laquo;&nbsp;<span class="quote">SIMD <span class="foreignphrase"><em class="foreignphrase">Within A 
Register</em></span></span>&nbsp;&raquo;&nbsp;: SWAR) n'est pas un concept 
r&eacute;cent. En consid&eacute;rant une machine dot&eacute;e de registres, bus de donn&eacute;es et 
unit&eacute;s de fonctions de <span class="emphasis"><em>k</em></span> bits, il est connu depuis 
longtemps que les op&eacute;rations sur les registres ordinaires peuvent se 
comporter comme des op&eacute;rations parall&egrave;les SIMD sur 
<span class="emphasis"><em>n</em></span> champs de 
<span class="emphasis"><em>k</em></span>/<span class="emphasis"><em>n</em></span> bits chacun. Ce n'est en 
revanche qu'avec les efforts r&eacute;cents en mati&egrave;re de multim&eacute;dia que 
l'acc&eacute;l&eacute;ration d'un facteur deux &agrave; huit apport&eacute;e par les techniques SWAR 
a commenc&eacute; &agrave; concerner l'informatique g&eacute;n&eacute;rale. Les versions de 1997 de 
la plupart des microprocesseurs incorporent une prise en charge 
mat&eacute;rielle du SWAR<sup>[<a href="#ftn.N111BA" name="N111BA">23</a>]</sup>&nbsp;:

</p><p>

<div class="itemizedlist"><ul type="disc"><li><p>
<a href="http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/20726.pdf" target="_top">AMD K6 MMX (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">MultiMedia eXtensions</em></span></span>&nbsp;&raquo;)</a>

</p></li><li><p>
<a href="http://www.sun.com/sparc/vis/index.html" target="_top">Sun SPARC V9 VIS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Visual Instruction Set</em></span></span>&nbsp;&raquo;)</a>
</p></li></ul></div>

</p><p>
Il existe quelques lacunes dans la prise en charge mat&eacute;rielle
apport&eacute;e par les nouveaux microprocesseurs, des caprices comme
par exemple la prise en charge d'un nombre limit&eacute; d'instructions
pour certaines tailles de champ. Il est toutefois important de
garder &agrave; l'esprit que bon nombre d'op&eacute;rations SWAR peuvent se
passer d'acc&eacute;l&eacute;ration mat&eacute;rielle. Par exemple, les op&eacute;rations
au niveau du bit sont insensibles au partitionnement logique d'un
registre.
</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N111DA"></a>4.1.&nbsp;Quels usages pour le SWAR&nbsp;?</h3></div></div></div><p>
Bien que <span class="emphasis"><em>tout</em></span> processeur moderne soit capable
d'ex&eacute;cuter un programme en effectuant un minimum de parall&eacute;lisme
SWAR, il est de fait que m&ecirc;me le plus optimis&eacute; des jeux d'instructions
SWAR ne peut g&eacute;rer un parall&eacute;lisme d'int&eacute;r&ecirc;t vraiment g&eacute;n&eacute;ral. A dire
vrai, de nombreuses personnes ont remarqu&eacute; que les diff&eacute;rences de
performance entre un Pentium ordinaire et un Pentium &laquo;&nbsp;<span class="quote">avec technologie MMX</span>&nbsp;&raquo;
&eacute;taient surtout d&ucirc;es &agrave; certains d&eacute;tails, comme le fait que l'augmentation
de la taille du cache L1 co&iuml;ncide avec l'apparition du MMX. Alors,
concr&egrave;tement, &agrave; quoi le SWAR (ou le MMX) est-il utile&nbsp;?
</p><div class="itemizedlist"><ul type="disc"><li><p>

Dans le traitement des entiers, les plus courts &eacute;tant les meilleurs. 
Deux valeurs 32 bits tiennent dans un registre MMX 64 bits, mais forment 
aussi une cha&icirc;ne de huit caract&egrave;res, ou encore permettent de repr&eacute;senter un
&eacute;chiquier complet, &agrave; raison d'un bit par case. Note&nbsp;: il 
<span class="emphasis"><em>existera une version &laquo;&nbsp;<span class="quote">virgule flottante</span>&nbsp;&raquo; du 
MMX</em></span>, bien que tr&egrave;s peu de choses aient &eacute;t&eacute; dites &agrave; ce sujet. 
Cyrix a d&eacute;pos&eacute; une pr&eacute;sentation, <a href="ftp://ftp.cyrix.com/developr/mpf97rm.pdf" target="_top">ftp://ftp.cyrix.com/developr/mpf97rm.pdf</a>, qui contient quelques 
commentaires concernant <span class="emphasis"><em>MMFP</em></span>. Apparemment, MMFP 
pourra prendre en charge le chargement de nombres 32 bits en virgule 
flottante dans des registres MMX 64 bits. Ceci combin&eacute; &agrave; deux pipelines 
MMFP fournirait quatre FLOP <sup>[<a href="#ftn.N111F5" name="N111F5">24</a>]</sup> en simple pr&eacute;cision par cycle d'horloge.

</p></li><li><p>

Pour le SIMD, ou parall&eacute;lisme vectoris&eacute;. La m&ecirc;me op&eacute;ration s'applique &agrave; 
tous les champs, simultan&eacute;ment. Il existe des moyens d'annihiler ses 
effets sur des champs s&eacute;lectionn&eacute;s (l'&eacute;quivalent du 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">SIMD enable masking</em></span></span>&nbsp;&raquo;, ou 
&laquo;&nbsp;<span class="quote">activation du masquage</span>&nbsp;&raquo;), mais ils sont compliqu&eacute;s &agrave; 
mettre en &#339;uvre et gr&egrave;vent les performances.

</p></li><li><p>

Pour les cas o&ugrave; la r&eacute;f&eacute;rence &agrave; la m&eacute;moire se fait de mani&egrave;re localis&eacute;e 
et r&eacute;guli&egrave;re (et de pr&eacute;f&eacute;rence regroup&eacute;e). Le SWAR en g&eacute;n&eacute;ral, et le MMX 
en particulier se r&eacute;v&egrave;lent d&eacute;sastreux sur les acc&egrave;s al&eacute;atoires. 
Rassembler le contenu d'un vecteur <code class="literal">x[y]</code> (o&ugrave; 
<code class="literal">y</code> est l'index d'un tableau) est extr&ecirc;mement co&ucirc;teux.

</p></li></ul></div><p>
Il existe donc d'importantes limitations, mais ce type de parall&eacute;lisme
intervient dans un certain nombre d'algorithmes, et pas seulement dans les
applications multim&eacute;dia. Lorsque l'algorithme est adapt&eacute;, le SWAR est plus
efficace que le SMP ou le parall&eacute;lisme en <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>&hellip; et son utilisation
ne co&ucirc;te rien&nbsp;!
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1121A"></a>4.2.&nbsp;Introduction &agrave; la programmation SWAR</h3></div></div></div><p>
Le concept de base du SWAR (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">SIMD Within A Register</em></span></span>&nbsp;&raquo;,
ou &laquo;&nbsp;<span class="quote">SIMD &agrave; l'int&eacute;rieur d'un registre</span>&nbsp;&raquo;) r&eacute;side dans le fait que l'on peut utiliser des op&eacute;rations sur
des registres de la taille d'un mot pour acc&eacute;l&eacute;rer les calculs en effectuant des
op&eacute;rations parall&egrave;les SIMD sur <span class="emphasis"><em>n</em></span> champs
de <span class="emphasis"><em>k</em></span>/<span class="emphasis"><em>n</em></span> bits. En revanche,
employer les techniques du SWAR peut parfois s'av&eacute;rer maladroit, et certaines de leurs
op&eacute;rations peuvent au final &ecirc;tre plus co&ucirc;teuses que leurs homologues en s&eacute;rie
car elles n&eacute;cessitent des instructions suppl&eacute;mentaires pour forcer le partitionnement
des champs.
</p><p>
Pour illustrer ce point, prenons l'exemple d'un m&eacute;canisme SWAR
grandement simplifi&eacute; g&eacute;rant quatre champs de 8 bits dans chaque
registre de 32 bits. Les valeurs de ces deux registres pourraient
&ecirc;tre repr&eacute;sent&eacute;es comme suit&nbsp;:
</p><p>

<pre class="programlisting">
         PE3     PE2     PE1     PE0
      +-------+-------+-------+-------+
Reg0  | D 7:0 | C 7:0 | B 7:0 | A 7:0 |
      +-------+-------+-------+-------+
Reg1  | H 7:0 | G 7:0 | F 7:0 | E 7:0 |
      +-------+-------+-------+-------+
</pre>

</p><p>

Ceci indique simplement que chaque registre est vu essentiellement comme 
un vecteur de quatre valeurs enti&egrave;res 8 bits ind&eacute;pendantes. 
Alternativement, les valeurs <code class="literal">A</code> et 
<code class="literal">E</code> peuvent &ecirc;tre vues comme les valeurs, dans Reg0 et 
Reg1, de l'&eacute;l&eacute;ment de traitement 0 (ou &laquo;&nbsp;<span class="quote">PE0</span>&nbsp;&raquo; pour 
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Processing Element 0</em></span></span>&nbsp;&raquo;), 
<code class="literal">B</code> et <code class="literal">F</code> comme celles de l'&eacute;l&eacute;ment 
1, et ainsi de suite.

</p><p>
Le reste de ce document passe rapidement en revue les classes de
base des op&eacute;rations parall&egrave;les SIMD sur ces vecteurs d'entiers et
la fa&ccedil;on dont ces fonctions peuvent &ecirc;tre impl&eacute;ment&eacute;es.
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11252"></a>4.2.1.&nbsp;Op&eacute;rations polymorphiques</h4></div></div></div><p>
Certaines op&eacute;rations SWAR peuvent &ecirc;tre effectu&eacute;es de fa&ccedil;on triviale
en utilisant des op&eacute;rations ordinaires sur des entiers 32 bits, sans
avoir &agrave; se demander si l'op&eacute;ration est r&eacute;ellement faite pour agir en
parall&egrave;le et ind&eacute;pendemment sur ces champs de 8 bits. On qualifie ce
genre d'op&eacute;ration SWAR de <span class="emphasis"><em>polymorphique</em></span>, parce que
la fonction est insensible aux types des champs (et &agrave; leur taille).
</p><p>
Tester si un champ quelconque est non-nul est une op&eacute;ration polymorphique,
tout comme les op&eacute;rations logiques au niveau du bit. Par exemple, un &laquo;&nbsp;<span class="quote">ET</span>&nbsp;&raquo;
logique bit-&agrave;-bit ordinaire (l'op&eacute;rateur &laquo;&nbsp;<span class="quote"><code class="literal">&amp;</code></span>&nbsp;&raquo;
du registre C) effectue son calcul bit-&agrave;-bit, quelque soit la taille des champs.
Un &laquo;&nbsp;<span class="quote">ET</span>&nbsp;&raquo; logique simple des registres ci-dessus donnerait&nbsp;:
</p><p>

<pre class="programlisting">
          PE3       PE2       PE1       PE0
      +---------+---------+---------+---------+
Reg2  | D&amp;H 7:0 | C&amp;G 7:0 | B&amp;F 7:0 | A&amp;E 7:0 |
      +---------+---------+---------+---------+
</pre>

</p><p>
Puisque le bit de r&eacute;sultat <span class="emphasis"><em>k</em></span> de l'op&eacute;ration &laquo;&nbsp;<span class="quote">ET</span>&nbsp;&raquo; logique
n'est affect&eacute; que par les valeurs des bits d'op&eacute;rande <span class="emphasis"><em>k</em></span>,
toutes les tailles de champs peuvent &ecirc;tre prises en charge par une m&ecirc;me instruction.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11278"></a>4.2.2.&nbsp;Op&eacute;rations partitionn&eacute;es</h4></div></div></div><p>
Malheureusement, de nombreuses et importantes op&eacute;rations SWAR ne sont
pas polymorphiques. Les op&eacute;rations arithm&eacute;tiques comme l'addition, la
soustraction, la multiplication et la division sont sujettes aux
interactions de la &laquo;&nbsp;<span class="quote">retenue</span>&nbsp;&raquo; entre les champs. Ces op&eacute;rations sont
dites <span class="emphasis"><em>partitionn&eacute;es</em></span> car chacune d'elles doit
cloisonner effectivement les op&eacute;randes et le r&eacute;sultat pour &eacute;viter les
interactions entre champs. Il existe toutefois trois m&eacute;thodes diff&eacute;rentes
pouvant &ecirc;tre employ&eacute;es pour parvenir &agrave; ces fins&nbsp;:
</p><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N11283"></a>4.2.2.1.&nbsp;Instructions partitionn&eacute;es</h5></div></div></div><p>
L'approche la plus &eacute;vidente pour impl&eacute;menter les op&eacute;rations partitionn&eacute;es
consiste peut-&ecirc;tre &agrave; proposer une prise en charge mat&eacute;rielle des
&laquo;&nbsp;<span class="quote">instructions parall&egrave;les partitionn&eacute;es</span>&nbsp;&raquo; coupant le syst&egrave;me de retenue
entre les champs. Cette approche peut offrir les performances les plus
&eacute;lev&eacute;es, mais elle n&eacute;cessite la modification du jeu d'instruction du
processeur et implique en g&eacute;n&eacute;ral des limitations sur la taille des
champs (Ex&nbsp;: ces instructions pourraient prendre en charge des champs larges
de 8 bits, mais pas de 12).
</p><p>
Le MMX des processeurs AMD, Cyrix et Intel, Le MAX de Digital, celui d'HP,
et le VIS de Sun impl&eacute;mentent tous des versions restreintes des instructions
partitionn&eacute;es. Malheureusement, ces diff&eacute;rents jeux d'instructions posent
des restrictions assez diff&eacute;rentes entre elles, ce qui rend les algorithmes
difficilement portables. &Eacute;tudions &agrave; titre d'exemple l'&eacute;chantillon d'op&eacute;rations
partitionn&eacute;es suivant&nbsp;:
</p><p>

<pre class="programlisting">
  Instruction           AMD/Cyrix/Intel MMX   DEC MAX   HP MAX   Sun VIS
+---------------------+---------------------+---------+--------+---------+
| Diff&eacute;rence Absolue  |                     |       8 |        |       8 |
+---------------------+---------------------+---------+--------+---------+
| Maximum             |                     |   8, 16 |        |         |
+---------------------+---------------------+---------+--------+---------+
| Comparaison         |           8, 16, 32 |         |        |  16, 32 |
+---------------------+---------------------+---------+--------+---------+
| Multiplication      |                  16 |         |        |    8x16 |
+---------------------+---------------------+---------+--------+---------+
| Addition            |           8, 16, 32 |         |     16 |  16, 32 |
+---------------------+---------------------+---------+--------+---------+
</pre>

</p><p>
Dans cette table, les chiffres indiquent les tailles de champ reconnues
par chaque op&eacute;ration. M&ecirc;me si la table exclut un certain nombre d'instructions
dont les plus exotiques, il est clair qu'il y a des diff&eacute;rences. Cela a pour effet
direct de rendre inefficaces les mod&egrave;les de programmation en langages de haut niveau,
et de s&eacute;v&egrave;rement restreindre la portabilit&eacute;.

</p></div><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N11295"></a>4.2.2.2.&nbsp;Op&eacute;rations non partitionn&eacute;es avec code de correction</h5></div></div></div><p>
Impl&eacute;menter des op&eacute;rations partitionn&eacute;es en utilisant des instructions
partitionn&eacute;es est certainement tr&egrave;s efficace, mais comment faire lorsque
l'op&eacute;ration dont vous avez besoin n'est pas prise en charge par le mat&eacute;riel&nbsp;?
R&eacute;ponse&nbsp;: utiliser une s&eacute;rie d'instructions ordinaires pour effectuer l'op&eacute;ration
malgr&eacute; les effets de la retenue entre les champs, puis effectuer la correction
de ces effets ind&eacute;sir&eacute;s.
</p><p>
C'est une approche purement logicielle, et les corrections apportent
bien entendu un surco&ucirc;t en temps, mais elle fonctionne pleinement avec
le partitionnement en g&eacute;n&eacute;ral. Cette approche est &eacute;galement &laquo;&nbsp;<span class="quote">g&eacute;n&eacute;rale</span>&nbsp;&raquo;
dans le sens o&ugrave; elle peut &ecirc;tre utilis&eacute;e soit pour combler les lacunes du
mat&eacute;riel dans le domaine des instructions partitionn&eacute;es, soit pour apporter
un soutien logiciel complet aux machines qui ne sont pas du tout dot&eacute;es
d'une prise en charge mat&eacute;rielle. &Agrave; dire vrai, en exprimant ces s&eacute;quences
de code dans un langage comme le C, on permet au SWAR d'&ecirc;tre totalement
portable.
</p><p>
Ceci soul&egrave;ve imm&eacute;diatement une question&nbsp;: quelle est pr&eacute;cis&eacute;ment l'inefficacit&eacute;
des op&eacute;rations SWAR partitionn&eacute;es simul&eacute;es &agrave; l'aide d'op&eacute;rations non partitionn&eacute;es&nbsp;?
Eh bien c'est tr&egrave;s certainement la question &agrave; 65536 dollars, mais certaines op&eacute;rations
ne sont pas aussi difficiles &agrave; mettre en &#339;uvre que l'on pourrait le croire.
</p><p>
Prenons en exemple le cas de l'impl&eacute;mentation de l'addition d'un vecteur de quatre
&eacute;l&eacute;ments 8 bits contenant des valeurs enti&egrave;res, soit <code class="literal">x</code>+<code class="literal">y</code>,
en utilisant des op&eacute;rations 32 bits ordinaires.
</p><p>
Une addition 32 bits ordinaire pourrait en fait rendre un r&eacute;sultat correct,
mais pas si la retenue d'un champ de 8 bits se reporte sur le champ suivant.
Aussi, notre but consiste simplement &agrave; faire en sorte qu'un tel report ne se produise
pas. Comme l'on est s&ucirc;r qu'additionner deux champs de <span class="emphasis"><em>k</em></span> bits
ne peut g&eacute;n&eacute;rer qu'un r&eacute;sultat large d'au plus <span class="emphasis"><em>k</em></span>+1 bits,
on peut garantir qu'aucun report ne va se produire en &laquo;&nbsp;<span class="quote">masquant</span>&nbsp;&raquo; simplement le
bit de poids fort de chaque champs. On fait cela en appliquant un &laquo;&nbsp;<span class="quote">ET</span>&nbsp;&raquo; logique
&agrave; chaque op&eacute;rande avec la valeur <code class="literal">0x7f7f7f7f</code>, puis
en effectuant un addition 32 bits habituelle.
</p><p>

<pre class="programlisting">
t = ((x &amp; 0x7f7f7f7f) + (y &amp; 0x7f7f7f7f));
</pre>

</p><p>
Ce r&eacute;sultat est correct&hellip; sauf pour le bit de poids fort de chacun
des champs. Il n'est question, pour calculer la valeur correcte, que
d'effectuer deux additions 1-bit partitionn&eacute;es, depuis <code class="literal">x</code>
et <code class="literal">y</code> vers le r&eacute;sultat &agrave; 7 bits calcul&eacute;
pour <code class="literal">t</code>. Heureusement, une addition
partitionn&eacute;e sur 1 bit peut &ecirc;tre impl&eacute;ment&eacute;e &agrave; l'aide d'une
op&eacute;ration &laquo;&nbsp;<span class="quote">OU Exclusif</span>&nbsp;&raquo; logique ordinaire. Ainsi, le r&eacute;sultat est
tout simplement&nbsp;:
</p><p>

<pre class="programlisting">
(t ^ ((x ^ y) &amp; 0x80808080))
</pre>

</p><p>
D'accord. Peut-&ecirc;tre n'est-ce pas si simple, en fin de compte. Apr&egrave;s tout, cela fait six
op&eacute;rations pour seulement quatre additions. En revanche, on remarquera
que ce nombre d'op&eacute;rations n'est pas fonction du nombre de champs. Donc,
avec plus de champs, on gagne en rapidit&eacute;. &Agrave; dire vrai, il se peut que
l'on gagne quand m&ecirc;me en vitesse simplement parce que les champs sont
charg&eacute;s et red&eacute;pos&eacute;s en une seule op&eacute;ration (vectoris&eacute;e sur des entiers),
que la disponibilit&eacute; des registres est optimis&eacute;e, et parce qu'il y a
moins de code dynamique engendrant des d&eacute;pendances (parce que l'on &eacute;vite
les r&eacute;f&eacute;rences &agrave; des mots incomplets).
</p></div><div class="sect4" lang="fr"><div class="titlepage"><div><div><h5 class="title"><a name="N112DC"></a>4.2.2.3.&nbsp;Contr&ocirc;ler les valeurs des champs</h5></div></div></div><p>
Alors que les deux autres approches de l'impl&eacute;mentation des op&eacute;rations
partitionn&eacute;es se centrent sur l'exploitation du maximum d'espace possible
dans les registres, il peut &ecirc;tre, au niveau du calcul, plus efficace de
contr&ocirc;ler les valeurs des champs de fa&ccedil;on &agrave; ce que l'interaction
de la retenue entre ceux-ci ne se produise jamais. Par exemple, si l'on sait que
toutes les valeurs de champs additionn&eacute;es sont telles qu'aucun d&eacute;passement
ne peut avoir lieu, une addition partitionn&eacute;e peut &ecirc;tre impl&eacute;ment&eacute;e &agrave;
l'aide d'une instruction ordinaire. Cette contrainte pos&eacute;e, une addition
ordinaire pourrait en fin de compte appara&icirc;tre polymorphique,
et &ecirc;tre utilisable avec n'importe quelle taille de champ sans programme de
correction. Ce qui nous am&egrave;ne ainsi &agrave; la question suivante&nbsp;: Comment s'assurer
que les valeurs des champs ne vont pas provoquer d'&eacute;v&eacute;nements dus &agrave; la
retenue&nbsp;?
</p><p>
Une mani&egrave;re de faire cela consiste &agrave; impl&eacute;menter des instructions partitionn&eacute;es
capables de restreindre la port&eacute;e des valeurs des champs. Les instructions
vectoris&eacute;es de maximum et de minimum du MAX de Digital peuvent &ecirc;tre
assimil&eacute;es &agrave; une prise en charge mat&eacute;rielle de l'&eacute;cr&ecirc;tage des valeurs des
champs pour &eacute;viter les interactions de la retenue entre les champs.
</p><p>
En revanche, si l'on ne dispose pas d'instructions partitionn&eacute;es &agrave; m&ecirc;me
de restreindre efficacement la port&eacute;e des valeurs des champs, existe-t-il
une condition qui puisse &ecirc;tre impos&eacute;e &agrave; un co&ucirc;t raisonnable et qui soit
suffisante pour garantir le fait que les effets de la retenue n'iront pas
perturber les champs adjacents&nbsp;? La r&eacute;ponse se trouve dans l'analyse des
propri&eacute;t&eacute;s arithm&eacute;tiques. Additionner deux nombres de <span class="emphasis"><em>k</em></span>
bits renvoie un r&eacute;sultat large d'au plus <span class="emphasis"><em>k</em></span>+1 bits.
Ainsi, un champ de <span class="emphasis"><em>k</em></span>+1 bits peut recevoir en toute
s&eacute;curit&eacute; le r&eacute;sultat d'une telle op&eacute;ration, m&ecirc;me s'il est produit par
une instruction ordinaire.
</p><p>
Supposons donc que les champs de 8 bits de nos pr&eacute;c&eacute;dents exemples
soient d&eacute;sormais des champs de 7 bits avec &laquo;&nbsp;<span class="quote">espaces de s&eacute;paration
pour retenue</span>&nbsp;&raquo; d'un bit chacun&nbsp;:
</p><p>

<pre class="programlisting">
              PE3          PE2          PE1          PE0
      +----+-------+----+-------+----+-------+----+-------+
Reg0  | D' | D 6:0 | C' | C 6:0 | B' | B 6:0 | A' | A 6:0 |
      +----+-------+----+-------+----+-------+----+-------+
</pre>

</p><p>
Mettre en place un vecteur d'additions 7 bits se fait de la mani&egrave;re
suivante&nbsp;: On part du principe qu'avant l'ex&eacute;cution de toute instruction
partitionn&eacute;e, les bits des s&eacute;parateurs de retenue
(<code class="literal">A'</code>, <code class="literal">B'</code>,
<code class="literal">C'</code>, et <code class="literal">D'</code>)
sont nuls. En effectuant simplement une addition ordinaire, tous les
champs re&ccedil;oivent une valeur correcte sur 7 bits. En revanche, certains
bits de s&eacute;paration peuvent passer &agrave; 1. On peut rem&eacute;dier &agrave; cela en
appliquant une seule op&eacute;ration suppl&eacute;mentaire conventionnelle et masquant
les bits de s&eacute;paration. Notre vecteur d'additions enti&egrave;res sur 7 bits,
<code class="literal">x</code>+<code class="literal">y</code>, devient
ainsi&nbsp;:
</p><p>

<pre class="programlisting">
((x + y) &amp; 0x7f7f7f7f)
</pre>

</p><p>
Ceci n&eacute;cessite seulement deux op&eacute;rations pour effectuer quatre additions.
Le gain en vitesse est &eacute;vident.
</p><p>
Les lecteurs avertis auront remarqu&eacute; que forcer les bits de s&eacute;paration
&agrave; z&eacute;ro ne convient pas pour les soustractions. La solution est toutefois
remarquablement simple&nbsp;: Pour calculer <code class="literal">x</code>-<code class="literal">y</code>,
on garantira simplement la condition initiale, qui veut que tous les bits
de s&eacute;paration de <code class="literal">x</code> valent 1 et que tous ceux
de <code class="literal">y</code> soient nuls. Dans le pire des cas, nous
obtiendrons&nbsp;:
</p><p>

<pre class="programlisting">
(((x | 0x80808080) - y) &amp; 0x7f7f7f7f)
</pre>

</p><p>
On peut toutefois souvent se passer du &laquo;&nbsp;<span class="quote">OU</span>&nbsp;&raquo; logique en s'assurant
que l'op&eacute;ration qui g&eacute;n&egrave;re la valeur de <code class="literal">x</code>
utilise <code class="literal">| 0x80808080</code> plut&ocirc;t que
<code class="literal">&amp; 0x7f7f7f7f</code> en derni&egrave;re &eacute;tape.
</p><p>
Laquelle de ces m&eacute;thodes doit-on appliquer aux op&eacute;rations partitionn&eacute;es du
SWAR&nbsp;? La r&eacute;ponse est simple&nbsp;: &laquo;&nbsp;<span class="quote">celle qui offre le gain en rapidit&eacute; le
plus important</span>&nbsp;&raquo;. Ce qui est int&eacute;ressant, c'est que la m&eacute;thode id&eacute;ale peut
&ecirc;tre diff&eacute;rente selon chaque taille de champ, et ce &agrave; l'int&eacute;rieur d'un
m&ecirc;me programme, s'ex&eacute;cutant sur une m&ecirc;me machine.
</p></div></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11349"></a>4.2.3.&nbsp;Op&eacute;rations de communication et de conversion de type</h4></div></div></div><p>
Bien que certains calculs en parall&egrave;le, incluant les op&eacute;rations sur les
pixels d'une image, ont pour propri&eacute;t&eacute; le fait que la <span class="emphasis"><em>n</em></span>i&egrave;me
valeur d'un vecteur soit une fonction des valeurs se trouvant &agrave; la <span class="emphasis"><em>n</em></span>i&egrave;me
position des vecteurs des op&eacute;randes, ce n'est g&eacute;n&eacute;ralement pas le cas. Par exemple,
m&ecirc;me les op&eacute;rations sur les pixels telles que l'adoucissement ou le flou r&eacute;clament
en op&eacute;rande les valeurs des pixels adjacents, et les transformations comme la transform&eacute;e
de Fourrier (FFT) n&eacute;cessitent des sch&eacute;mas de communications plus complexes (et moins
localis&eacute;s).
</p><p>
Il est relativement facile de mettre efficacement en place un syst&egrave;me de communication
&agrave; une dimension entre les valeurs du voisinage imm&eacute;diat pour effectuer du SWAR, en utilisant
des op&eacute;rations de d&eacute;calage non partitionn&eacute;es. Par exemple, pour d&eacute;placer une valeur depuis
<code class="literal">PE</code> vers <code class="literal">PE</code>(<span class="emphasis"><em>n</em></span>+1),
un simple d&eacute;calage logique suffit. Si les champs sont larges de 8 bits, on utilisera&nbsp;:
</p><p>

<pre class="programlisting">
(x &lt;&lt; 8)
</pre>

</p><p>
Pourtant, ce n'est pas toujours aussi simple. Par exemple, pour d&eacute;placer une valeur
depuis <code class="literal">PE</code><span class="emphasis"><em>n</em></span> vers
<code class="literal">PE</code>(<span class="emphasis"><em>n</em></span>-1), un simple d&eacute;calage vers la
droite devrait suffire&hellip; mais le langage C ne pr&eacute;cise pas si le d&eacute;calage vers la
droite conserve le bit de signe, et certaines machines ne proposent, vers la droite,
qu'un d&eacute;calage sign&eacute;. Aussi, d'une mani&egrave;re g&eacute;n&eacute;rale, devons-nous mettre explicitement &agrave;
z&eacute;ro les bits de signe pouvant &ecirc;tre r&eacute;pliqu&eacute;s.
</p><p>

<pre class="programlisting">
((x &gt;&gt; 8) &amp; 0x00ffffff)
</pre>

</p><p>

L'ajout de connexions &laquo;&nbsp;<span class="quote">&agrave; enroulement</span>&nbsp;&raquo; 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">wrap-around</em></span></span>&nbsp;&raquo;) &agrave; l'aide de 
d&eacute;calages non partitionn&eacute;s <sup>[<a href="#ftn.N11385" name="N11385">25</a>]</sup> est aussi raisonnablement efficace. Par exemple, pour 
d&eacute;placer une valeur depuis <code class="literal">PE</code><span class="emphasis"><em>i</em></span> 
vers <code class="literal">PE</code>(<span class="emphasis"><em>i</em></span>+1) avec 
enroulement&nbsp;:

</p><pre class="programlisting">
((x &lt;&lt; 8) | ((x &gt;&gt; 24) &amp; 0x000000ff))
</pre><p>

Les probl&egrave;mes s&eacute;rieux apparaissent lorsque des sch&eacute;mas de communications 
plus g&eacute;n&eacute;raux doivent &ecirc;tre mis en &#339;uvre. Seul le jeu 
d'instructions du MAX de HP permet le r&eacute;arrangement arbitraire des 
champs en une seule instruction, nomm&eacute;e <code class="literal">Permute</code>. 
Cette instruction <code class="literal">Permute</code> porte vraiment mal son 
nom&nbsp;: Non seulement elle effectue une permutation arbitraire des 
champs, mais elle autorise &eacute;galement les r&eacute;p&eacute;titions. En bref, elle met 
en place une op&eacute;ration arbitraire de <code class="literal">x[y]</code>.

</p><p>

Il reste malheureusement tr&egrave;s difficile d'impl&eacute;menter 
<code class="literal">x[y]</code> sans le concours d'une telle instruction. La 
s&eacute;quence de code est g&eacute;n&eacute;ralement &agrave; la fois longue et inefficace, parce 
qu'en fait, il s'agit d'un programme s&eacute;quentiel. Ceci est tr&egrave;s d&eacute;cevant. 
La vitesse relativement &eacute;lev&eacute;e des op&eacute;rations de type 
<code class="literal">x[y]</code> sur les les supercalculateurs SIMD MasPar 
MP1/MP2 et Thinking Machines CM1/CM2/CM200 &eacute;tait une des cl&eacute;s majeures 
de leur succ&egrave;s. Toutefois, un <code class="literal">x[y]</code> reste plus lent 
qu'une communication de proximit&eacute;, m&ecirc;me sur ces supercalculateurs. 
Beaucoup d'algorithmes ont donc &eacute;t&eacute; con&ccedil;us pour r&eacute;duire ces besoins en 
op&eacute;rations de type <code class="literal">x[y]</code>. Pour simplifier, disons que 
sans soutien mat&eacute;riel, le meilleur est encore tr&egrave;s certainement de 
d&eacute;velopper des algorithmes SWAR en consid&eacute;rant <code class="literal">x[y]</code> 
comme &eacute;tant interdit, ou au moins tr&egrave;s co&ucirc;teux.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N113BD"></a>4.2.4.&nbsp;Op&eacute;rations r&eacute;currentes (r&eacute;ductions, balayages, et c&aelig;tera)</h4></div></div></div><p>
Une r&eacute;currence est un calcul dans lequel il existe une relation
s&eacute;quentielle apparente entre les valeurs &agrave; traiter. Si toutefois des
op&eacute;rations associatives sont impliqu&eacute;es dans ces r&eacute;currences, il peut
&ecirc;tre possible de recoder ces calculs en utilisant un algorithme
parall&egrave;le &agrave; structure organis&eacute;e en arbre.
</p><p>
Le type de r&eacute;currence parall&eacute;lisable le plus courant est probablement
la classe connue sous le nom de r&eacute;duction associative. Par exemple,
pour calculer la somme des valeurs d'un vecteur, on &eacute;crit du code C
purement s&eacute;quentiel, comme&nbsp;:
</p><p>

<pre class="programlisting">
t = 0;
for (i=0; i&lt;MAX; ++i) t += x[i];
</pre>

</p><p>

Cependant, l'ordre des additions est rarement important. Les op&eacute;rations
en virgule flottante et les saturations peuvent rendre diff&eacute;rents r&eacute;sultats
si l'ordre des additions est modifi&eacute;, mais les additions ordinaires sur les
entiers (et qui reviennent au d&eacute;but apr&egrave;s avoir atteint la valeur maximum)
renverront exactement les m&ecirc;mes r&eacute;sultats, ind&eacute;pendemment de l'ordre dans
lequel elles sont effectu&eacute;es. Nous pouvons ainsi r&eacute;&eacute;crire cette s&eacute;quence
sous la forme d'une somme parall&egrave;le structur&eacute;e en arbre, dans laquelle nous
additionnons d'abord des paires de valeurs, puis des paires de ces sous-totaux,
et ainsi de suite jusqu'&agrave; l'unique somme finale. Pour un vecteur de quatre
valeurs 8 bits, seulement deux additions sont n&eacute;cessaires. La premi&egrave;re effectue
deux additions de 8 bits et retourne en r&eacute;sultat deux champs de 16 bits,
contenant chacun un r&eacute;sultat sur 9 bits&nbsp;:

</p><p>

<pre class="programlisting">
t = ((x &amp; 0x00ff00ff) + ((x &gt;&gt; 8) &amp; 0x00ff00ff));
</pre>

</p><p>
La deuxi&egrave;me fait la somme de ces deux valeurs de 9 bits dans
un champs de 16 bits, et renvoie un r&eacute;sultat sur 10 bits&nbsp;:
</p><p>

<pre class="programlisting">
((t + (t &gt;&gt; 16)) &amp; 0x000003ff)
</pre>

</p><p>

En r&eacute;alit&eacute;, la seconde op&eacute;ration effectue l'addition de deux champs de 
16 bits&hellip; mais les 16 bits de poids fort n'ont pas de 
signification. C'est pourquoi le r&eacute;sultat est masqu&eacute; en une valeur de 10 
bits.

</p><p>

Les balayages (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">scans</em></span></span>&nbsp;&raquo;), 
aussi connus sous le nom d'op&eacute;rations &laquo;&nbsp;<span class="quote">&agrave; pr&eacute;fixe parall&egrave;le</span>&nbsp;&raquo; 
sont un peu plus difficile &agrave; mettre en &#339;uvre efficacement. Ceci 
est d&ucirc; au fait que contrairement aux r&eacute;ductions, les balayages peuvent 
produire des r&eacute;sultats partitionn&eacute;es.

</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N113E5"></a>4.3.&nbsp;SWAR MMX sous Linux</h3></div></div></div><p>

Pour Linux, nous nous soucierons principalement des processeurs IA32. La 
bonne nouvelle, c'est qu'AMD, Cyrix et Intel impl&eacute;mentent tous le m&ecirc;me 
jeu d'instructions MMX. En revanche, les performances de ces diff&eacute;rents 
MMX sont variables. Le K6, par exemple, n'est dot&eacute; que d'un seul 
<span class="foreignphrase"><em class="foreignphrase">pipeline</em></span> l&agrave; o&ugrave; le Pentium MMX en a deux. 
La seule nouvelle vraiment mauvaise, c'est qu'Intel diffuse toujours ces 
stupides films publicitaires sur le MMX&hellip; ;-)

</p><p>

Il existe trois approches s&eacute;rieuses du SWAR par le MMX&nbsp;:

</p><div class="orderedlist"><ol type="1"><li><p>

L'utilisation des fonctions d'une biblioth&egrave;que d&eacute;di&eacute;e au MMX. Intel, en 
particulier, a d&eacute;velopp&eacute; plusieurs &laquo;&nbsp;<span class="quote">biblioth&egrave;ques de 
performances</span>&nbsp;&raquo;, offrant toute une gamme de fonctions optimis&eacute;es &agrave; 
la main et destin&eacute;es aux t&acirc;ches multim&eacute;dia courantes. Avec un petit 
effort, bon nombre d'algorithmes non-multim&eacute;dia peuvent &ecirc;tre 
retravaill&eacute;s pour permettre &agrave; quelques unes des zones de calcul intensif 
de s'appuyer sur une ou plusieurs de ces biblioth&egrave;ques. Ces 
biblioth&egrave;ques ne sont pas disponibles sous Linux, mais pourraient &ecirc;tre 
adapt&eacute;es pour le devenir.

</p></li><li><p>

L'utilisation directe des instructions MMX. C'est assez compliqu&eacute;, pour
deux raisons&nbsp;: D'abord, le MMX peut ne pas &ecirc;tre disponible sur le processeur
concern&eacute;, ce qui oblige &agrave; fournir une alternative. Ensuite, l'assembleur IA32
utilis&eacute; en g&eacute;n&eacute;ral sous Linux ne reconna&icirc;t actuellement pas les instructions
MMX.

</p></li><li><p>

L'utilisation d'un langage de haut niveau ou d'un module du 
compilateur qui puisse directement g&eacute;n&eacute;rer des instructions MMX 
appropri&eacute;es. Quelques outils de ce type sont actuellement en cours de 
d&eacute;veloppement, mais aucun n'est encore pleinement fonctionnel sous 
Linux. Par exemple, &agrave; l'Universit&eacute; de Purdue (<a href="http://dynamo.ecn.purdue.edu/~hankd/SWAR/" target="_top">http://dynamo.ecn.purdue.edu/~hankd/SWAR/</a>), nous d&eacute;veloppons 
actuellement un compilateur qui admettra des fonctions &eacute;crites dans un 
&laquo;&nbsp;<span class="quote">dialecte</span>&nbsp;&raquo; explicite parall&egrave;le au langage C et qui g&eacute;n&eacute;rera 
des modules SWAR accessibles comme des fonctions C ordinaires, et qui 
feront usage de tous les supports SWAR disponibles, y compris le MMX. 
Les premiers prototypes de compilateurs &agrave; modules ont &eacute;t&eacute; g&eacute;n&eacute;r&eacute;s &agrave; 
Fall, en 1996. Amener cette technologie vers un &eacute;tat r&eacute;ellement 
utilisable prend cependant beaucoup plus de temps que pr&eacute;vu 
initialement.

</p></li></ol></div><p>
En r&eacute;sum&eacute;, le SWAR par MMX est toujours d'un usage malais&eacute;. Toutefois, avec
quelques efforts suppl&eacute;mentaires, la seconde approche d&eacute;crite ci-dessus peut
&ecirc;tre utilis&eacute;e d&egrave;s &agrave; pr&eacute;sent. En voici les bases&nbsp;:
</p><p>

<div class="orderedlist"><ol type="1"><li><p>
Vous ne pourrez pas utiliser le MMX si votre processeur ne le prend pas
en charge. Le code GCC suivant vous permettra de savoir si votre processeur
est &eacute;quip&eacute; de l'extension MMX. Si c'est le cas, la valeur renvoy&eacute;e sera
diff&eacute;rente de z&eacute;ro, sinon nulle.

<pre class="programlisting">
inline extern
int mmx_init(void)
{
	int mmx_disponible;

	__asm__ __volatile__ (
		/* R&eacute;cup&egrave;re la version du CPU */
		"movl $1, %%eax\n\t"
		"cpuid\n\t"
		"andl $0x800000, %%edx\n\t"
		"movl %%edx, %0"
		: "=q" (mmx_disponible)
		: /* pas d'entr&eacute;e */
	);
	return mmx_disponible;
}
</pre>

</p></li><li><p>
Un registre MMX contient essentiellement ce que GCC appellerait un
<code class="literal">unsigned long long</code>. Ainsi, ce sont des variables
de ce type et r&eacute;sidant en m&eacute;moire qui vont former le m&eacute;canisme de communication
entre les modules MMX et le programme C qui les appelle. Vous pouvez aussi d&eacute;clarer
vos donn&eacute;es MMX comme &eacute;tant des structures de donn&eacute;es align&eacute;es sur des adresses
multiples de 64 bits (il convient de garantir l'alignement sur 64 bits en d&eacute;clarant
votre type de donn&eacute;es comme &eacute;tant membre d'une <code class="literal">union</code>
comportant un champ <code class="literal">unsigned long long</code>).
</p></li><li><p>
Si le MMX est disponible, vous pouvez &eacute;crire votre code MMX en utilisant
la directive assemble <code class="literal">.byte</code> pour coder chaque
instruction. C'est un travail p&eacute;nible s'il est abattu &agrave; la main, mais pour un
compilateur, les g&eacute;n&eacute;rer n'est pas tr&egrave;s difficile. Par exemple, l'instruction MMX
<code class="literal">PADDB MM0,MM1</code> pourrait &ecirc;tre encod&eacute;e par la ligne
d'assembleur in-line GCC suivante&nbsp;:


<pre class="programlisting">
__asm__ __volatile__ (".byte 0x0f, 0xfc, 0xc1\n\t");
</pre>


Souvenez-vous que le MMX utilise une partie du mat&eacute;riel destin&eacute; aux op&eacute;rations
en virgule flottante, et donc que le code ordinaire m&eacute;lang&eacute; au MMX ne doit pas
invoquer ces derni&egrave;res. La pile en virgule flottante doit &eacute;galement &ecirc;tre vide
avant l'ex&eacute;cution de tout code MMX. Cette pile est normalement vide &agrave; l'entr&eacute;e
d'une fonction C ne faisant pas usage de la virgule flottante.


</p></li><li><p>
Cl&ocirc;turez votre code MMX par l'ex&eacute;cution de l'instruction <code class="literal">EMMS</code>,
qui peut &ecirc;tre encod&eacute;e par&nbsp;:


<pre class="programlisting">
__asm__ __volatile__ (".byte 0x0f, 0x77\n\t");
</pre>

</p></li></ol></div>

</p><p>
Tout ce qui pr&eacute;c&egrave;de para&icirc;t r&eacute;barbatif, et l'est. Ceci dit, le MMX
est encore assez jeune&hellip; de futures versions de ce document proposeront
de meilleures techniques pour programmer le SWAR MMX.
</p></div></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N1143E"></a>5.&nbsp;Processeurs auxiliaires des machines Linux</h2></div></div></div><p>
M&ecirc;me si cette approche n'est plus &agrave; la mode depuis quelques temps,
il reste virtuellement impossible aux autres m&eacute;thodes de maintenir &agrave;
la fois prix bas et performances &eacute;lev&eacute;es, chose habituellement rendue
possible par l'utilisation d'un syst&egrave;me Linux, lorsqu'il s'agit
d'h&eacute;berger dans la machine une unit&eacute; de calcul d&eacute;di&eacute;e. Le probl&egrave;me est
que la prise en charge logicielle de ces outils est tr&egrave;s limit&eacute;e.
Nous serons plus ou moins livr&eacute;s &agrave; nous m&ecirc;mes.
</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N11443"></a>5.1.&nbsp;Un PC Linux est une bonne station d'accueil</h3></div></div></div><p>
En g&eacute;n&eacute;ral, les processeurs secondaires tendent &agrave; se sp&eacute;cialiser dans
l'ex&eacute;cution de fonctions bien sp&eacute;cifiques.
</p><p>
Avant de se sentir d&eacute;courag&eacute;s parce que livr&eacute;s &agrave; nous-m&ecirc;mes, il est
utile de comprendre que, bien qu'il puisse &ecirc;tre difficile de le pr&eacute;parer
&agrave; recevoir un syst&egrave;me particulier, un PC sous Linux reste l'une des rares
plate-formes qui se pr&ecirc;tent convenablement &agrave; cet usage.
</p><p>
Un PC forme un bon syst&egrave;me d'accueil pour deux raisons&nbsp;: La premi&egrave;re est sa capacit&eacute;
&agrave; &eacute;voluer ais&eacute;ment et de fa&ccedil;on peu on&eacute;reuse. Les ressources telles que
la m&eacute;moire, les disques, le r&eacute;seau et autres peuvent &ecirc;tre ajout&eacute;es
vraiment tr&egrave;s facilement &agrave; un PC. La seconde est sa facilit&eacute; d'interfa&ccedil;age.
Non seulement les prototypes de cartes ISA et PCI sont largement r&eacute;pandus
et disponibles, mais le port parall&egrave;le fournit &eacute;galement des performances
raisonnables dans une interface discr&egrave;te. L'adressage s&eacute;par&eacute; des entr&eacute;es&nbsp;/&nbsp;sorties
de l'architecture IA32 facilite &eacute;galement l'interfa&ccedil;age en offrant
une protection de ces adresses au niveau du port individuel.
</p><p>
Linux est &eacute;galement un bon syst&egrave;me d'exploitation pour l'h&eacute;bergement de
syst&egrave;mes d&eacute;di&eacute;s. La libre et compl&egrave;te disponibilit&eacute; du code source, et les
nombreux manuels de programmation avanc&eacute;e constituent &eacute;videment une aide
tr&egrave;s pr&eacute;cieuse. Mais Linux apporte &eacute;galement une gestion des t&acirc;ches pratiquement
temps-r&eacute;el. Il existe m&ecirc;me une version fonctionnant en vrai temps-r&eacute;el.
Mais une chose peut-&ecirc;tre plus importante encore est le fait que, m&ecirc;me
dans un environnement Unix complet, Linux sait prendre en charge des outils
de d&eacute;veloppement &eacute;crits pour fonctionner sous MS-DOS ou Windows. Les programmes
DOS peuvent &ecirc;tre ex&eacute;cut&eacute;s dans un processus Unix en utilisant
<code class="literal">dosemu</code>, qui dresse une machine virtuelle prot&eacute;g&eacute;e
qui peut litt&eacute;ralement ex&eacute;cuter du code MS-DOS. La prise en charge des programmes
Windows 3.xx sous Linux est encore plus directe&nbsp;: certains 
logiciels libres comme
<a href="http://www.winehq.org" target="_top"><code class="literal">wine</code></a>
simulent Windows 3.11<sup>[<a href="#ftn.N11458" name="N11458">26</a>]</sup>

suffisamment bien pour permettre &agrave; la plupart des programmes
Windows d'&ecirc;tre ex&eacute;cut&eacute;s correctement sur une machine Unix &eacute;quip&eacute;e de 
X-Window.

</p><p>
Les deux sections suivantes donnent des exemples de syst&egrave;mes parall&egrave;les
auxiliaires que j'aimerais voir &ecirc;tre exploit&eacute;s sous Linux&nbsp;:
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N1145E"></a>5.2.&nbsp;Avez-vous essay&eacute; le DSP&nbsp;?</h3></div></div></div><p>
Un march&eacute; prosp&egrave;re s'est d&eacute;velopp&eacute; autour des puces DSP (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Digital
Signal Processing</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">Traitement Num&eacute;rique du Signal</span>&nbsp;&raquo;) &agrave; hautes performances. Bien
qu'elles soient en g&eacute;n&eacute;ral con&ccedil;ues pour &ecirc;tre embarqu&eacute;es dans des syst&egrave;mes d&eacute;di&eacute;s &agrave;
des usages bien sp&eacute;cifiques, elles peuvent aussi &ecirc;tre utilis&eacute;es comme de tr&egrave;s bons
ordinateurs parall&egrave;les annexes. Voici pourquoi&nbsp;:
</p><p>

<div class="itemizedlist"><ul type="disc"><li><p>
Plusieurs d'entre elles, comme le TMS320 de
<a href="http://www.ti.com" target="_top">Texas Instruments</a> et la famille
SHARC d'<a href="http://www.analog.com" target="_top">Analog Devices</a> sont
con&ccedil;ues pour permettre l'assemblage de machines parall&egrave;les
en n'utilisant que tr&egrave;s peu, voire aucun circuit logique interm&eacute;diaire.
</p></li><li><p>
Ces puces sont bon march&eacute;, sp&eacute;cialement au niveau du co&ucirc;t par MIP ou par
MFLOP. Prix des circuits de gestion logique de base compris, on peut trouver
un processeur DSP pour le dixi&egrave;me du prix du processeur d'un PC, &agrave; performances
comparables.

</p></li><li><p>
Elle ne n&eacute;cessitent que peu de puissance et ne dissipent pas beaucoup de
chaleur. Cela signifie qu'il est possible d'alimenter toute une poign&eacute;e de processeurs
de ce type avec une simple alimentation PC conventionnelle, et de les enfermer
dans le bo&icirc;tier d'un PC sans transformer celui-ci en haut-fourneau.

</p></li><li><p>
La plupart des jeux d'instructions des DSP contiennent des choses assez
exotiques et que les langages de haut niveau (c'est-&agrave;-dire&nbsp;: comme le
C) ne sont pas &agrave; m&ecirc;me d'utiliser correctement. Par exemple, l'&laquo;&nbsp;<span class="quote">adressage de
bit invers&eacute;</span>&nbsp;&raquo;. En utilisant des syst&egrave;mes parall&egrave;les inclus dans une machine,
il est possible de compiler et d'ex&eacute;cuter directement la plupart du code sur
cette machine, tout en confiant l'ex&eacute;cution des quelques algorithmes consommant
la plupart du temps syst&egrave;me aux DSP, par l'entremise d'un code particuli&egrave;rement soign&eacute;
et optimis&eacute; &agrave; la main.

</p></li><li><p>
Ces DSP ne sont pas vraiment con&ccedil;us pour faire fonctionner un syst&egrave;me d'exploitation
UNIX ou assimil&eacute;, et ne sont g&eacute;n&eacute;ralement pas non plus adapt&eacute;s &agrave; une utilisation
autonome comme processeur g&eacute;n&eacute;ral d'un ordinateur. Le syst&egrave;me de gestion de la m&eacute;moire,
par exemple, est insuffisant pour beaucoup d'entre eux. En d'autres mots, ils sont bien
plus efficaces lorsqu'ils sont int&eacute;gr&eacute;s au sein d'une machine h&ocirc;te plus polyvalente
&hellip; telle qu'un PC sous Linux.
</p></li></ul></div>

</p><p>
Bien que de nombreux modems et cartes son contiennent des processeurs DSP
accessibles par des pilotes Linux, les grands profits arrivent avec l'utilisation de
syst&egrave;mes parall&egrave;les int&eacute;gr&eacute;s comprenant au moins quatre processeurs DSP.
</p><p>
M&ecirc;me si la s&eacute;rie TMS320 de Texas Instruments, sur <a href="http://dspvillage.ti.com" target="_top">http://dspvillage.ti.com</a>, est populaire depuis un bon moment, les syst&egrave;mes de ce type &eacute;tant
disponibles sont encore peu nombreux. Il existe &agrave; la fois une version
uniquement enti&egrave;re et une version virgule flottante du TMS320. Les
anciens mod&egrave;les utilisaient un format de virgule flottante en simple pr&eacute;cision assez
inhabituel, mais les nouveaux g&egrave;rent &agrave; pr&eacute;sent les formats IEEE. Les anciens TMS320C4x
(ou simplement &laquo;&nbsp;<span class="quote">C4x</span>&nbsp;&raquo;), accomplissaient jusqu'&agrave; 80 MFLOP en utilisant le format de
nombre en virgule flottante simple pr&eacute;cision sp&eacute;cifique &agrave; TI. En comparaison, un seul
&laquo;&nbsp;<span class="quote">C67x</span>&nbsp;&raquo; apportera jusqu'&agrave; 1 GFLOP en simple pr&eacute;cision et 420 MFLOP en double pr&eacute;cision
au format IEEE, en utilisant une architecture &agrave; base de VLIW nomm&eacute;e VelociTI. Non seulement
il est ais&eacute; de configurer un groupe de circuits de ce type pour en faire un multiprocesseur,
mais un de ces circuits, le multiprocesseur &laquo;&nbsp;<span class="quote">'C8x</span>&nbsp;&raquo;, formera &agrave; lui seul un processeur
principal &agrave; technologie RISC accusant 100 MFLOP au format IEEE, et dot&eacute; soit de deux, soit
de quatre DSP esclaves int&eacute;gr&eacute;s.
</p><p>
L'autre famille de processeurs DSP qui ne soit pas simplement utilis&eacute;e
ces temps-ci par quelques rares syst&egrave;mes en parall&egrave;le est SHARC (ou
&laquo;&nbsp;<span class="quote">ADSP-2106x</span>&nbsp;&raquo;), d'<a href="http://www.analog.com/" target="_top">Analog Devices</a>.
Ces circuits peuvent &ecirc;tre configur&eacute;s en un multiprocesseur &agrave; m&eacute;moire partag&eacute;e
form&eacute; par 6 processeurs sans n&eacute;cessiter de logique externe pour les associer, et
peuvent aussi former de plus grands syst&egrave;mes gr&acirc;ce &agrave; des circuits de liaison de
4 bits. Les syst&egrave;mes au-del&agrave; de celui-ci sont essentiellement destin&eacute;s aux applications
militaires, et reviennent assez cher. Toutefois, 
<a href="http://www.iced.com/" target="_top">Integrated Computing Engines, Inc.</a> produit un petit jeu de deux cartes PCI assez int&eacute;ressant, nomm&eacute; GreenICE. Cette
unit&eacute; contient un r&eacute;seau de 16 processeurs SHARC, et affiche une vitesse de pointe
d'1.9 GFLOP au format IEEE simple pr&eacute;cision. GreenICE co&ucirc;te moins de 5000 dollars.
</p><p>
&Agrave; mon avis, les circuits DSP parall&egrave;les int&eacute;gr&eacute;s m&eacute;riteraient une plus grande attention
de la part du petit monde du calcul en parall&egrave;le sous Linux&hellip;
</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N114A8"></a>5.3.&nbsp;Calcul &agrave; l'aide des FPGA et circuits logiques reconfigurables</h3></div></div></div><p>

Si l'objectif final du traitement en parall&egrave;le reste l'atteinte des plus 
hautes vitesses possibles, et bien pourquoi ne pas fabriquer du mat&eacute;riel 
sur mesure&nbsp;? Nous connaissons tous la r&eacute;ponse&nbsp;: Trop cher, 
trop long &agrave; d&eacute;velopper, le mat&eacute;riel ainsi con&ccedil;u devient inutile lorsque 
l'algorithme change m&ecirc;me de fa&ccedil;on minime, et c&aelig;tera. Cependant, les 
r&eacute;centes avanc&eacute;es faites dans le domaine des FPGA 
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Field Programmable Gate 
Array</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">R&eacute;seaux Logiques Programmables 
&agrave; effet de Champ</span>&nbsp;&raquo;) ont r&eacute;duit &agrave; n&eacute;ant la plupart de ces 
objections. Aujourd'hui, la densit&eacute; des portes logiques est suffisamment 
&eacute;lev&eacute;e pour qu'un processeur entier puisse &ecirc;tre int&eacute;gr&eacute; dans un seul 
FPGA, et le temps de reconfiguration (ou de reprogrammation) d'un de ces 
FPGA a &eacute;galement chut&eacute; &agrave; un niveau o&ugrave; l'on peut raisonnablement 
reconfigurer le circuit m&ecirc;me entre deux phases d'un m&ecirc;me algorithme.

</p><p>
Il ne faut pas avoir froid aux yeux pour utiliser ces techniques&nbsp;: Il
faudra travailler avec des langages de description mat&eacute;rielle tels que
le VHDL pour configurer les FPGA, tout comme &eacute;crire du code de bas niveau
pour programmer les interfaces avec le syst&egrave;me d'accueil Linux. En revanche,
le co&ucirc;t des FPGA est assez bas, sp&eacute;cialement pour les algorithmes op&eacute;rant
sur des donn&eacute;es enti&egrave;res et de pr&eacute;cision normale (ce qui ne repr&eacute;sente en fait
qu'un surensemble restreint de tout ce &agrave; quoi le SWAR s'applique bien), ces FPGA
peuvent effectuer des op&eacute;rations complexes &agrave; peu pr&egrave;s aussi vite qu'on peut les
leur transmettre. Par exemple, de simples syst&egrave;mes &agrave; base de FPGA ont accus&eacute; des
performances sup&eacute;rieures &agrave; celles des supercalculateurs lors de recherches sur
des bases de donn&eacute;es g&eacute;n&eacute;tiques.
</p><p>
Il existe plusieurs compagnies produisant du mat&eacute;riel appropri&eacute; &agrave; base de FPGA,
mais les deux suivantes en sont un bon exemple&nbsp;:
</p><p>

Virtual Computer Company propose toute une gamme de produits utilisant 
des FPGA Xilinx &agrave; base de SRAM et reconfigurables dynamiquement. Leur 
&laquo;&nbsp;<span class="quote">Virtual ISA Proto Board</span>&nbsp;&raquo; 8/16 bits co&ucirc;te moins de 2000 
dollars.

</p><p>
L'ARC-PCI d'Altera (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Altera Reconfigurable Computer</em></span></span>&nbsp;&raquo;, sur bus PCI), sur <a href="http://www.altera.com/html/new/pressrel/pr_arc-pci.html" target="_top">http://www.altera.com/html/new/pressrel/pr_arc-pci.html</a>, est un type de carte similaire, mais utilisant les FPGA d'Altera et un
bus PCI comme interface plut&ocirc;t qu'un bus ISA.
</p><p>

Bon nombre de ces outils de conception, langages de description 
mat&eacute;rielle, compilateurs, routeurs, et c&aelig;tera, sont livr&eacute;s sous forme 
d'un code objet qui ne fonctionne que sous DOS ou Windows. Nous 
pourrions simplement conserver une partition DOS/Windows sur notre PC 
d'accueil et red&eacute;marrer lorsque nous en avons besoin. Toutefois, la 
plupart de ces outils peuvent probablement fonctionner sous Linux en 
utilisant <code class="literal">dosemu</code> ou des &eacute;mulateurs Windows tels que 
<code class="literal">wine</code>.

</p></div></div><div class="sect1" lang="fr"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="N114D1"></a>6.&nbsp;D'int&eacute;r&ecirc;t g&eacute;n&eacute;ral</h2></div></div></div><p>
Les sujets couverts dans cette section s'appliquent aux quatre
mod&egrave;les de traitement en parall&egrave;le sous Linux.
</p><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N114D6"></a>6.1.&nbsp;Compilateurs et langages de programmation</h3></div></div></div><p>
Je suis principalement un &laquo;&nbsp;<span class="quote">chercheur en compilateurs</span>&nbsp;&raquo;. Je devrais
donc &ecirc;tre capable de dire s'il y a beaucoup de compilateurs vraiment
performants, g&eacute;n&eacute;rant du code parall&egrave;le efficace pour les syst&egrave;mes Linux.
Malheureusement, et pour dire la v&eacute;rit&eacute;, il est difficile de battre les performances
obtenues en exprimant votre programme en parall&egrave;le avec des communications
et autres op&eacute;rations parall&egrave;les, le tout dans un code en langage C, compil&eacute;
par GCC.
</p><p>
Les projets de compilateurs ou de langages suivants repr&eacute;sentent une
partie des meilleurs efforts produits pour la g&eacute;n&eacute;ration d'un code
raisonnablement efficace depuis les langages de haut niveau. G&eacute;n&eacute;ralement,
chacun d'eux est raisonnablement efficace pour les t&acirc;ches qu'il vise,
mais aucun ne correspond aux langages et aux compilateurs puissants et
tout-terrain qui vous feront abandonner d&eacute;finitivement l'&eacute;criture de programmes
C compil&eacute;s par GCC&hellip; ce qui est tr&egrave;s bien comme &ccedil;&agrave;. Utilisez ces langages
et compilateurs &agrave; ce pour quoi ils ont &eacute;t&eacute; con&ccedil;us, et vous serez r&eacute;compens&eacute;s
par des dur&eacute;es de d&eacute;veloppement plus courtes, des op&eacute;rations de maintenance et de
d&eacute;bogages r&eacute;duites, et c&aelig;tera.
</p><p>
Il existe un grand nombre de langages et de compilateurs en dehors de ceux list&eacute;s
ici (dans l'ordre alphab&eacute;tique). Une liste de compilateurs librement
disponibles (la plupart n'ayant rien &agrave; voir avec le traitement en parall&egrave;le
sous Linux) est accessible ici&nbsp;: <a href="http://www.idiom.com/free-compilers" target="_top">http://www.idiom.com/free-compilers</a>.
</p><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N114E6"></a>6.1.1.&nbsp;Fortran 66/77/PCF/90/HPF/95</h4></div></div></div><p>
Au moins dans la communaut&eacute; de l'informatique scientifique, le Fortran
sera toujours pr&eacute;sent. Bien s&ucirc;r, le Fortran d'aujourd'hui n'a plus la
m&ecirc;me signification que celle d&eacute;finie par le standard ANSI de 1966. En
quelques mots, le Fortran 66 &eacute;tait tr&egrave;s limit&eacute;. Le Fortran 77 a ajout&eacute;
des nuances dans ses fonctions, la plus int&eacute;ressante &eacute;tant la prise en
charge am&eacute;lior&eacute;e des donn&eacute;es de type caract&egrave;re et la modification de la
s&eacute;mantique des boucles <code class="literal">DO</code>. Le Fortran PCF
(&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Computing Forum</em></span></span>&nbsp;&raquo;) a tent&eacute; d'ajouter diverses fonctions de
gestion de traitement en parall&egrave;le au Fortran 77. Le Fortran 90 est un langage moderne
et pleinement fonctionnel, qui apporte essentiellement des facilit&eacute;s de programmation
orient&eacute;e objet ressemblant au C++ et une syntaxe de tableaux en parall&egrave;le au langage
du Fortran 77. HPF (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">High-Performance Fortran</em></span></span>&nbsp;&raquo;, lui-m&ecirc;me
propos&eacute; en deux versions (HPF-1 et HPF-2), est essentiellement
la version standardis&eacute;e et am&eacute;lior&eacute;e de ce que beaucoup d'entre nous ont connu
sous les noms de CM Fortran, MasPar Fortran, ou Fortran D. Il &eacute;tend le Fortran 90
avec diverses am&eacute;liorations d&eacute;di&eacute;es au traitement en parall&egrave;le, tr&egrave;s centr&eacute;es
sur la sp&eacute;cification d'agencements de donn&eacute;es. Citons enfin le Fortran 95,
version quelque peu am&eacute;lior&eacute;e et raffin&eacute;e du Fortran 90.
</p><p>
Ce qui fonctionne avec le C peut en g&eacute;n&eacute;ral fonctionner aussi avec  <code class="literal">f2c</code>,
<code class="literal">g77</code>, ou les produits Fortran 90/95 commerciaux de NAG.
Ceci est d&ucirc; au fait que tous ces compilateurs peuvent au final produire le
m&ecirc;me code que celui utilis&eacute; en arri&egrave;re-boutique par GCC.
</p><p>
Les parall&eacute;liseurs Fortran commerciaux pouvant g&eacute;n&eacute;rer du code pour SMP
sont disponibles sur <a href="http://www.kai.com" target="_top">http://www.kai.com</a> et <a href="http://www.crescentbaysoftware.com" target="_top">http://www.crescentbaysoftware.com</a>.
Il n'est pas clairement indiqu&eacute; si ces compilateurs sont utilisables pour
des machines SMP Linux, mais ils devraient l'&ecirc;tre &eacute;tant donn&eacute; que les <span class="foreignphrase"><em class="foreignphrase">threads</em></span>
POSIX standards (soit les LinuxThreads) fonctionnent sur des syst&egrave;mes SMP sous Linux.
</p><p>
Le <a href="http://www.pgroup.com/" target="_top">Portlan Group</a> propose des compilateurs Fortran HPF (et C/C++) parall&egrave;les commerciaux
g&eacute;n&eacute;rant du code Linux SMP. Il existe aussi une version ciblant les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>
s'appuyant sur MPI ou PVM. Les produits de <a href="http://www.apri.com" target="_top">http://www.apri.com</a> pourraient aussi &ecirc;tre utiles sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> ou des machines SMP.
</p><p>
Parmi les Fortran parall&eacute;lis&eacute;s disponibles librement et qui pourraient
fonctionner sur des syst&egrave;mes Linux en parall&egrave;le, citons&nbsp;:
</p><div class="itemizedlist"><ul type="disc"><li><p>
ADAPTOR (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Automatic DAta Parallelism TranslaTOR</em></span></span>&nbsp;&raquo;, <a href="http://www.gmd.de/SCAI/lab/adaptor/adaptor_home.html" target="_top">http://www.gmd.de/SCAI/lab/adaptor/adaptor_home.html</a>,
qui peut traduire du Fortran HPF en Fortran 77/90 avec appels MPI
ou PVM, mais qui ne mentionne pas Linux.
</p></li><li><p>
Fx, de l'Universit&eacute; Carnegie Mellon (N.D.T.&nbsp;: Pittsburgh), vise 
certains
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de stations de travail, mais Linux&nbsp;?
</p></li><li><p>
HPFC (un prototype de Compilateur HPF) g&eacute;n&egrave;re du code Fortran 77
avec appels PVM. Est-il utilisable sur un <span class="foreignphrase"><em class="foreignphrase">cluster</em></span> Linux&nbsp;?
</p></li><li><p>
PARADIGM (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">PARAllelizing compiler for DIstributed-memory
General-purpose Multicomputers</em></span></span>&nbsp;&raquo;, <a href="http://www.crhc.uiuc.edu/Paradigm" target="_top">http://www.crhc.uiuc.edu/Paradigm</a>) peut-il
lui aussi &ecirc;tre utilis&eacute; avec Linux&nbsp;?
</p></li><li><p>
Le compilateur Polaris g&eacute;n&egrave;re du code Fortran pour multiprocesseurs &agrave; m&eacute;moire partag&eacute;e,
et pourrait bient&ocirc;t &ecirc;tre recibl&eacute; vers les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux PAPERS.

</p></li><li><p>
PREPARE, <a href="http://www.irisa.fr/EXTERNE/projet/pampa/PREPARE/prepare.html" target="_top">http://www.irisa.fr/EXTERNE/projet/pampa/PREPARE/prepare.html</a>,
vise les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> MPI&hellip; On ne sait pas
vraiment s'il sait g&eacute;n&eacute;rer du code pour processeurs IA32.

</p></li><li><p>
Combinant ADAPT et ADLIB, &laquo;&nbsp;<span class="quote">shpf</span>&nbsp;&raquo; (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Subset High Performance Fortran
compilation system</em></span></span>&nbsp;&raquo;)
est dans le domaine public et g&eacute;n&egrave;re du code Fortran 90 avec appels MPI.
Donc, si vous avez un compilateur Fortran 90 sous Linux&hellip;

</p></li><li><p>
SUIF (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Stanford University Intermediate Form</em></span></span>&nbsp;&raquo;, voir <a href="http://suif.stanford.edu" target="_top">
http://suif.stanford.edu</a>) propose des compilateurs parall&eacute;lis&eacute;s pour le C et le Fortran.
C'est aussi l'objectif du <span class="foreignphrase"><em class="foreignphrase">National Compiler Infrastructure Project</em></span>&hellip;
&laquo;&nbsp;<span class="quote">Quelqu'un s'occupe-t-il des syst&egrave;mes parall&egrave;les sous Linux&nbsp;?</span>&nbsp;&raquo;
</p></li></ul></div><p>

Je suis s&ucirc;r d'avoir omis plusieurs compilateurs potentiellement utiles 
dans les diff&eacute;rents dialectes du Fortran, mais ils sont si nombreux 
qu'il est difficile d'en garder la trace. A l'avenir, je pr&eacute;f&eacute;rerais ne 
lister que ceux qui sont r&eacute;put&eacute;s fonctionner sous Linux. Merci de 
m'envoyer commentaires et corrections en anglais par courrier 
&eacute;lectronique &agrave;

<code class="email">&lt;<a href="mailto:hankd CHEZ engr POINT uky POINT edu">hankd CHEZ engr POINT uky POINT edu</a>&gt;</code>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11573"></a>6.1.2.&nbsp;GLU (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Granular Lucid</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>
GLU (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Granular Lucid</em></span></span>&nbsp;&raquo;) est un syst&egrave;me de programmation de tr&egrave;s haut niveau
bas&eacute; sur un mod&egrave;le hybride combinant les mod&egrave;les intentionnel (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Lucid</em></span></span>&nbsp;&raquo;) et
imp&eacute;ratif. Il reconna&icirc;t &agrave; la fois les <span class="foreignphrase"><em class="foreignphrase">sockets</em></span> PVM et TCP. Fonctionne-t-il sous
Linux&nbsp;? (Le site du &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Computer Science Laboratory</em></span></span>&nbsp;&raquo; se trouve sur
<a href="http://www.csl.sri.com" target="_top">http://www.csl.sri.com</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1158F"></a>6.1.3.&nbsp;Jade et SAM</h4></div></div></div><p>
Jade est un langage de programmation en parall&egrave;le sous forme d'extension au
langage C et exploitant les concurrences de la granularit&eacute; des programmes s&eacute;quentiels et
imp&eacute;ratifs. Il s'appuie sur un mod&egrave;le de m&eacute;moire partag&eacute;e distribu&eacute;e, mod&egrave;le impl&eacute;ment&eacute;
par SAM dans les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de stations de travail utilisant PVM. Plus d'informations
sur
<a href="http://suif.stanford.edu/~scales/sam.html" target="_top">http://suif.stanford.edu/~scales/sam.html</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N1159B"></a>6.1.4.&nbsp;Mentat et Legion</h4></div></div></div><p>
Mentat est un syst&egrave;me de traitement en parall&egrave;le orient&eacute; objet qui fonctionne
avec des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de stations de travail et qui a &eacute;t&eacute; port&eacute; vers Linux. Le
&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Mentat Programming Language</em></span></span>&nbsp;&raquo; (MPL) est un langage de programmation orient&eacute;
objet bas&eacute; sur le C++. Le syst&egrave;me temps-r&eacute;el de Mentat utilise quelque chose
qui ressemble vaguement &agrave; des appels de proc&eacute;dures &agrave; distance (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">remote
proc&eacute;dure calls</em></span></span>&nbsp;&raquo;&nbsp;: RPC) non bloquantes. Plus d'informations sur <a href="http://www.cs.virginia.edu/~mentat" target="_top">http://www.cs.virginia.edu/~mentat</a>.
</p><p>
Legion <a href="http://www.cs.virginia.edu/~legion/" target="_top">http://www.cs.virginia.edu/~legion</a> est construit au dessus de Mentat, et simule une machine virtuelle unique
au travers des machines d'un r&eacute;seau large zone.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N115B5"></a>6.1.5.&nbsp;MPL (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">MasPar Programming Language</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>

&Agrave; ne pas confondre avec le MPL de Mentat, ce langage &eacute;tait initialement 
d&eacute;velopp&eacute; pour &ecirc;tre le dialecte C parall&egrave;le natif des supercalculateurs 
SIMD MasPar. MasPar ne se situe plus vraiment sur ce segment (ils 
s'appellent maintenant <a href="http://www.neovista.com" target="_top">NeoVista 
Solutions</a> et se sp&eacute;cialisent dans le <span class="foreignphrase"><em class="foreignphrase">data 
mining</em></span><sup>[<a href="#ftn.N115C4" name="N115C4">27</a>]</sup>, mais leur compilateur MPL a &eacute;t&eacute; construit en 
utilisant GCC. Il est donc librement disponible. Dans un effort concert&eacute; 
entre les universit&eacute; de l'Alabama, &agrave; Huntsville, et celle de Purdue, le 
MPL MasPar a &eacute;t&eacute; recibl&eacute; pour g&eacute;n&eacute;rer du code C avec appels AFAPI (voir 
la section 3.6), et fonctionne ainsi &agrave; la fois sur des machines Linux de 
type SMP et sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>. Le 
compilateur est, n&eacute;anmoins, quelque peu bogu&eacute;&hellip; voir <a href="http://www.math.luc.edu/~laufer/mspls/papers/cohen.ps" target="_top">http://www.math.luc.edu/~laufer/mspls/papers/cohen.ps</a>.

</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N115D1"></a>6.1.6.&nbsp;PAMS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel Application Management System</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>
Myrias est une compagnie qui vend un logiciel nomm&eacute; PAMS (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Parallel
Application Management System</em></span></span>&nbsp;&raquo;). PAMS fournit des directives tr&egrave;s
simples destin&eacute;es au traitement en parall&egrave;le utilisant de la m&eacute;moire
partag&eacute;e virtuelle. Les r&eacute;seaux de machines Linux ne sont pas encore
pris en charge.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N115DE"></a>6.1.7.&nbsp;Parallaxis-III</h4></div></div></div><p>
Parallaxis-III est un langage de programmation structur&eacute;e qui reprend
Modula-2, en ajoutant des &laquo;&nbsp;<span class="quote">processeurs et connexions virtuels</span>&nbsp;&raquo; pour
le parall&eacute;lisme des donn&eacute;es (un mod&egrave;le SIMD). La suite Parallaxis
comprend des compilateurs pour ordinateurs s&eacute;quentiels et parall&egrave;les,
un d&eacute;bogueur (extension des d&eacute;bogueurs <code class="literal">gdb</code>
et <code class="literal">xgdb</code>), et une vaste gamme d'algorithmes
d'exemple de diff&eacute;rents domaines, en particulier dans le traitement des
images. Il fonctionne sur les syst&egrave;mes Linux s&eacute;quentiels. Une ancienne
version prenait d&eacute;j&agrave; en charge diverses plate-formes parall&egrave;les, et une
prochaine version le fera &agrave; nouveau (comprendre&nbsp;: ciblera les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> PVM).
Plus d'informations sur <a href="http://www.informatik.uni-stuttgart.de/ipvr/bv/p3/p3.html" target="_top">http://www.informatik.uni-stuttgart.de/ipvr/bv/p3/p3.html</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N115F5"></a>6.1.8.&nbsp;pC++/Sage++</h4></div></div></div><p>
pC++/Sage++ est une extension au langage C autorisant les op&eacute;rations de
type &laquo;&nbsp;<span class="quote">donn&eacute;es en parall&egrave;le</span>&nbsp;&raquo;, en utilisant des &laquo;&nbsp;<span class="quote">collections d'objets</span>&nbsp;&raquo;
form&eacute;es &agrave; partir de classes &laquo;&nbsp;<span class="quote">&eacute;l&eacute;ments</span>&nbsp;&raquo; de base. Il s'agit d'un pr&eacute;processeur
g&eacute;n&eacute;rant du code C++ pouvant fonctionner en environnement PVM. Est-ce que
cela fonctionne sous Linux&nbsp;? Plus d'informations ici&nbsp;: <a href="http://www.extreme.indiana.edu/sage/" target="_top">http://www.extreme.indiana.edu/sage</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11607"></a>6.1.9.&nbsp;SR (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Synchronizing Resources</em></span></span>&nbsp;&raquo;)</h4></div></div></div><p>
SR (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Synchronizing Resources</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">ressources synchronis&eacute;es</span>&nbsp;&raquo;)
est un langage de programmation de situations concurrentes et dans lequel les ressources
encapsulent les processus et les variables qu'ils partagent. Les &laquo;&nbsp;<span class="quote">op&eacute;rations</span>&nbsp;&raquo;
forment le principal m&eacute;canisme d'interactions entre processus. SR propose une
int&eacute;gration novatrice des m&eacute;canismes utilis&eacute;es pour invoquer et traiter les
op&eacute;rations. En cons&eacute;quence, les appels de proc&eacute;dures locaux ou &agrave; distance,
les rendez-vous, le <span class="foreignphrase"><em class="foreignphrase">message passing</em></span>, la cr&eacute;ation dynamique de processus, la
multi-diffusion et les s&eacute;maphores sont tous pris en charge. SR g&egrave;re &eacute;galement
les variables et op&eacute;rations globales partag&eacute;es.
</p><p>
Il existe une adaptation Linux, mais le type de parall&eacute;lisme que SR peut y
faire fonctionner n'est pas clairement d&eacute;fini. Plus d'informations sur <a href="http://www.cs.arizona.edu/sr/www/index.html" target="_top">http://www.cs.arizona.edu/sr/www/index.html</a>.
</p></div><div class="sect3" lang="fr"><div class="titlepage"><div><div><h4 class="title"><a name="N11623"></a>6.1.10.&nbsp;ZPL et IronMan</h4></div></div></div><p>
ZPL est un langage de programmation par matrices con&ccedil;u pour g&eacute;rer les
applications scientifiques et d'ing&eacute;nierie. Il g&eacute;n&egrave;re des appels &agrave; une
interface de <span class="foreignphrase"><em class="foreignphrase">message-passing</em></span> assez simple nomm&eacute;e IronMan, et les quelques
fonctions qui constituent cette interface peuvent facilement &ecirc;tre
impl&eacute;ment&eacute;es &agrave; l'aide de pratiquement n'importe quel syst&egrave;me de <span class="foreignphrase"><em class="foreignphrase">message-passing</em></span>.
Il est toutefois principalement tourn&eacute; vers PVM et MPI sur des <span class="foreignphrase"><em class="foreignphrase">clusters</em></span>
de stations de travail, et peut fonctionner sous Linux. Plus d'informations sur
<a href="http://www.cs.washington.edu/research/zpl/home/index.html" target="_top">http://www.cs.washington.edu/research/zpl/home/index.html</a>.
</p></div></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N11635"></a>6.2.&nbsp;Question de performance</h3></div></div></div><p>
Beaucoup de gens passent beaucoup de temps &agrave; faire des mesures de
performances &laquo;&nbsp;<span class="quote">au banc d'essai</span>&nbsp;&raquo; de cartes-m&egrave;res particuli&egrave;res, de
carte r&eacute;seau, et c&aelig;tera, pour d&eacute;terminer laquelle d'entre elles est la
meilleure. Le probl&egrave;me de cette approche est que, le temps d'arriver &agrave; des
r&eacute;sultats probants, le mat&eacute;riel test&eacute; peut ne plus &ecirc;tre le meilleur.
Il peut m&ecirc;me &ecirc;tre retir&eacute; du march&eacute; et remplac&eacute; par un mod&egrave;le r&eacute;vis&eacute; et
aux propri&eacute;t&eacute;s radicalement diff&eacute;rentes.
</p><p>
Acheter du mat&eacute;riel pour PC, c'est un peu comme acheter du jus d'orange.
Il est g&eacute;n&eacute;ralement fabriqu&eacute; &agrave; partir de produits de bonne qualit&eacute;, et ce
quelque soit le nom de la compagnie qui le distribue. Peu de gens connaissent
ou se soucient de la provenance de ces composant (ou du concentr&eacute; de jus
d'orange). Ceci dit, il existe quelques diff&eacute;rences auxquelles on devrait
&ecirc;tre attentif. Mon conseil est simple&nbsp;: Soyez simplement conscients de ce
que vous pouvez attendre de votre mat&eacute;riel lorsqu'il fonctionne sous Linux,
puis portez votre attention sur la rapidit&eacute; de livraison, un prix raisonnable
et une garantie convenable.
</p><p>
Il existe un excellent aper&ccedil;u des diff&eacute;rents processeurs pour PC sur <a href="http://www.pcguide.com/ref/cpu/fam/" target="_top">http://www.pcguide.com/ref/cpu/fam/</a>; En fait, le site <a href="http://www.pcguide.com/" target="_top">PC Guide</a> entier est rempli de bonnes pr&eacute;sentations techniques
de l'&eacute;lectronique d'un PC. Il est &eacute;galement utile d'en savoir un peu sur les
performances et la configuration d'un mat&eacute;riel sp&eacute;cifique, et le <a href="http://www.traduc.org/docs/howto/lecture/Benchmarking-HOWTO.html" target="_top">Linux&nbsp;Benchmarking&nbsp;HOWTO</a> est un bon document pour commencer.
</p><p>

Les processeurs Intel IA32 sont dot&eacute;s de plusieurs registres sp&eacute;ciaux 
qui peuvent &ecirc;tre utilis&eacute;s pour mesurer les performances d'un syst&egrave;me en 
fonction dans ses moindres d&eacute;tails. Le VTune d'Intel, sur <a href="http://developer.intel.com/design/perftool/vtune" target="_top">http://developer.intel.com/design/perftool/vtune</a>, fait un usage 
pouss&eacute; de ces registres de performances, dans un syst&egrave;me d'optimisation 
du code vraiment tr&egrave;s complet&hellip; qui malheureusement ne fonctionne 
pas sous Linux. Un pilote sous forme de module chargeable et une 
biblioth&egrave;que de fonctions servant &agrave; acc&eacute;der aux registres de 
performances du Pentium sont disponibles et ont &eacute;t&eacute; &eacute;crits par Cuneyt 
Akinlar. Souvenez-vous que ces registres de performance sont diff&eacute;rents 
selon les processeurs IA32. Ce code ne fonctionnera donc que sur un 
Pentium, pas sur un 486, ni sur un Pentium Pro, un Pentium II, un K6, et 
c&aelig;tera.

</p><p>

Il y a encore un commentaire &agrave; faire &agrave; propos des performances, destin&eacute; 
sp&eacute;cialement &agrave; ceux qui veulent monter de grands 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> dans de petits espaces. Les 
processeurs modernes incorporent pour certains des capteurs de 
temp&eacute;rature, et utilisent des circuits ralentissant la cadence du 
processeur lorsque cette temp&eacute;rature d&eacute;passe un certain seuil (tentative 
de r&eacute;duction des &eacute;missions de chaleur et d'am&eacute;lioration de la 
fiabilit&eacute;). Je ne suis pas en train de dire que tout le monde devrait 
sortir s'acheter un module &agrave; effet Peltier (ou &laquo;&nbsp;<span class="quote">pompe &agrave; 
chaleur</span>&nbsp;&raquo;) pour refroidir chaque CPU, mais que nous devrions &ecirc;tre 
conscients du fait qu'une temp&eacute;rature trop &eacute;lev&eacute; ne se contente pas de 
raccourcir la dur&eacute;e de vie des composants, mais agit aussi directement 
sur les performances d'un syst&egrave;me. Ne placez vos ordinateurs dans des 
configurations pouvant bloquer le flux d'air, pi&eacute;geant la chaleur dans 
des zones confin&eacute;es, et c&aelig;tera.

</p><p>

Enfin, les performances ne sont pas seulement une question de vitesse, 
mais &eacute;galement de fiabilit&eacute; et de disponibilit&eacute;. Une haute fiabilit&eacute; 
signifie que votre syst&egrave;me ne plantera pratiquement jamais, m&ecirc;me si un 
composant vient &agrave; tomber en panne&hellip; ce qui n&eacute;cessite g&eacute;n&eacute;ralement 
un mat&eacute;riel sp&eacute;cial comme une alimentation redondante et une carte-m&egrave;re 
autorisant le remplacement &laquo;&nbsp;<span class="quote">&agrave; chaud</span>&nbsp;&raquo; des &eacute;quipements qui y 
sont connect&eacute;s. Tout cela est en g&eacute;n&eacute;ral assez cher. Une haute 
disponibilit&eacute; signifie que votre syst&egrave;me sera pr&ecirc;t &agrave; &ecirc;tre utilis&eacute; 
pratiquement tout le temps. Le syst&egrave;me peut planter si l'un des 
composants tombe en panne, mais il pourra &ecirc;tre r&eacute;par&eacute; et red&eacute;marr&eacute; tr&egrave;s 
rapidement. Le High-Availability HOWTO traite plusieurs des cas de base 
de la haute disponibilit&eacute;. En revanche, dans le cas des 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span>, une haute disponibilit&eacute; peut 
&ecirc;tre assur&eacute;e en pr&eacute;voyant simplement quelques pi&egrave;ces de rechange. 
&Eacute;liminer le mat&eacute;riel d&eacute;fectueux et le remplacer par une de ces pi&egrave;ces de 
rechange peut apporter une plus grande disponibilit&eacute; pour un co&ucirc;t 
d'entretien moins &eacute;lev&eacute; que celui d'un contrat de maintenance.

</p></div><div class="sect2" lang="fr"><div class="titlepage"><div><div><h3 class="title"><a name="N11662"></a>6.3.&nbsp;Conclusion &mdash; C'est fini&nbsp;!</h3></div></div></div><p>
Alors, y a-t-il quelqu'un dans l'assembl&eacute;e qui fasse du traitement en
parall&egrave;le sous Linux&nbsp;? Oui&nbsp;!
</p><p>

Il y a encore peu de temps, beaucoup de gens se demandaient si la mort 
de plusieurs compagnies fabricant des supercalculateurs parall&egrave;les 
n'annon&ccedil;ait pas la fin du traitement en parall&egrave;le. Ce n'&eacute;tait alors pas 
mon opinion (voir <a href="http://dynamo.ecn.purdue.edu/~hankd/Opinions/pardead.html" target="_top">http://dynamo.ecn.purdue.edu/~hankd/Opinions/pardead.html</a> pour 
un aper&ccedil;u assez amusant de ce qui, &agrave; mon avis, s'est vraiment pass&eacute;), et 
il semble assez clair qu'aujourd'hui le traitement en parall&egrave;le est &agrave; 
nouveau sur une pente ascendante. M&ecirc;me Intel, qui n'a cess&eacute; que 
r&eacute;cemment de produire des supercalculateurs parall&egrave;les, est fier du 
soutien apport&eacute; au traitement en parall&egrave;le par des choses comme le MMX, 
et l'EPIC (&laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Explicitly Parallel Instruction 
Computer</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">Ordinateur &agrave; jeu 
d'Instructions Parall&egrave;les Explicites</span>&nbsp;&raquo;) de l'IA64.

</p><p>

Si vous faites une recherche sur les mots &laquo;&nbsp;<span class="quote">Linux</span>&nbsp;&raquo; et 
&laquo;&nbsp;<span class="quote">Parall&egrave;le</span>&nbsp;&raquo; dans votre moteur de recherche favori, vous 
trouverez un certain nombre d'endroits impliqu&eacute;s dans le traitement en 
parall&egrave;le sous Linux. Les <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> de PC 
sous Linux, en particulier, font leur apparition un peu partout. Le fait 
que Linux se pr&ecirc;te particuli&egrave;rement bien &agrave; ce genre de t&acirc;che combin&eacute; aux 
co&ucirc;ts r&eacute;duits et aux hautes performances du mat&eacute;riel pour PC ont fait du 
traitement en parall&egrave;le sous Linux une approche populaire, tant pour les 
groupes au budget restreint que pour les vastes laboratoires de 
recherche nationaux confortablement subventionn&eacute;s.

</p><p>

Certains projets &eacute;num&eacute;r&eacute;s dans ce document tiennent une liste de sites 
de recherche &laquo;&nbsp;<span class="quote">apparent&eacute;s</span>&nbsp;&raquo; utilisant des configurations 
Linux parall&egrave;les similaires. Vous trouvez cependant sur <a href="http://yara.ecn.purdue.edu/~pplinux/Sites" target="_top">http://yara.ecn.purdue.edu/~pplinux/Sites</a> un document hypertexte 
dont le but est d'apporter photographies, descriptions et contacts vers 
les sites de tous les projets utilisant Linux pour effectuer des 
traitement en parall&egrave;le. Si vous voulez voir votre propre site y 
figurer&nbsp;:

</p><div class="itemizedlist"><ul type="disc"><li><p>

Vous devez avoir un site &laquo;&nbsp;<span class="quote">permanent</span>&nbsp;&raquo; pr&eacute;sentant le 
fonctionnement d'un syst&egrave;me Linux en parall&egrave;le&nbsp;: Une machine SMP, 
un syst&egrave;me SWAR, ou un PC &eacute;quip&eacute; de processeurs d&eacute;di&eacute;s, configur&eacute; pour 
permettre aux utilisateurs d'<span class="emphasis"><em>ex&eacute;cuter des programmes 
parall&egrave;les sous Linux</em></span>. Un environnement logiciel bas&eacute; sur 
Linux et prenant directement en charge la gestion du traitement en 
parall&egrave;le (tel que PVM, MPI ou AFAPI) doit &ecirc;tre install&eacute; sur le syst&egrave;me. 
En revanche, le mat&eacute;riel ne doit pas n&eacute;cessairement &ecirc;tre d&eacute;di&eacute; au 
traitement en parall&egrave;le sous Linux et peut &ecirc;tre employ&eacute; &agrave; des t&acirc;ches 
compl&egrave;tement diff&eacute;rentes lorsque les programmes parall&egrave;les ne sont pas 
en cours d'ex&eacute;cution.

</p></li><li><p>

Vous devez formuler une demande pour que votre site apparaisse sur cette 
liste. Envoyez vos informations en anglais &agrave;

<code class="email">&lt;<a href="mailto:hankd CHEZ engr POINT uky POINT edu">hankd CHEZ engr POINT uky POINT edu</a>&gt;</code>.

Merci de respecter le format utilis&eacute; par les autres entr&eacute;es pour les 
informations concernant votre site. <span class="emphasis"><em>Aucun site ne sera 
r&eacute;pertori&eacute; sans demande explicite de la part du responsable de ce 
site</em></span>.

</p></li></ul></div><p>

Quatorze <span class="foreignphrase"><em class="foreignphrase">clusters</em></span> sont actuellement 
r&eacute;pertori&eacute;s dans cette liste, mais nous avons &eacute;t&eacute; inform&eacute;s de 
l'existence de plusieurs douzaines de 
<span class="foreignphrase"><em class="foreignphrase">clusters</em></span> Linux &agrave; travers le monde. Bien 
s&ucirc;r, cette inscription n'implique aucun engagement. Notre objectif est 
simplement de favoriser le d&eacute;veloppement des connaissances, de la 
recherche et de la collaboration impliquant le traitement en parall&egrave;le 
sous Linux.

</p></div></div><div class="footnotes"><br><hr align="left" width="100"><div class="footnote"><p><sup>[<a href="#N101F7" name="ftn.N101F7">1</a>] </sup>

N.D.T.&nbsp;: &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">flat memory 
model</em></span></span>&nbsp;&raquo;, dans lequel toute la m&eacute;moire se trouve 
dans le m&ecirc;me plan m&eacute;moire, par opposition aux segments, m&eacute;moire pagin&eacute;e, 
et tout autre mod&egrave;le compos&eacute;.

</p></div><div class="footnote"><p><sup>[<a href="#N102D7" name="ftn.N102D7">2</a>] </sup>

N.D.T.&nbsp;: d&eacute;composition d'un programme en plusieurs processus 
distincts, mais travaillant simultan&eacute;ment et de concert.

</p></div><div class="footnote"><p><sup>[<a href="#N102EE" name="ftn.N102EE">3</a>] </sup>

N.D.T.&nbsp;: qui a d&eacute;marr&eacute; la machine avant de passer en mode SMP.

</p></div><div class="footnote"><p><sup>[<a href="#N1030A" name="ftn.N1030A">4</a>] </sup>
N.D.T.&nbsp;: le lien vers l'ancienne version 1.1, lui, n'existe plus. La 
documentation la plus r&eacute;cente se trouve &agrave; ce jour sur <a href="http://www.intel.com/design/Pentium4/documentation.htm" target="_top">http://www.intel.com/design/Pentium4/documentation.htm</a>.
</p></div><div class="footnote"><p><sup>[<a href="#N104B6" name="ftn.N104B6">5</a>] </sup>

N.D.T.&nbsp;: ins&eacute;r&eacute; au sein du code source, ici en C.

</p></div><div class="footnote"><p><sup>[<a href="#N10520" name="ftn.N10520">6</a>] </sup>
 
N.D.T.&nbsp;: soit, sous Unix, &ecirc;tre sous le compte <code class="literal">root</code>.

</p></div><div class="footnote"><p><sup>[<a href="#N10544" name="ftn.N10544">7</a>] </sup>

N.D.T.&nbsp;: temporisations introduites au sein d'un programme en 
utilisant par exemple des boucles et en consommant ainsi tout le temps 
machine allou&eacute; au processus plut&ocirc;t qu'en rendant la main au syst&egrave;me.

</p></div><div class="footnote"><p><sup>[<a href="#N105CD" name="ftn.N105CD">8</a>] </sup>

N.D.T.&nbsp;: &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">spin 
locks</em></span></span>&nbsp;&raquo;&nbsp;: mise en &eacute;tat d'attente jusqu'&agrave; ce 
qu'une condition soit remplie.

</p></div><div class="footnote"><p><sup>[<a href="#N106BE" name="ftn.N106BE">9</a>] </sup>

N.D.T.&nbsp;: Il s'agit en fait d'un appel Unix standard.

</p></div><div class="footnote"><p><sup>[<a href="#N107AC" name="ftn.N107AC">10</a>] </sup>

N.D.T.&nbsp;: il semble que ce panel ne soit plus mis &agrave; jour depuis un 
certain temps. Le site sugg&eacute;r&eacute; proposait &agrave; la date de r&eacute;daction de la 
version fran&ccedil;aise le bulletin le plus r&eacute;cent, mais n'est pas officiel.

</p></div><div class="footnote"><p><sup>[<a href="#N109FA" name="ftn.N109FA">11</a>] </sup>

N.D.T.&nbsp;: d&eacute;sormais int&eacute;gr&eacute; aux sources du noyau.

</p></div><div class="footnote"><p><sup>[<a href="#N10A0B" name="ftn.N10A0B">12</a>] </sup>

N.D.T.&nbsp;: les pilotes FC pour le noyau Linux ont en fait &eacute;t&eacute; &eacute;crits 
en 1999.

</p></div><div class="footnote"><p><sup>[<a href="#N10A3B" name="ftn.N10A3B">13</a>] </sup>

N.D.T.&nbsp;: La <span class="foreignphrase"><em class="foreignphrase">Fibre Channel 
Association</em></span> (FCA) et la <span class="foreignphrase"><em class="foreignphrase">Fibre Channel 
Loop Community</em></span> (FCLC) ont fusionn&eacute; en 2000 pour former 
la <span class="foreignphrase"><em class="foreignphrase">Fibre Channel Industry Association</em></span>.

</p></div><div class="footnote"><p><sup>[<a href="#N10A53" name="ftn.N10A53">14</a>] </sup>

N.D.T.&nbsp;: les premiers pilotes FireWire pour Linux ont &eacute;t&eacute; &eacute;crits en 
1999 mais, bien que disponibles en standard, sont toujours consid&eacute;r&eacute;s 
comme exp&eacute;rimentaux.

</p></div><div class="footnote"><p><sup>[<a href="#N10A9D" name="ftn.N10A9D">15</a>] </sup>

N.D.T.&nbsp;: HiPPI est aujourd'hui pris en charge par Linux, mais de 
fa&ccedil;on restreinte et exp&eacute;rimentale.

</p></div><div class="footnote"><p><sup>[<a href="#N10AEE" name="ftn.N10AEE">16</a>] </sup>

N.D.T.&nbsp;: l'IrDA est pris en charge par le noyau depuis 2000.

</p></div><div class="footnote"><p><sup>[<a href="#N10B60" name="ftn.N10B60">17</a>] </sup>

SAN&nbsp;: &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">System Area 
Network</em></span></span>&nbsp;&raquo;.

</p><p>

N.D.T.&nbsp;: &agrave; ne pas confondre avec &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">Storage Area 
Network</em></span></span>&nbsp;&raquo;, qui partage le m&ecirc;me acronyme.

</p></div><div class="footnote"><p><sup>[<a href="#N10C39" name="ftn.N10C39">18</a>] </sup>

N.D.T.&nbsp;: Sequent a &eacute;t&eacute; absorb&eacute; par IBM en septembre 1999.

</p></div><div class="footnote"><p><sup>[<a href="#N10CC0" name="ftn.N10CC0">19</a>] </sup>

N.D.T.&nbsp;: et donc d&eacute;sormais &agrave; Hewlett-Packard.

</p></div><div class="footnote"><p><sup>[<a href="#N10DDD" name="ftn.N10DDD">20</a>] </sup>

N.D.T.&nbsp;: les ports et le standard USB sont aujourd'hui parfaitement 
reconnus par Linux.

</p></div><div class="footnote"><p><sup>[<a href="#N10EB3" name="ftn.N10EB3">21</a>] </sup>

N.D.T.&nbsp;: par opposition &agrave; un &laquo;&nbsp;<span class="quote">appel syst&egrave;me</span>&nbsp;&raquo;, donc sans franchir
la barri&egrave;re du passage en mode noyau.

</p></div><div class="footnote"><p><sup>[<a href="#N110E5" name="ftn.N110E5">22</a>] </sup>

N.D.T.&nbsp;: MOSIX fonctionne aujourd'hui sous Linux.

</p></div><div class="footnote"><p><sup>[<a href="#N111BA" name="ftn.N111BA">23</a>] </sup>

N.D.T.&nbsp;: initialement, huit liens &eacute;taient propos&eacute;s &agrave; cet endroit, 
devenus pour la plupart obsol&egrave;tes.

</p></div><div class="footnote"><p><sup>[<a href="#N111F5" name="ftn.N111F5">24</a>] </sup>

N.D.T.&nbsp;: FLOP = &laquo;&nbsp;<span class="quote"><span class="foreignphrase"><em class="foreignphrase">FLoating point 
OPeration</em></span></span>&nbsp;&raquo;, ou &laquo;&nbsp;<span class="quote">OP&eacute;ration en Virgule 
Flottante</span>&nbsp;&raquo;.

</p></div><div class="footnote"><p><sup>[<a href="#N11385" name="ftn.N11385">25</a>] </sup>

N.D.T.&nbsp;: il s'agit en fait de simuler la r&eacute;-entr&eacute;e par le cot&eacute; 
oppos&eacute; des donn&eacute;es &eacute;ject&eacute;es par le d&eacute;calage. L'exemple qui suit permet 
d'impl&eacute;menter ce cas en langage C, mais la plupart des microprocesseurs 
sont &eacute;quip&eacute;s d'instructions de rotation effectuant cela en une seule 
op&eacute;ration.

</p></div><div class="footnote"><p><sup>[<a href="#N11458" name="ftn.N11458">26</a>] </sup>

N.D.T.&nbsp;: WINE a &eacute;volu&eacute; avec le temps et reconna&icirc;t naturellement
les versions plus r&eacute;centes de ce syst&egrave;me d'exploitation.

</p></div><div class="footnote"><p><sup>[<a href="#N115C4" name="ftn.N115C4">27</a>] </sup>

N.D.T.&nbsp;: le <span class="foreignphrase"><em class="foreignphrase">data mining</em></span> consiste &agrave; 
&eacute;tudier et mettre au point des techniques efficaces de recherche d'une 
information au travers d'une immense quantit&eacute; de donn&eacute;es.

</p></div></div></div></body></html>